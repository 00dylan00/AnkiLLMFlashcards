{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\n",
    "\n",
    "Structure:\n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "import sys, os, numpy as np, pandas as pd\n",
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# variables\n",
    "\n",
    "specialization = \"mathematics-for-ml\"\n",
    "course = \"machine-learning-calculus\"\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\", specialization, course)\n",
    "\n",
    "\n",
    "# functions\n",
    "\n",
    "\n",
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 14:31:48,741 - INFO - Retrieving data for course machine-learning-calculus \n",
      "2024-07-15 14:31:48,741 - INFO - Number of files: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/machine-learning/machine-learning-calculus/01_week-1-derivatives-and-optimization\n",
      "../data/machine-learning/machine-learning-calculus/machine-learning-calculus-syllabus-parsed.json\n",
      "../data/machine-learning/machine-learning-calculus/03_week-3-optimization-in-neural-networks-and-newtons-method\n",
      "../data/machine-learning/machine-learning-calculus/02_week-2-gradients-and-gradient-descent\n"
     ]
    }
   ],
   "source": [
    "all_text = list()\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    # week folders\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    print(folder_path)\n",
    "    lecture = folder.split(\"/\")[-1]\n",
    "    lecture = re.sub(r\"week-\\d+-\", \"\", lecture)\n",
    "    tag = f\"{specialization.replace('-', ' ')}::{course.replace('-', ' ')}::{lecture.replace('-', ' ').replace('_', ' ')}\"\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        # lesson folder\n",
    "        for lesson in os.listdir(folder_path):\n",
    "            lesson_path = os.path.join(folder_path, lesson)\n",
    "            for file in os.listdir(lesson_path):\n",
    "\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(lesson_path, file)\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        text = f.read()\n",
    "                        all_text.append((text, tag))\n",
    "logging.info(f\"Retrieving data for course {data_path.split('/')[-1]} \")\n",
    "logging.info(f\"Number of files: {len(all_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"By now you have lots of tools\\nto work with derivatives. But the question is, what\\nare they useful for, other than calculating rates of change and more specifically, why are they useful\\nin machine learning? Well, the main application\\nhere in machine learning of derivatives is that they\\nare used for optimization. Optimization is when\\nyou want to find the maximum or the minimum\\nvalue of a function. This is very important\\nin machine learning because in machine learning, you want to find the model\\nthat best fits your dataset, and in order to find this model what you do is you calculate an error function\\nthat tells you how far are you from an ideal model. When you are able to minimize this error function then\\nyou have the best model. Let me show you how. Consider\\nthe following example. Imagine that you're\\nsitting in a bench in a sauna and you\\nstart feeling too hot so you're going\\nto try to switch places to find the coldest\\nspot on the bench. With you, you have\\na thermometer, and it reads 85 degrees Celsius so you're going\\nto try to move around, and see where the\\ntemperature is colder. You try your luck by\\nmoving to the left, and then the thermometer\\nreads 90 degrees Celsius. That's not good, it got hotter. You try moving in\\nthe other direction, and then the thermometer\\nreads 80 degrees Celsius, which is much better. You continue moving in\\nthat direction since the coldest spot seems\\nto be in that direction. You move always recording lower temperatures\\nuntil you get to a point where in any direction that you\\nmove, it gets hotter. You conclude that, that must be the coldest\\nspot in the sauna. Now let's look at this curve, and this curve represents the temperature at each\\npoint in the bench. This was your initial point. This is the first\\ntry when you were trying to get colder\\nbut instead got hotter. This is where you moved\\nfor the second try, and you continued moving, always finding a\\ncolder and colder spot until eventually you\\nreach this point over here where no matter which direction you moved you\\nwould find a hotter place. You concluded that this must be the coldest\\nplace of the sauna. Now this has to do\\nwith derivatives. Let's look at the slope of the tangent at\\nall these points. Notice that, for all\\nthe points where moving to the right\\nled to a colder spot, the slopes are less than zero. For the points where moving\\nto the left leads to a colder spot the slope\\nis bigger than zero, and for the coldest place, the one where no\\nmatter where you move, you get hotter, the\\nslope was zero. That's a very\\ninteresting property of maximum and minimum points. The slope at those\\npoints is always zero. Now, it's not always true that a point with slope zero\\nis a maximum or a minimum. Take a look at this, it's a much more\\ncomplicated sauna, and the coldest part is\\nsomewhere to the right of two, but there's a bunch of points where the\\nderivative is zero. Unfortunately you\\nhave to try them all and see which one is the\\nactual coldest spot. These are all candidates, but at least you reduce it\\nto a small number of points. These candidates for the\\nminimum are the points of zero slope and those are\\ncalled local minima, and the absolute minimum is\\ncalled the global minimum. When you want to optimize a function whether maximizing or minimizing it and the function is differentiable\\nat every point, then you know one thing, it's that the candidates\\nfor maximum and minimum are those points for which the\\nderivative is zero.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"So now let's say you have two power lines,\\na blue one and an orange one. And for reference, here is the origin. So the blue power line is\\na meters away from the origin. The orange power line is b\\nmeters away from the origin, and your house is x meters\\naway from the origin. And the point is to find out\\nwhere to locate the house, to minimize the cost of connecting\\nto the two power lines. So, the distance from the house to\\nthe blue power line is x minus a. And the distance from the house\\nto the orange power line is going to be b minus x. So, the cost of connecting to\\nthe blue power line is going to be the distance squared,\\nthat's x minus a square. And the cost of connecting to the orange\\npower line is going to be that distante square, which is b minus x squared. Now, notice that b minus x squared\\nis the same as x minus b squared, because the square of a negative number\\nis the square of the positive number two. So therefore we're going to consider\\nthis distance as x minus a, x minus b, x minus c, etc. And even if they're negative, that doesn't matter because\\nthe squares are the right thing. So now, what is the total cost of\\nconnecting to both power lines? Well, is the sum of these two costs,\\nx minus a squared, plus x minus b squared. And if x is over here, then the point of\\nthe problem is to find the distance x, that will minimize this cost function. So spoiler alert, the solution is to put\\nit in the very middle of the two power lines to reduce the cost, and\\nyou're going to see that very soon. But first, let's try to visualize the\\nproblem to better understand why the house should go in the middle. So, let's actually visualize\\nthe cost as the area of a square, because the area of the square\\nis x minus a squared. That's the cost of connecting\\nto the blue power line. And the cost of connecting to\\nthe orange power line is represented by the area of this squared over here,\\nwhich is x minus b square. And the sum of the two areas, or the total area is the total cost\\nof connecting to both power lines. That is the function that we\\nare supposed to minimize. So now let's look at some cases,\\nsee if the house is somewhere here, the sum of the areas is not that big. If we put it too much to the left,\\nthen the orange square is too big. And if we put it too much to the right,\\nthen the blue square is too big. So it makes sense that somewhere in\\nthe middle should be the perfect spot. But let's use mathematics\\nto find the perfect spot. So, the cost function is x minus\\na squared, plus x minus b squared. This is a quadratic and\\nquadratics look like this, either they point upwards or\\nthey point downwards. This one in particular points upwards, because the coefficient\\nof x squared is positive. In other words, it's two x squared,\\nplus something else, plus something else. Now, in this parabola,\\nwe have to find the minimum point. Which is the point where\\nthe derivative is zero, the point where the slope\\nof the tangent is zero. So this tangent is horizontal, and\\nwe can find that using some calculations. So let's take the derivative\\nof the cost function. That's d by dx of x minus a squared\\nplus x minus b squared, and set that equal to zero. Because when that is equal to zero,\\nthat is the minimum point. So, by using the chain rule, you can see that the derivative of x\\nminus a squared is 2 times x minus a. And the derivative of x minus b\\nsquared is 2 times x minus b, and so that sum is equal to zero. You can divide by two to get x minus a,\\nplus x minus b equals zero. And now reorganize to get 2x\\nminus a minus b equals zero, and that's equivalent to x equal a plus b. So, x is equal to a plus b over 2. So the optimal solution is\\nto put the house in the very middle of the two power lines\\nin order to minimize the cost. So now, let's do it with more power lines.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In this video, you're going to see\\na slightly more complicated example of optimizing a function. The example consists on locating a house\\nin the perfect place, in order to minimize the cost of connecting\\nthis house to several power lines. Now, why is this example important? Because the function you're minimizing is\\nvery similar to the squared error, and the squared error is one of the most\\nimportant functions in machine learning. It is used to train linear regression\\nproblems, and certain neural networks. So here is the problem. You have a number of power lines in\\nyour area and you need to build a house somewhere, and you need to connect to\\nall of them for the best power supply. Now, connecting to these power grids is\\nexpensive, because the further you are, the longer and\\nwider the cable that you need. So, for example, let's say you\\nwant to connect to this power line and the distance between your house and\\nthe power line is X meters. So the cost of connecting to this\\npower line is going to be X2, so that distance squared. And the goal is to minimize the cost\\nof connecting to all three power lines. So the question is, where should you build\\nyour house in order to minimize this cost. So let's start with a very simple problem,\\nlet's say that there's only one power line and you want to get the power\\nwith the least amount of cost, where would you put the house? So if you set right by the power line,\\nthen you are right. Because if the distance from\\nthe power line is zero units, then the cost of connecting to the power\\nline is going to be 0 squared which is 0. So that's an easy problem, right? Let's try it with more power lines.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous video, you saw an optimization example that\\nled to the square loss. Now there's another\\nreally important function in machine learning\\ncalled the log loss. In this video, I'm\\ngoing to show you that also with an example. The way I like to see log loss is the same\\nway I like to see most of machine learning and\\nstatistics using coin flips. Let's play a game. I'm\\ngoing to toss a coin 10 times and we're going\\nto look at the results. If the results are seven heads\\nfollowed by three tails, then you win a lot of money. If they're not, then you\\ndon't win any money. The chances of winning\\nare pretty slim. However, you can pick the coin you want to use and\\nit can be a biased coin. Here's coin 1. It has a probability\\nof landing in heads of 70 percent and for landing\\nin tales of 30 percent. Coin 2 is a fair coin, so it has probabilities\\nof landing in heads and tails of 50 percent. Coin 3 is also biased and\\nis the opposite of coin 1. This one has a probability\\nof landing in heads of 30 percent and\\ntales of 70 percent. Here is a quiz. Which one of the\\nthree coins would you choose to maximize your\\nchances of winning? Coin 1, coin 2, or coin 3? Remember that to\\nwin at this game, you have to land\\nthe coin in heads seven times and then in\\ntails the next three times. In order to pick the best coin, let's calculate the\\nprobability of winning at the game with each\\nof the three coins. For coin 1, the probability\\nof landing in heads is 0.7 and for tails is 0.3. To land in heads seven times is 0.7^7 because\\nthese probabilities gets multiplied as they\\nare independent events. For the last three tails, then you multiply by 0.3^3. So 0.7^7 times 0.3^3 is 0.00222. That's quite a\\nsmall probability, but let's calculate\\nthe other ones. For coin 2 is 0.5^7 for the\\nseven heads times 0.5^3 for the three tails,\\nand that's 0.00097. Finally, for coin 3 is 0.3^7 times 0.7^3, and that's 0.00008. Those are even worse. As you can see, coin\\n1 is the best one. Is the one that helps us win the game with the\\nhighest probability. So that's the one you can pick. However, here's a question. Was coin 1 the best coin among all the possible coins you\\ncould possibly create? Well, for that,\\nwe need calculus. Let's pick a coin and say that the probability of\\nlanding in heads is p and the probability of\\nlanding in tails is one minus p because they're\\nsupposed to add to one. What we have to do is find\\nthe optimal value of p, the one that helps us maximize the\\nprobability of winning. First, what is the\\nprobability of winning? Well, to land the coin\\nin heads seven times, the probability is p^7, and to land it in tails three\\ntimes is one minus p cubed. Let's say g of p is the\\nprobability of winning, and that's p^7 times 1\\nminus p to the three, and we need to use\\ncalculus to maximize g of p. How do we maximize g of p? Well, we can take the derivative and\\nset it equal to zero. Here is the derivative\\nof g with respect to p. We're going to calculate the derivative of\\np^7 times 1 minus p cubed. First, we use the\\nproduct rule to split the p^7 and the\\none minus p cubed. We get derivative of p^7 times 1 minus p cubed plus p^7 times derivative\\nof 1 minus p cubed. Now we're going to calculate each one of these separately. For the first one, remember that the derivative p^7 is 7p^6. For the second one, you can\\nuse the chain rule over here. Derivative of 1 minus p cubed is 3 times 1 minus p squared, but then you need to take the\\nderivative of the inside, and the derivative\\nof 1 minus p with respect to p is minus 1. Now we can do some\\nalgebra to reorganize it like this and factor\\nit like this, and that needs to\\nbe equal to zero. There is a product of three\\nthings needs to be zero. For a product of three\\nthings to be zero, one of them has to\\nbe zero, at least. If the first factor is zero, p^6, that means p must be zero. For the second\\nfactor to be zero, then 1 minus p must be zero, which means p is equal to 1. For the third factor to be zero, we need p to be equal to 0.7. These are the three candidates\\nfor the optimal point. Now, does the first\\none makes sense? Well, if p equals 0, then the coin will\\nalways land in tails and you will have no\\nchance of landing in heads seven times.\\nThat one is discarded. For the same reason, the second one is\\ndiscarded as well, because if p equals 1, then the coin always\\nlands in heads and it has no chance of falling\\nin tails three times. Therefore, the answer\\nis p equals 0.7. Indeed, coin 1 was the best possible\\ncoin one could pick. That was a lot of work. Is there an easier way to do it? Well, next I'm\\ngoing to show you a much easier way to do it. It's actually a bit\\ncounter-intuitive because we have to take a step that makes it look harder and then it's\\ngoing to look easier. The step is take the\\nlogarithm of g of p. Because, if g of p is maximum, then so it's the\\nlogarithm of g of p. So maximizing the\\nlogarithm of g of p is the same thing\\nas maximizing g of p. Now what is the\\nlogarithm of g of p? Well, it's the logarithm of\\np^7 times 1 minus p cubed. Now logarithm has a\\nreally nice property, which is that the logarithm of a product is the sum\\nof the logarithms. So this is logarithm of p^7 plus logarithm of\\n1 minus p cubed. It also has another really\\nnice property which is a logarithm of p^7\\nis 7 logarithm of p, and logarithm of 1 minus p cubed is 3 logarithm of 1 minus p. Let's call that G of p. This\\nis a simplified version. This is the logarithm\\nof the probability. We want to maximize G\\nof p. How do we do it? Well, now we take\\nthe derivative. This derivative is actually\\nreally nice because remember that the derivative of logarithm of p is 1 over p. When we calculate the derivative of each\\none of the summons, we get 7 times 1 over p plus 3 times 1 over 1 minus\\np times minus 1, that negative 1 comes from the chain rule because it's the derivative of 1 minus p with respect to p. We can join the denominators to get 7 times 1 minus p minus 3p divided\\nby p times 1 minus p, and that's what's supposed\\nto be equal to 0. When we can solve\\nfor that equal to 0, we get the candidates\\nfor optimal p. Now notice that we've already ruled out the chances\\nof p being zero or one. Therefore, the denominator\\nis never zero. Setting this equal\\nto 0 is the same as setting the top equals 0. So 7 times 1 minus p minus\\n3p has to be equal to 0. We can solve for p to get\\nthat p is equal to 0.7. We get the same\\nsolution as before, except in a much simpler way. Now, this trick of\\ntaking a logarithm is a pretty popular trick\\nin machine learning. Logarithms of\\nprobabilities are very, very, very common\\nand very useful. As a matter of fact, what\\nwe really want is the negative of g of p. That is called the log loss and it's a very useful loss function\\nin machine learning. You're going to see it a lot\\nin classification problems. The reason that we\\nactually take negative g of p instead of g of p is that logarithm of p is actually a negative number when p is\\nin between zero and one. We want negative g of p\\nto be a positive number, and instead of maximizing it, like we did here, we\\nminimize negative g of p.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"So now the problem gets harder because\\nthere are three power lines and the third one is at\\na distance c from the origin. So again, the problem is where to\\nlocate the house to minimize the cost. So recall that the distances to\\nthe power lines are x- a for the first house, b- x for the second\\nhouse and c- x for the third house. And again,\\nwe don't have to worry if it's x- b or b- x in the houses to the right\\nto the left of power line. Because when we square these numbers,\\nwe always get positive numbers. If it's b- x or x- b,\\nit's still going to be x- b square. So let's actually draw these as areas. So, this is x- a squared area\\nof this square over here. That's the cost of connecting\\nto the first power line. This one is the cost of connecting to\\nthe second power line, x- b squared. And this one is the cost of\\nconnecting to the third power line, it's much more expensive because it's\\nmuch farther than the x- c squared. And again, the goal is to minimize the\\ncost of connecting to the power lines and that's the same as minimizing the area. So I wanted to take a wild guess and\\ntell me, what do you think is the cost\\nfunction of this whole problem? That's in the next quiz. If you said it's (x- a) squared +\\n(x + b) squared + (x- c) squared, then you are right because\\nthat is the cost function and that's the total area\\nof the three squares. So the question is where should we\\nput the house to minimize the cost? So now I'd like you to take a guess. What do you think is the solution? What do you think is the optimal\\nx that will minimize this cost? And so if you said that\\nthe solution is a + b + c over 3. So put the house in the average of\\nthe three coordinates of the power lines, then you were right. And now let me tell you why. So I like to see this\\ngeometrically in the following way, let's put a parabola\\nover the blue power line. And what does this problem mean? Well this is exactly\\nthe function x- a squared. If you look at where the house is\\nthe height in that parabola is the price of connecting to that power line because\\nit's precisely the parabola x- a squared. Here we put the parabola x - b squared\\non top of the orange power line and x- c squared on top of\\nthe green power line and the some of the streets\\nis parabola over here. If you summed the heights of the three\\nparabolas, you get the black parabola. And the point is to find that\\nminimum point in the black parabola, the point where the slope\\nof the tangent is zero. And as you can see it hints to be\\nthe average of the three coordinates. But let's do some math just like before\\nin order to solve for that perfect x. And again it's going to be the point\\nwhere the slope of the tangent is zero. So just some similar math time before, now we take the derivative of\\nthe cost function set equal to zero. The derivative is this and\\nwe can divide by 2 and still get 0. And we organize this as 3x- a- b- c = 0. So 3x is equal to a + b+ c. And therefore X is equal to\\nthe average of a, b and c. And so if you look at it\\nfrom the area perspective, what you're trying to do is minimize\\nthe total area of the squares. If you put your house way to the left,\\nyou have some big areas. If you put your house somewhere in\\nthe middle, you get some small areas and if you put it to the right,\\nyou get again some big areas. Now, this can be generalized. You can imagine that if you were to\\nhave n power lines, for example, in coordinates a1, a2 up to an. Then the solution would be to\\nput the house in the average of the coordinates of all the power lines. And this is actually the square\\nloss of very important loss function in machine learning. And that's going to pop out\\nagain later in this course.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous video, you found the optimal coin\\nto win at again. But if you look closely, what we did was exactly\\nmachine learning. It was finding the best\\nmodel for a dataset. What was the dataset?\\nThe 10 coin flips, which are seven heads\\nfollowed by three tails, and the model was the coin. The model is a coin that lands in heads with probability\\nP and in tails with probability one minus\\np. What you did was finding the model that most likely fits the\\ndataset on the left. How do you find that model? You minimize the log\\nloss and obtained that the optimal p is 0.7. Now the question is, why did that log loss have a logarithm? Why couldn't we simply solve the derivative\\nof a big product? Well, one reason is that\\nderivative products are hard, whereas derivative sums is easy. Of course, if it's a\\nproduct of two things, you can use the product rule. But what do you do for three\\nthings or four things, or many things like this? You have to iterate the product rule and\\nit gets really messy. The derivative of\\nthis function over here is actually this formula. That's something that is\\ntoo hard to work out. If there's an easier way, we should always take it, if we take the logarithm, then we get this\\nderivative over here; 6 over p plus 2 over\\n1 minus p et cetera. That's much cleaner. We do get some denominators, and that's because of\\nthe logarithm because the derivative of the\\nlogarithm of p is 1 over p. But denominators are\\na small price to pay to not have to work out the\\nformula on the left. But that's not the only\\nreason we use logarithms. Another reason why we use\\nlogarithms is because the product of tiny\\nthings is tiny. Imagine if you have a\\nprobability that is the product of 1,000\\nprobabilities. That's the product of 1,000\\nnumbers, between 0 and 1. That number could\\nbe really small and maybe a computer is not able\\nto handle a number so small. On the other hand, if\\nit's the logarithm, the logarithm of a\\nvery small number is a very large negative number and computers can\\nhandle those numbers. In short, anytime in machine learning that you have a very complicated product, think of using the logarithm. It may simplify it a lot, especially if you're\\ntaking derivatives.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Congratulations. You've\\nfinished week 1. This week you'll learn\\nwhat a derivative is, how to calculate them, how to use them for optimization\\nand most importantly, how this optimization problem is translated to machine\\nlearning problems. However, everything you've done so far is in one variable. As you may imagine, large machine-learning\\nproblems use lots and lots of variables. That's what you're\\ngoing to learn next, how to do all this\\nderivative stuff but in more variables.\\nSee you in week 2.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now you learned the derivative of several functions here's an important concept\\nthat's going to help you find the derivative\\nof more functions. It's the concept of\\nthe inverse function. Now, what is an\\ninverse function? Well, if your function\\ndoes a certain thing, then the inverse\\nfunction is the one that undoes that thing. So for example, if\\nthe function f, sends the number 3\\nto the number 5, then the inverse function\\nsends the number 5 back to the number 3. Now you're going to learn that if you know the\\nderivative of function f, then the derivative of the\\ninverse function is actually very easy to calculate because they are\\nvery well connected. The mental image I have of an inverse function\\nis the following. Let's say that you\\nhave a person and the function f does\\nsomething to the person. It puts a hat on them. What would the function g have\\nto do if it's the inverse? Well, it would just\\nremove the hat so that you get the\\noriginal person back. In other words, the inverse does the opposite\\nof what f does. It undoes what f does. In a more mathematical example, let's say you have\\nthe variable x and f puts on a hat on\\nit, so it squares it. Let's see. What\\nwould g have to do? Well, undo the square, so remove the hat, g is a function\\nthat takes as input x squared and returns x. Before we get to see\\nwhat f and g are, let's take a look\\nat some notation, g of x and f of x are inverses. We write this as g of x equals f inverse of x with an\\nexponent of minus 1. But this is not 1 over f of x. This is purely notation. If you take x and apply\\nf and then apply g, you get back the x\\nyou started with. Basically, g undoes what f does. What are f and g on the\\nexample on the left? Well, f is easy, it's simply x squared and g is the function\\nthat undoes the square, so it's basically\\nsquare root of x. The reason is that if you take x and square it and then\\ntake the square root, you get back x. This is only for x\\npositive numbers and also we're only taking the positive square\\nroot, not the negative. But in short, if\\nf does something, g has to undo it to get\\nyour first input back. Now the derivatives of\\ninverses are really nice. Let's take a look at the\\nplot of x squared and now the plot of g of y\\nequals square root of y. Notice that in this plot the units are different in\\nthe left and the right. Also in the plotting the left, the horizontal axis is x, and in the plotting the right,\\nthe horizontal axis is y. But in short, if a point\\nwith coordinates a,b appears on the left\\nthen the point with coordinates b,a\\nappears on the right. So for example, on the left, we have the point 1/2,1/4, on the right we have\\nthe point 1/4,1/2. This is because 1/4\\nis the square of 1/2 and 1/2 is the\\nsquare root of 1/4. The point basically f\\none-half is equal to one-quarter and g of\\n1/4 is equal to 1/2. The point 1,1 also\\nappears on both because f of 1 is 1\\nas 1 squared is 1, and g of 1 is 1 because\\nthe square root of 1 is 1. The point 3/2, 9/4 also appears on the left, and the point 9/4, 3/2\\nappears on the right. Similarly, point square root of 3,3 appears on the\\nleft and the point 3, square root of 3\\nappears on the right. Now I want you to take\\na careful look at the tangents of these points in the left plot and in the right plot and let's\\nquestion yourself. Do you think there's\\na connection between the slopes\\nof the tangent on the left and the slopes of\\nthe tangents of the right? Well, consider that the\\nplot on the right is the reflection of the plot on the left over the\\ndiagonal y equals x. There's going to be a\\nreally nice connection between these slopes. Let's take, for\\nexample, a secant. Before we draw a tangent,\\nlet's draw a secant. Let's take the point\\n1,1 which is in both plots and let's\\ntake the point 1.5, 2.25 on the left because\\n2.25 is the square of 1.5, and the point 2.25, 1.5 on the right. Let's calculate the slope\\nof this secant line. On the left, it's Delta\\nf divided by Delta x. The change in the\\nvertical coordinate divided by the change in\\nthe horizontal coordinate. On the right, this is going\\nto be Delta g over Delta y, where Delta g is the change in the vertical coordinate and Delta y is the change in\\nthe horizontal coordinate. However, these plots are a\\nreflection of the other. Therefore, any horizontal\\ndistance on the left is equal to the vertical distance on the right and vice versa. In other words, this Delta g\\nis the same as this Delta x and this Delta y is the same\\nas this Delta f. Therefore, the slope Delta g over\\nDelta y is the same as Delta x over Delta f. Now, notice that as\\nDelta x goes to 0, f prime of x is Delta\\nf over Delta x. Equally as Delta y goes to 0, then g prime of y is\\nDelta g over Delta y. Putting this together, we get that g prime of y is\\n1 over f prime of x. This looks complicated,\\nbut it only says that if f and g are\\ninverse functions, then the derivative of g is\\n1 over the derivative of f. Let's look at a simple example. This time we have two plots of x squared and\\nsquare root of y, where the actual units are going to be the\\nsame on the left, and on the right; each\\nsquare is one by one. Let's look at the tangents. At the point 1,1 we have that f of 1 is 1\\nand g of 1 is 1. The point 1,1 appears\\non both plots. Now, take a look at the\\ntangent on the left. Since f prime of x is 2x because it's the\\nderivative of x squared, then f prime of 1 is 2, which means that this orange\\nline has a slope of 2. Now let's take a look at\\nthe slope on the right, the tangent g prime of y\\nis what we want to find. We want to find the\\nslope of that tangent. Well, since g prime of 1\\nis 1 over f prime of 1, then this is 1/2,\\nwhich is a half. Therefore, a half is the slope of the\\ntangent on the right. It makes sense, right? If the slope of the\\ntangent on the left, the orange one is 2, then the slope of the\\ntangent on the right is 1/2 the reciprocal of 2. Now let's do another\\nexample, the point 2,4. On the left, we have the\\npoint 2,4 and on the right, we have the point\\n4,2 because f of 2 is 4 and g of 4 is 2. Now, the slope on\\nthe left, well, since the derivative\\nf prime of x is 2x, then f prime of 2 is 4, which is 2 times 2. What do you think this\\nslope is going to be? It's probably going to\\nbe 1/4 because it's 1 divided by the\\nslope on the left. And indeed it is, because g prime of 4\\nis 1 over f prime of 2, and that's 1/4. In other words, if f and\\ng are inverse functions, then the derivative of g is\\n1 over the derivative of x.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you've\\nlearned the sum rule, you're ready for\\nthe product rule. Which is almost the\\nsame thing except just a little bit\\nmore complicated. Imagine that I have a\\nfunction f and that's a product of functions g\\nand h. The question is, what is the derivative of f with respect to the\\nderivatives of g and h? Well, for this, I\\nlike to imagine a derivative as hitting the\\nfunction with the hammer. I want to hit f for the hammer and that means I need to\\nhit g and h with a hammer. But that's too much work, so I have to do it by parts. First, I have to hit g with\\na hammer and leave h alone, and then I have to hit h with\\na hammer and leave g alone. In other words,\\nhere's what happens. F prime is like hitting\\nf with a hammer. If I hit only g\\nwith a hammer and leave h alone and I have g prime h. If I hit h with a\\nhammer and leave me alone, I have gh prime. The derivative of f is\\nf prime equals gh prime plus g prime h.That\\nis the product rule. The way I like to\\nimagine the product rule is by building a house. Imagine that you're\\nbuilding a house and you have two groups of workers, one working on the front wall and one working\\non the side wall. They work at different paces, so at a given time they will have different\\nlengths of wall built. G of t represents the length of the\\nsidewall as a function of time and h of t represents the length on the front\\nwall as a function of time. What is the resulting\\narea of the house? Well, it's the product\\nof g of t and h of t. Let's call that f of t. F of t is the function of the area of a\\nhouse with respect to time. Now we want to find the\\nderivative of f and we know the derivatives of g\\nand h. In other words, we want the rate of change of the area with respect to time. Let's look at a\\nhouse from the top. Now what happens when\\nthe workers built a little bit of\\nsidewall Delta g and a little bit of\\nsidewall Delta h. That also builds a little bit\\nover here at the top. Now let's look at\\nwhat these areas are. This area over here\\nis going to be g of t times h of t. This\\nsmall area here, g of t times Delta h\\nof t. This one here is Delta g of t times h\\nof t. This tiny one is Delta g of t times Delta h of t. Now the rate of change\\nof the area with respect to time after some small time Delta t is going\\nto be Delta f of t divided by Delta t.\\nWhat is Delta f of t? Well, the change in area is\\nthis blue area over here, which is precisely the sum of\\nthe three blue rectangles. On top we have Delta g of th of t plus g of t Delta h of t plus Delta g of t Delta h of\\nt. On the bottom we have Delta t. We can rewrite\\nthis in the following way. Delta gt over Delta t times h of t plus Delta ht over\\nDelta t times g of t plus Delta g of t Delta h of t divided by Delta t. Now to take\\nthe derivative, we need to let\\nDelta t go to zero. What happens with Delta\\nf of t over Delta t? That becomes the\\nderivative of f prime of t. Each one of these becomes\\nsomething interesting. This one becomes g prime of th of t. This one becomes g of th prime of t. This one\\nover here becomes zero, because it's too\\ntiny for us to care. It really doesn't make a big difference because the limit of that is zero when\\nDelta t goes to zero. Therefore, we have\\nthe product rule. F prime of t is g\\nprime of t times h of t plus g of th prime of t.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Welcome to week one. In this week, you're going to learn the intuition\\nbehind the concept of the derivative. What's the derivative? Well, my favorite example of\\na derivative is velocity. So the first example you're going to\\nlearn is a speedometer example. Then you're going to see\\nseveral very important and very basic functions in mathematics, such\\nas a constant function, linear functions, quadratic polynomial, exponential\\nfunctions and logarithmic functions. And you're going to learn\\nthe derivative of those functions. Now, in order to find derivatives\\nof more complicated functions, we're going to use several rules such\\nas the sum rule, the product rule, the chain rule and\\nmultiplication by scalars. But here's a question\\nwhy are derivatives and calculus so important in machine learning? And one of these reasons is that\\nderivatives are used to optimize functions in particular to maximize them and\\nminimize them. That means finding the maximum value or\\nthe minimum value of a function. And this is very important\\nin the machine learning. The reason is that when you want to find\\nthe model that fits your data in the best possible way you do this by calculating\\na loss function and minimizing it. So we're going to see several\\nexamples of minimizing functions. The first example is going to\\nuse an example of temperature. And then we're going to see other\\nexamples that actually will introduce you to two of the most important\\nloss functions in machine learning. The square loss and the log loss. >> So\\nlet's begin by considering this problem. You have the following houses. Each one has a different\\nnumber of bedrooms. The first one has one bedroom and\\nthe second one has two bedrooms. Now, the first house costs $150,000 and the second $1, and in this problem\\nyou would like to be able to predict the price of a house using\\nthe number of bedrooms it has. So luckily you have a lot more\\ndata about the pricing of house, given the number of bedrooms\\nas is shown on this table. See in this table,\\nyou have houses with 1 2 3 5 6 7 8 and 10 bedrooms with their prices. And then there is this nine bedroom\\nhouse that is still missing its price. So you decide to build a machine\\nlearning model to help you predict the price of\\nthe house with nine bedrooms. Now, to make this easier,\\nlet's plot the data. In the following graph, the horizontal\\naxis represents the number of bedrooms and the vertical axis, the price of the house. And you can see that the houses\\nare here represented by dots. Now the idea is that the machine\\nlearning model takes in all the input data of the house prices and\\nbegins a process called model training. You can think of this face model\\ntraining as the most important part of running a car. The engine. Model training is what's under the hood\\nof any experiment and in this case, in fact it's a smart engine. Always looking for ways to optimize in\\norder to produce the best possible result. So when I say model, in this case, what I mean is simply a line that goes\\nas close as possible to all the points. And this line would represent\\nthe predicted price. So when model training begins\\nstarts with any random line. And the idea is that it tweaks\\nthe results in order to optimize the best possible prediction for\\nthe existing data points. And once training is done, the model is\\nable to give you a good prediction for the house with nine bedrooms. So in this case the house with\\nnine bedrooms is predicted to have a price of $950,000. This problem is called\\na linear regression problem. Now how does training work under the hood? Well you'll find out soon enough. But let's actually\\nconsider another example. Let's say that you travel\\nto a distant planet and you're lucky enough to\\nmeet some alien life. And you're able to interact with them. So the first alien says the following,\\naack, aack, aack. And let's say that you have enough\\ninformation to know that this alien is in a happy mood. Then let's say that another\\nalien says beep, beep. And you know that this alien is sad. Let's have more data points. An alien says aack, beep,\\naack and that alien is happy. And then it says aack beep beep beep and\\nit's sad. So the idea is that you don't necessarily\\nwant to learn the alien language because it may be too hard, but you want to be\\nable to tell if an alien is happy or sad. So let's look at the data set. So these are the four\\nsentences aack aack aack, beep beep, aack beep aack and\\naack beep beep beep. For each one of them you're\\ngoing to collect a number of times the word aack appears and\\nthe word beep appears and now you're also going to\\ncollect the mood of the alien. So this is going to be\\na classification problem. Why? Because given any sentence, the idea\\nis to classify it as happy or sad. And because you're classifying it a sad,\\nhappy or sad, this is called\\na sentiment analysis model. But everything works nicer with a plot. So let's make a plot of\\neach of the sentences. In the horizontal axis you have the number\\nof times the word aack appears and in the vertical axis you have\\nthe number of times beep appears. And what's a model this time? Well again, it's going to be a line,\\nlet's say this line, well this line doesn't do very well. The idea of the model is\\nto be able to classify or to split apart the happy points and\\nthe sad points. So for example, here's a better model and as you train the model, let's say,\\nit gets to an ideal point here, and this ideal point here splits\\nthe plane into two regions. The sad region here shown in green and\\nthe happy region here shown in orange. And the idea is that if a new sentence\\ncomes in, depending on which area it is the model will predict if\\nthe sentence is happy or sad. The model may make mistakes, but\\nfor the most part it will do well, at least based on how it\\ndid on this data set. So again, how does the model do this? Well, it turns out that there's a lot of\\nmathematics driving the process of your model, the training process. Some of the math concepts involved\\nare gradients, derivatives, optimization, loss and cost functions,\\ngradient descent and many, many more. So in this course, you're going to\\nlearn about model training, optimization and\\nhow it works at the core with mathematics. And in our senior example,\\nwe use several models, we use linear regression classification\\nexamples to motivate the problem. But the techniques and concepts you're\\ngoing to learn here can be applied to a wide range of other machine learning\\nalgorithms such as for example, neural networks\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Our goal is to be\\nable to calculate the derivatives of most of\\nthe functions that we know. Let's start with\\nthe simple ones. What are the simple ones? Constant functions, linear\\nfunctions, quadratics, polynomial functions, exponential and\\nlogarithmic functions, and the trigonometric functions. You're going to be glad\\nto know that these have very nice and\\nsimple derivatives. Let's start with lines. The simplest line is\\nactually a constant, so a horizontal line. This function is such that\\nthe height at every point, for example, x_0 and\\nx_1 is always the same, some constant c. The\\nc could be -173.5, any number as long as\\nis always the same. Now let's look, for example, at the derivative at x_0. This tangent line has the same\\nslope as the entire line. What is the slope? Well, the slope is\\nDelta y over Delta x. Now the change in y is always zero because y is always c, so it's always c minus c. Therefore the numerator\\nhere is zero, and the denominator is not zero if we take x_0\\ndifferent than x_1. So therefore, the\\nslope is always zero. We conclude that\\na horizontal line or a constant always has\\na derivative of zero. Now let's consider\\nsome generic line that is not horizontal. It can have the equation\\nf of x equals a_x plus b, where a is the slope and\\nb is the y-intercept. What is the derivative\\nfor this function? Well, this derivative\\nis very simple. Let's take a point,\\nfor example, this one. We're going to\\ncalculate it by taking any other point and\\nlooking at Delta x, looking at Delta\\ny and calculating the slope Delta y over Delta x. Now this is rise over run, so it's going to be a,\\nwhich is the slope. Notice that as the second point is closer to the first one, well, the tangent stays\\non the same slope for the same reason as what happened with\\nthe constant line. When you have a line,\\nthe tangent that every point has the same\\nslope as the entire line. This line has a slope of eight. Therefore, the equation\\nf of x equals a_x plus b of a line has a derivative\\nf prime of x of a. If you want to see\\nthe math happening, well, let's look at the points. This point has equation x, ax plus b. Those are\\nthe coordinates. This point has coordinates\\nx plus Delta x, a x plus Delta x b plus b. Therefore, when we\\ntake the slope, it's Delta y over Delta x. That's the difference between this number and this\\nnumber, the Delta y. That's a_x plus Delta x\\nplus b minus a_x plus b. The denominator is Delta x. So a lot of things\\ncancel out here. This a _x cancels\\nout with this a_x. This b cancels out with this b, and then we have a times\\nDelta x over Delta x, the two delta x's cancel\\nout and the slope is a. Therefore, the\\nderivative of the line with equation a x plus\\nb is the slope a.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Let's take a look\\nat another family of functions, the\\ntrigonometric ones. Let's start with\\nthe sine function. Let's consider a few\\nparticular points. Minus Pi over 2, Pi over 2, 0, and minus Pi. Let's look at the\\nslopes at this point. The slope at Pi over 2 is 0, the slope at minus\\nPi over 2 is 0, the slope at 0 is actually 1, and the slope at minus Pi, it's going to be minus 1, that's the one on the left. Now let's compare it with\\nthe function cosine. Now we're not going to\\nlook at the slopes, we're actually going\\nto look at the values. At Pi over 2, the value of cos is 0, at minus Pi over 2 is 0. At 0 it's 1 and at\\nminus Pi it's minus 1. Do you think that's a\\ncoincidence? Actually, it's not. It's always the case that\\nif f of x is sine of x, then the derivative f prime\\nof x is the cosine of x. Why would this happen? We'll take a look\\nat it in a minute, but it's a really,\\nreally nice property. Now, the same thing happens\\nin the opposite way, except with the minus sign. If this is cosine, let's take a look\\nat some slopes. At 0 minus Pi, Pi over 2 and minus Pi over 2. At 0, the slope is 0, at minus Pi the\\nslope is 0 again. At Pi over 2 it's minus 1, and at minus Pi over 2 it is 1 Now let's look at sine of x, but the values now. At 0 you get 0, at minus Pi you get 0. At Pi over 2 you get 1 and at minus Pi over 2\\nyou get a minus 1. It's almost the\\nsame, we just have to flip it with a sign. If f of x is cos x, then derivative is\\nnegative sine of x. Again, we'll see\\nwhy in a minute, but this is something\\nto remember. The derivative of\\nsine is cosine and the derivative of cosine\\nis negative sine. There's a nice geometric\\nreason why this works and take a look. Let's take a circle of\\nradius 1 and let's draw any radial line\\nand x is the angle between the horizontal\\naxis and the line. The projection\\nover the x-axis is cosine effects and\\nthe projection over the y-axis is sine of x. Now let's move the\\nangle x slightly by a quantity of Delta x. Now the projection on the\\nhorizontal axis is going to be cosine of x plus Delta x, and the projection over the vertical axis is going to\\nbe sine of x plus Delta x. Now let's focus on\\nthis orange triangle over here and let's actually\\nblow it up a little bit. This triangle has\\none base equal to negative Delta cosine of x, and the other side\\nis Delta sine of x. Now we can call the\\ntop angle Phi and the hypotenuse h. If you\\nrecall from trigonometry, there is a rule that states\\nthat given a right triangle, the cosine of one of\\nthe angles is equal to the adjacent side divided\\nby the hypotenuse. The adjacent side\\nhere is Delta sine x, while the hypotenuse\\nis h. Now given that Delta sine x equals h cos Phi. Similarly, there's\\na rule that states that the sine is equal to the opposite\\ndivided by the hypotenuse. In this triangle,\\nthe opposite side is negative Delta cos x, while the hypotenuse is again h. Therefore negative Delta\\ncos h is h sine of Theta. Now what happens as delta\\ngets smaller and smaller? Well, the hypotenuse of\\nthis triangle almost coincides with the\\narch of the circle and approaches Delta of x. The angle Phi approaches x\\nand with this new values, we get that Delta sine\\nx equals Delta x cos x, and negative cos x\\nequals Delta x sine x. Also Delta sine x divided by Delta x goes to the\\nderivative of sine x, and Delta cos x divided by Delta x is the\\nderivative of cosine of x. This actually confirms the\\nintuition you built before. Sine prime of x is cos x and cos prime of x\\nis negative sine x.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Let's talk quickly about\\nprogramming experience. This course is\\ndesigned to both provide you a theoretical\\nbackground in the mathematics underlying machine learning and show you\\nhow those concepts are applied in practice. That means you'll need\\nto do some programming. This course includes graded programming\\nassignments and ungraded programming labs that focus on the application of the\\nskills and concepts you're learning. These exercises are written in Python\\nand will appear as Jupyter notebooks, an interactive\\nweb-based interface that allows you to read, run, and edit those programs. You won't need to be an expert Python\\nprogrammer to be successful in the exercises, but you should be comfortable with\\nthe concepts that would typically be taught in an introductory\\nPython programming course. This includes things like\\ndifferent data types and data structures, control flow using conditional\\nstatements, loops and functions, as well as importing and\\nusing different Python libraries. You should be comfortable reading and\\nediting Python code using these concepts, writing and debugging your own code, and occasionally referring to\\nthe documentation of new packages. If you are a confident programmer\\nin another programming language, you should be just fine learning the\\nPython you need for this course as you go. If you're new to programming, however, I would recommend you take an introductory Python course before\\nyou begin this course. In the next reading item, you will find some great resources for\\nwhere to get started learning Python.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you know lines,\\nlet's take a look at a slightly more complicated\\nfunction, quadratics. The simplest quadratic\\nis this parabola with equation y\\nequals x squared. Let's look at the derivatives. Notice that to the\\nleft of the y-axis, these are negative\\nand to the right of the y-axis, these are positive. The formula for the derivative\\nis Delta f over Delta x, as Delta x goes to 0. Now, Delta f is the change\\nin y or the change in f, which is x plus Delta x\\nsquared minus x squared. Before we do this formally, let's take a look at an example. What happens when x equals 1, then y is x squared,\\nwhich is also 1? Let's take a look\\nat some secants, calculate their\\nslope, and with that, we calculate the slope\\nof the tangent line. Let's start with a Delta x of 1. That means an interval of\\nlength 1 horizontally. What is Delta f\\nwhen Delta x is 1? Well, it's the difference\\nbetween 4 and 1. 4 is 1 plus 1 squared\\nand 1 is 1 squared. This is 3 and what's the slope? Well, it's going to be 3\\nover 1 because Delta f is 3, Delta x is 1, and so 3 over 1 is 3, so the slope is 3. Now let's make the interval\\nsmaller and see what happens. If we made this interval of\\nlength 1 over 2 horizontally, then Delta x is 1 over 2. What is Delta y or Delta f? Well, it's going to be 1 plus 0.5 squared minus 1 squared, which is 2.25 minus 1, which is 1.25 and the slope\\nis 1.25 over one-half, and that is a value of 2.5. Now let's make an\\ninterval even smaller. Let's make it a length\\none-quarter horizontally. What is Delta f now? Well, Delta f is 1 plus 0.25\\nsquared minus 1 squared, which is 0.56 and the\\nslope is 0.56 divided by one-quarter and that is 2.25. Now let's continue making\\nthis interval smaller to see where the slope goes. It went from 3 to 2.5 to 2.25. Where do you think\\nit's going to go? Let's look at an interval\\nof length 1 over 8. The slope is going to be 2.125\\nand an interval of 1/16. The slope is 2.065.\\nLet's go extreme. Let's do an integral\\nof 1 over 1,000. The slope is going to be 2.001. It looks like the slope is going towards 2 and that's\\nactually the answer. The slope of the\\ntangent at 1 is 2. Notice that 2 is 2 times\\none and that's actually the formula because the\\nderivative of x squared is 2x. Let's actually\\ncalculate it formally. What's Delta f over Delta x? Well, Delta f, just like above, it's x plus Delta\\nx squared minus x squared divided by Delta x. Let's expand that and let's\\ncancel some things out. The x squared goes with\\nthe minus x squared, the Delta x goes\\nwith the delta x, and you get 2x plus Delta x. Now, what's the value\\nof 2x plus Delta x as Delta x goes to 0? Well, the 2x stays put\\nand this Delta x is 0. Therefore, the derivative df\\nover dx is going to be 2x. When f of x equals x squared, then the derivative is\\nf prime of x equals 2x.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous\\nvideo, you learned the exponential function e^x, where e is the Euler number and you learned the\\nproperty that it has, which is that the\\nderivative of e^x is e^x. In this video,\\nyou're going to use that and the inverse\\nfunction rule in order to calculate the derivative of the logarithmic function and you're going to see that\\nit's actually very simple. But before we get\\nthe derivatives, let's look at\\nwhat's a logarithm. Imagine that you want\\nto find a number such that e to that number equals 3. That number is precisely\\nthe logarithm of 3. That actually works\\nfor any number x. If I want to find\\nthe number such that e raised to\\nthat number is x, that number is logarithm of x. We also call them\\nnatural logarithm. You may see in other places, logarithms in other\\nbases such as 2 or 10, but in this course,\\nit's always going to be base e or the natural logarithm. Now, the fact that e\\nto logarithm of x is x means that the function f of\\nx equals e^x has an inverse, which is logarithm of y. F inverse of y is\\nthe logarithm of y because e to the\\nlogarithm of x is x and logarithm of e^y is y. Now let's plot them. On the left we have\\nthe plot of e^x and on the right we have the\\nplot of logarithm of y. As usual, the left plot has a horizontal axis x and the right plot has a\\nhorizontal axis y. Let's look at some points\\nin the plot left and their corresponding points in\\nthe plot in the right. These correspond to\\neach other because recall the fact that these\\nfunctions are inverses means that if we reflect one about the line with\\nequation y equals x, we get the other one\\nbecause logarithm of x is the inverse of e^x. Now, let's take a look\\nat the derivatives. We're going to use the result\\nfor inverses in order to calculate the derivative\\nof logarithm of x and you're going to see that\\nit's actually very simple. Let's take one\\npoint, for example, on the left we have 2, 7.39. Recall that 7.39 is e squared. That means that on the\\nplot on the right, we're going to have\\nthe point 7.39, 2, which is e squared, 2. The slope of this tangent is the one we're going\\nto find and we're going to use this one over here. Recall that the slope on the right is 1 over the slope on the left, that's\\nwhat we're going to use. The slope on the left is\\nf prime of x equals e^x, that's the derivative, because recall that\\nthe derivative of e^x is e^x, therefore, because x is 2, the slope here is f prime of 2, which is e^2 or e squared. That's precisely 7.39. The height at 2 is 7.39 and the slope of the tangent\\nat 2 is also 7.39. Now, what's the slope on\\nthe graph in the right? Well, because it's the\\nreciprocal of e squared, then it's 1 over e squared. This tangent, the\\nline on the right, the slope is 1 over e squared. Notice that e squared is y, therefore g prime of\\ny is simply 1 over y. Therefore, the derivative\\nof logarithm is 1 over y. Let's do this mathematically, using the result frame\\nversus d over dy of f inverse of y is\\n1 over f prime of x. Now x is f inverse of y, so let's actually\\nwrite it like this. If you take the logarithm\\nfunction and you take the derivative\\nwith respect to y, then you get 1 over f\\nprime of f inverse of y. Now, f prime is e^x and f inverse of y\\nis logarithm of y, so you get 1 over y. In other words,\\nthe derivative of logarithm of y is 1 over y.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Let's set our eyes in a very important function,\\nthe exponential function. For this, we need to\\ndefine Euler's number. Euler's number is quite a\\nspecial number of mathematics. It appears in many branches and it can be defined\\nin many ways. One of them is by the numerical\\nvalue 2.71828182, etc. The decimal never terminates because a number is irrational, which means it\\ncannot be expressed as a ratio between two integers. Another way to define it is using the expression\\n1 plus 1 over n to the n. Let's look at this expression for several\\nvalues of n. For 1, it's 1 plus 1 over 1 to\\nthe one which is two. For 10 is 1 plus a tenth\\nto the 10, which is 2.594. For 100 is 2.705. For 1,000 is 2.717. As you notice, the\\nmore we increase n, the more these converges\\nto a certain number. The actual number that this\\ngets closer and closer is e, 2.71828182, etc. That's how we're going to define the number e and e has a\\nvery particular property. If you consider the function\\nf of x equals e to the x, that function is\\nits own derivative. The derivative of f\\nof x is also f of x. That particular property\\nmakes sure that e appears in many places in science,\\nstatistics, probability, etc. Now, the way I like to define\\ne is using interest rates. Let's look at the\\nfollowing problem. Let's say that you're\\ntrying to find the best bank where\\nto put your money. Bank 1 says the following. Bank 1 says, I'm not going to give you any money for a year. But after a year, I will give you as interest,\\n100 percent of your money. That means all your\\nmoney once a year. Means if you put in\\n$5 today in a year, they give you another $5. That's pretty good, but\\nyou continue looking. Let's go to bank number 2. Bank number 2 says, well, I'm going to give you 50 percent of your money every six months. If you put in $10\\ntoday, in six months, you're going to have $15 because bank 2 gives you half of\\nyour money twice a year. Bank 3 is slightly different. This one says, I'm\\ngoing to give you 1/3 of your money\\nthree times a year, so 33 percent every four months. In summary, bank 1 gives you\\nall your money once a year, bank 2 gives you half\\nfor money twice a year, and bank 3 gives you a third of your money three times a year. The question is,\\nwhich bank is better? Let's take five seconds\\nfor you to figure out. If you need more\\ntime, feel free to pause the video and\\nthink about it. This is a very\\nimportant problem. The answer is that bank 3 is\\nthe best bank of the three. Why? Well, let's\\nsay you have $1. So bank 1 after a year\\ngives you another one, so you end up with $2. Now, the question is, what are you going to\\nend up with in bank 2 and what are you going\\nto end up with in bank 3? In other words, what's the\\npercentage that you end up getting in a year in\\nbank 2 and bank 3. Let's take a look. Bank 1, you have $1 now. In a year, you're going\\nto have your dollar plus the extra dollar that bank 1 gives you of\\ninterests, that's $2. We can express the two\\nas 1 plus 1 to the 1. That's going to be\\nimportant later. In bank 2, now, you have\\n$1 and in six months, you have $1 plus a half. In one year, you're going to have $8\\nplus a half of that. In six months, you end up with\\n1 plus 1/2, which is 1.5. In one year, you\\nend up with 2.25, which is 1 plus 1/2 squared is because you're adding\\nhalf of your money, which means you're\\nmultiplying it by 1 plus 1 half every time and 2.25 is bigger than two because\\nyour dollar got doubled. But this amount\\nof money that you earned actually earned\\nmore money later. This is called accrued\\ninterests into money that gains interests and then that\\ninterests gains interests. That's why you did better\\nwith bank 2 than with bank 1. Now, let's take a\\nlook at bank 3. In bank 3, you start\\nwith $1 right now. In four months, you have $1\\nplus 1/3, which is 1.33. In eight months,\\nyou have your 1 and 1/3 plus a 1/3 of that. That's 1.77. In one year you have 1.77 plus 1/3 of\\nthat, which is 2.37. Again, you did well\\nbecause this interest over here accrued interests\\nand accrued more interest. This interest over here\\naccrued more interests. Your money is making more money. At the beginning of\\nthe year, you had $1, then you have 1 plus 1/3, then you had 1 plus 1/3 squared, and then you had\\n1 plus 1/3 cube, and that becomes 237. In short, in bank 1 after\\none year, you have $2. In bank 2 after one\\nyear, you have 2.25. In bank 3 after one\\nyear, you have 2.37. That's why the money\\nis better in bank 3. This money actually\\nmultiplies a lot. Take a look at what happens\\nin bank 1 after four years, you're going to end up with $16. In bank 2 after four years, you're going to end\\nup with more money. You're going to end\\nup with $25.63. In bank 3, you're going to end up with quite a lot of money. You're going to end\\nup with $31.57. Definitely bank 3 is much\\nbetter than bank 1 and bank 2. Again, let's recall\\nthat after one year, in bank 1 you get\\n1 plus 1 to the 1, bank 2, you get 1 plus 1/2 to 2 and in bank 3 get 1\\nplus 1/3 to the 3. This is super important. Now, could you imagine a\\nbetter bank than these three? Well, of course, the more subdivisions\\nyou have, the better. Let's look at bank 12. Bank 12 gives you 1/12 of\\nyour money every month. You start with $1\\nnow, in a month, you have 1 plus\\n1/12, in two months, 1 plus 1/12 squared and\\nso on until in 12 months, you have 1 plus 1/12 to\\nthe 12th and that's $2.61. You did much better. In general, bank n, would have\\nn subdivisions. It subdivides the year into\\nn intervals and every time, it gives you one\\nnth of your money. You start with one\\non the first day. After first interval, 1 plus 1 over n. After the\\nsecond interval, 1 plus 1 over n squared. If this is not super\\nclear, let me elaborate. 1 plus 1 over n is the money you have and you add an nth of that. You have 1 plus 1\\nover n plus 1 over n, 1 plus 1 over n and that is the money you have plus\\nthe interests you accrue. That factors is 1\\nplus 1 over n square. After three intervals, you\\nhave 1 plus 1 over n cubed. After k intervals, you have 1 plus 1 over\\nn to the k. Eventually, after the end of the year, you have 1 plus 1 over n to\\nthe n. N can be really large, so you could potentially accrue money every second or\\nevery millisecond. In summary, bank n\\ndoes the following, it gives you 1 plus 1 over n to the n at\\nthe end of the year because it returns one nth of your money and times a year. Now imagine that you go\\naround looking for banks. There's bank 1, there's\\nbank 2, there's 3. Bank 4 gives you a quarter of your money\\nfour times a year. Bank 5 gives you a fifth\\nof your money five times a year and so on until\\nthe end of the universe. Here's bank 12 that gives you 1/12 of your money every month. Here's bank 365, that gives you 1/365 of your money every day. What's at the very\\nend of the years? What is this bank? Well, this bank, let's call\\nit bank infinity. What bank infinity does is it pretty much gives\\nyou instant interests. One infinity of your money\\ninfinity times a year. Let's see how much\\nmoney bank infinity would give you at\\nthe end of the year. Bank 1 ends up with\\n1 plus 1 to the 1. Bank 2 with 1 plus 1/2 to the 2. Bank 3 is 1 plus 1/3 to the 3 and 12 is 1 plus\\n1/12 to the 12. Bank 365 is 1 plus\\n1/365 to the 365, which already is 2.7145. Bank infinity gives you, well, this is not a good\\nmathematical expression, 1 plus 1 over infinity\\nto the infinity. But it's basically the number that all these numbers\\nconverge to 2.25, 2.37, 2.613, 2.7145, etc. Gets closer and closer\\nto a particular number. That number is 2.7182812\\nor the number e. So e is the amount of money that you get at the end of the\\nyear in bank infinity because bank infinity gives you an infinitesimal if your\\nmoney every single instant. That is one way to\\nlook at the number e.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Like any language, mathematics has\\nits way of expressing concepts. In this video, I'm going to show you\\ntwo ways to express the derivative. The first one is Leibniz's notation and\\nthe second one is Lagrange's notation. Let me show you how they work. So first recall that the slope was\\ncalculated as the change in distance over the change in time,\\nthat's delta x over delta t. And that the slope of a tangent\\nat a point was called dx over dt. Now, what was this dx and this dt? Well, in order to calculate the slope,\\nyou first started with a big interval calculated delta x over delta t,\\nthat's the slope of that secant. And then made the interval smaller and\\nsmaller, always calculating delta x over delta t. Until the interval was so\\ntiny that you get the tangent line, and this slope was called dx over dt. Now dx over dt represent infinitesimal\\nchanges in the x direction and in the t direction. So that's one way of writing\\nthe derivative as dx over dt. Now in general we're going to\\nhave X as the horizontal axis and Y as the vertical axis. So the derivative is going to\\nbe called dy over dx instead. So here's some notation, let's say\\nthat your function is called f(x), and that's y, so y is f(x). Now the derivative of f can be\\nexpressed in two ways, one is as f'(x). So recall the function is f and at the\\npoint x the slope of the line is f'(x), that is like Lagrange's notation. And the other one is the one you just saw,\\nwhich is dy over dx, that can also be expressed\\nas d over dx of f(x). In this case d over dx can be\\nthought of as an operator, something that when you apply\\nto f(x) you get something else, and this is Leibniz's notation. So sometimes in this course we're going to\\nuse Lagrange's notation, f'(x), and other times we're going to use dy over dx,\\nwhich is Leibniz's notation. And we're normally just going to\\npick the one that's more convenient.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"So now that you know the scalar\\nmultiplication rule, the sum rule is just as simple. Imagine that you have a function f and it's expressed as the sum\\nof two functions g and h. So what is the derivative of f? Well, it's simply the derivative\\nof g plus the derivative of h. I'd like to imagine the sum\\nrule in the following way. Imagine that there's a boat and\\nthere's a kid inside the boat. And the boat moves and\\nthe child also runs inside the boat. So the boat moves a distance let's say of\\n10, and the child moves a distance of 2. And we're going to keep\\ntrack with variables XB for the distance that the boat moves and\\nXC for the distance that the child moves. So if they're moving\\nin the same direction, what is the distance that the child\\nmoves with respect to the earth? Well, that distance is\\n12 because it's 10 + 2. It's the distance that the boat moves plus\\nthe distance that the child moves inside the boat. Now, that's with distances. But what happens with velocities? So let's do a quiz. Let's say that the speed of the boat\\nis 0.6 meters per second and the speed of the child inside\\nthe boat is 0.05 meter per second. And they're both traveling\\nin the same direction. So here is a question, what is the speed\\nof the child with respect to the earth? So if you answered 0.65 meters per\\nsecond then you are correct because velocities can be added if they\\nare taken in the same direction. And that's pretty much the sum rule. It says, if distances can be added\\nthen velocities can also be added, means that it functions can be added\\nthen their derivatives can be added. So let's look at it with the plot. Here is a plot of time in the horizontal\\naxis and distance in the vertical axis. And this is how the child\\nmoves inside the boat. This is how the boat moves. And this is the sum of the two which\\nis how the child moves with respect to the earth. So let's look at some slopes. Here is some point in time and we're\\ngoing to take some small interval delta t. And we're going to calculate the average\\nvelocity over that interval for the child, for the boat, and for the sun. So for this we take the horizontal\\nstep to be delta t and the vertical step to be the distance. So these distances are going to be XC for\\nchild, XB for boat, and X total for the sum. Now we know that X total is\\nXB + XC because these two distances add two X total. So if we divide everything\\nby the same delta t, we obtain that the velocities add. So V total is equal to VB for\\nthe boat plus VC for the child. And now all you need to do is let delta t\\ngo to zero and you get the derivatives. So if you have three functions f1,\\nf2 and f1 + f2, the derivative of f1 at\\na point is f1 one prime of x. The derivative of f2 at that\\nsame point x is f2 prime of x. And what's the derivative of the sum? Well, since f is f1 + f2, then f prime is f1 prime plus f2 prime. And the slope over here is going\\nto be the sum of the slopes. And that's the sum rule.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you know the\\nsum rule, product rule, and scalar rule, you are\\nready for the chain rule. It's probably the most\\ncomplicated in these rules, but if you look at it\\nfrom a certain angle, it can be pretty simple. Imagine that you have\\na function h of t and to output of this function you want to apply\\nanother function, g of t. You have a\\ncomposition of functions, and now you want to\\ntake the derivative of this composition\\nof functions with respect to t. Let's say, you know dh over dt, the derivative of h\\nwith respect to t. You also know the derivative\\nof g with respect to h. The derivative of the composition is simply\\nthe product of these two. Now why is it called\\nthe chain rule? Because you continue\\ncomposing functions. Let's say you want to\\napply another function, f to the output of g and you want to take\\nthis derivative, well, you need to know the\\nsame there before, in addition to df over dg, and the product is the\\ndesired derivative. Of course, as you can imagine,\\nyou can continue composing more and more functions and\\nyou're just going to get more terms in the product.\\nThat's the chain rule. Now, with the Leibniz notation, the chain rule is pretty clear. But with the Lagrange notation, there is something to keep track of because it can\\nbe a little tricky. Here it is again\\nfor two functions. Now, what is dh over dt\\nin Lagrange notation is simply h prime of t.\\nWhat is the dg over dh? Well, it would be lovely\\nif it was g prime of t, but it's not g prime of t. It's g prime of whatever you put into the function g.\\nIt's g prime of h of t, that's all we have\\nto keep track of, its g prime times h prime, except that you have to put the correct input\\nin the function. What happens with three\\nvariables? Well, the same thing. Dh over dt is h prime of t. Dg over dh is g prime\\nof the input to g, which is h of t. Df over dg\\nis f prime of the input to f, which is g of h of t.\\nThat is the chain rule. This is how I like\\nto see chain rule. Imagine that you're\\ndriving your car again and you'll\\nfeel of interests, so you're going to\\ngo up a mountain, the base of the\\nmountain is pretty hot, but the top is pretty cold. As you drive up, you notice that the temperature is changing with respect to height and the rate of\\nchange is dT over dh, where T keeps track of temperature and h\\nkeeps track of height. But also as time changes, you drive up the mountain. Therefore, the height changes\\nwith respect to time, and t keeps track of time. So the rate of change of height respect to time is dh over dt. Now notice that as time passes, you drive up and as you\\ndrive up it gets colder. Therefore, temperature also\\nchanges with respect to time and the rate of\\nchange is dT of the dt. When the chain rule says, is that you can find the third one from the\\nfirst and the second. If you know d temperature over d height and d\\nheight over d time, then their product is d\\ntemperature over d time, is pretty much which\\nchain rule says. But let's look at a plot\\nto understand this deeper. We're going to plot\\nin three-dimensions. One axis is going to be time, the other one height, and the other one temperature. In the bottom plane,\\nthere's the function h, which dictates how height change with respect\\nto temperature. In this plane over\\nhere at the back, it's the function t\\nthat keeps track of how temperature keeps changing\\nwith respect to height. Now, as time changes a\\nsmall amount Delta t, then height changes a\\nsmall amount Delta h, and then temperature\\nchanges a small amount Delta T and the way to\\nsee it is like this. Let's say here at\\nthis point in time. The bottom plot tells you that this point in\\ntime corresponds to this height and the top\\nplot tells you that this height corresponds\\nto this temperature. If you make a small\\nchange forwards in time, then now you're at a\\ndifferent time spot. That yields a higher altitude. That higher altitude\\nis changed by Delta h, a small amount of altitude. That Delta h creates\\na small change, Delta temperature, because now it gets colder\\nbecause you drove up, and so Delta t implies Delta\\nh implies Delta T. Now, these are very tiny numbers, Delta t, Delta h, and Delta T. This expression\\nholds Delta T over Delta t is Delta h over Delta t times Delta T\\nover Delta H. Again, these are numbers\\nso I can cancel Delta h on top and bottom. But now as I let\\nDelta t go to zero, Delta h also goes to zero\\nand Delta T goes to zero, and so this number in the\\nleft becomes dT over dt. This one over here\\nbecomes dT over dh, and this one over here\\nbecomes dh over dt. That's pretty much\\nthe chain rule.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you know\\nwhat a derivative is, let me show you a very\\ninteresting property of it. Let's go back to the table. But just as this\\nlittle piece between 19 seconds and 20 seconds, notice that the\\ndistance at 19 is 265 and the distance\\nof 20 is 265. The distance is the\\nsame at both points. It hits that perhaps the car didn't move\\nin that interval. Maybe it moved and\\nthen came back, but let's assume that\\nit didn't move at all. This is how the interval\\nbetween 19 and 20 looks. The line between these points is a horizontal line and a horizontal line\\nhas a slope of zero. Here's the calculation. The slope is the change in\\nx over the change in t. The change in x is the distance at 20\\nminus distance at 19. That is 265 minus 265. Now the change in t is 1 second. The slope is 0/1 because\\nthere is no rise in distance. Distance doesn't change. Therefore the slope is\\nzero meters per second. Now let's look at a\\ntrajectory of the car. We're going to plot\\nit on the right. First the cargo forward, then it stops, then\\nit goes backwards, and then it stops,\\ngoes forward again, and then backwards again\\nand then forwards again. Here is a quiz question for you. Given this trajectory, where was the velocity\\nof the car zero? If you said that at\\nany of the points where the tangent is horizontal, therefore, the\\ntangent has Slope 0. You were correct. Those are the points where the\\ncar was stopped. In total, they are five points. Now let me ask you\\nanother quiz question. At what time was the car farthest from its\\nstarting point? The answer is over here, because this is when\\nthe distance was 50 and that's the\\nhighest distance that you can find in this graph. Now notice something\\ninteresting. The point where the distance\\nwas the farthest is coincidentally one of the points where the car was stopped. This is no coincidence\\nbecause if the car is moving, then it could just go further. The point where the\\ncar is the farthest is a point where the\\ncar is not moving. That means if you want to find the maximum or the\\nminimum in a function, it occurs at one of the points where the derivative is zero. Or in other words, where the\\ntangent line is horizontal.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"So far, you've learned derivatives of some\\nsimple functions. In order to find the derivatives of more complicated functions, what you have to use this\\ncertain rules in order to piggyback from the simple ones into more complicated\\ncalculations. The rules you're\\ngoing to learn next, are multiplication by scalar, sum rule, product rule, and the chain rule for\\ncompositions of functions. So the first rule is\\nmultiplication by scalar. Let's say you have a\\nfunction f and it's four times a function g. If you want to take\\nthe derivative of f, that means is just going to\\nbe four times the derivative of g. That works for\\nevery value of four. If it's actually a constant\\nc can be any constant. Then if f is equal to c times g, then f prime is equal\\nto c times g prime. Let's take a look at\\nwhy this happens. Consider the function\\ny equals x squared drawn over here and let's\\ntake twice that function, so 2 x squared, therefore, the function\\ngets multiplied by two. Let's look at some points. On the left you\\nhave the point 1, 1 and the point 2, 4 because 2 squared is 4. The slope of the\\nsecant line, well, it's rise over run\\nwhere the run is one, which is 2 minus 1, and the rise is three\\nbecause that's 4 minus 1. This slope is 3 over\\n1, which is three. Now let's look at the\\ncorresponding points in the function in the right, which as we said, it's doubled\\nthe function in the left. The point corresponding\\nto 1, 1 is 1, 2 and the point\\ncorresponding to 2, 4 is 2, 8 and let's\\ncalculate the slope. Well, the run is the same as the run\\nin the previous one, which is 2 minus 1 equals 1. But the rise is double the previous one because\\non the plot on the right, everything gets doubled in\\nthe vertical direction. The 4 minus 1 equals\\n3 on the left, which is the right, becomes an 8 minus 2\\nequals 6 on the right and so the slope multiplies\\nby 2 because the numerator multiples by 2 and the denominator\\nstayed the same. Therefore, if the\\nfunction multiplies by 2, all these slopes get\\nmultiplied by 2. Now, if you take the limit, if you let the point on the right move towards\\nthe point in the left, you still have that all\\nthe slopes multiply by 2. Therefore, the derivative or\\nthe slope of the tangent, which over here is two as we saw before, now becomes four. In other words, if we take a function and multiply it by 2, the derivative gets\\nmultiplied by 2. This happens for every value. For example if we have\\na function like this, and we multiply it by c, which means the height on the right is c times\\nthe height on the left. That every point, then\\nthe derivative gets multiplied by c. If we\\nhave a point over here, x, y with slope f prime of\\nx for the tangent line. On the right, the point corresponding to\\nthat is going to be xcy and the slope is going\\nto be c times f prime of x. Because if you think about it, the plot on the right\\nis obtained from the plot left by basically\\nputting your hands on top and bottom and\\nstretching it by a factor of c. That is the multiplication\\nby scalar rule.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"When I think of a derivative the first thing that comes\\nto my mind is velocity. Let's say for example,\\nthat you're in a car and you go 100\\nkilometers in one hour. That means your\\naverage velocity was 100 kilometers per hour. However, maybe you started fast, then you went slow,\\nthen you stopped, then you went really fast, then maybe went reverse at some point so this\\nvelocity is not constant. The question is, at\\na given instant, what is the velocity\\nat that instant? That's called the\\ninstantaneous velocity. That is precisely\\nwhat a derivative is. A derivative is the\\ninstantaneous rate of change of a function. In this case, the function is distance and its\\nderivative is velocity. Let's add some numbers\\nto this example to understand the concept of\\nthe derivative more clearly. Imagine here that you are\\nin the car traveling on a straight road and you have a speedometer that\\ntells you the velocity. Now remember from\\nphysics that speed equals distance traveled\\ndivided by the time elapsed. Unfortunately, the\\nspeedometer breaks. But now you have an app that tells you the distance\\nyou have traveled, and you also happen to have\\na calculator in your hand. You'd drive for one\\nminute and you're able to generate\\na table of values showing the distance\\nyou traveled every five seconds\\nfor one minute. Now here is the first quiz. The question is, is the car\\nmoving at constant speed? The hint is take a look at the distance values\\nin the table. The car is not moving\\nat constant speed. You can see that between\\n10 and 15 seconds, which is a five-second interval, the car traveled 80 meters, but between 15 and 20 seconds, which is also a\\nfive-second interval, the car traveled 63 meters. This tells us that\\nfrom 10-15 seconds, the car was traveling at\\na faster average velocity than it was between\\n15 and 20 seconds. We can also conclude\\nthat the speed of the car is not constant across\\nthe one-minute interval. Great. You've determined that the car is not moving\\nat constant speed. Now here is the second quiz. Can you use this information to tell your velocity at\\ntime 12.5 seconds? The hint is that velocity equals distance traveled\\ndivided by time taken. Again, the quiz is,\\ncan you or can you not use this information\\nto determine the exact velocity at 12.5\\nseconds? The answer is no. You cannot find the\\nvelocity at time 12.5 seconds with the\\ndata given on this table, you can find the\\naverage velocity in the interval from 10-15, but you don't know what happened within that interval at 12.5, maybe you were going faster, maybe you are going slower\\nthan the average velocity. However, there is one\\nthing you can find. You can find the\\naverage velocity in the interval\\nfrom 10-15 seconds. That is the next quiz. What was the average\\nvelocity of the car in the interval from 10-15 seconds? Let's draw a graph\\nfor the points in the table to observe\\nwhat's going on. Let's zoom specifically\\nto the points 10, 122 and 15, 202 to find the slope at a point because the slope is going to be the\\naverage velocity. Here is the graph and\\nhere is the interval. The average velocity\\nbetween the time interval 10-15 seconds is\\nalso the slope of the line that joins the\\ntwo points in the graph. Now velocity is calculated with the formula, distance over time. This is synonymous to the\\nformula for calculating slope, which is rise over run. This is the rise and\\nthis is the run. In this case, the\\nrise is the change in distance and the run\\nis the change in time. This is because distance is on the vertical axis and time\\nin the horizontal axis. The distance traveled at\\n15 seconds is 202 meters, and the distance traveled at\\n10 seconds is 122 meters. The length of the\\ntime interval is five and the length of the\\ndistance interval is 80. Using the formula for\\nslope or velocity, we get 80 divided by 5\\nequals 16 meters per second. The slope of the line that\\npasses these two points is 16, which also translates to\\nthe velocity of the car between those two points\\nis 16 meters per second. Now while the average velocity\\nbetween 10-15 seconds was a good estimate for the velocity at t equals 12.5 seconds, could we do better? Yes, we can if we had more\\ndata about the distances of time close to t\\nequals 12.5 seconds. Let's say that we take more refined measurements,\\none every second. Here you have a lot more data about the distances you\\ntraveled every second and a specific distance you\\ncovered from time t equals 10 all the\\nway to t equals 20. The data is shown in\\nthis table over here. Here is another quiz. Can you find a better\\nestimate of the velocity of the car at time t equals 12.5\\nseconds using this data. Once again, we are\\nnot able to find the exact velocity of\\nthe car at 12.5 seconds. However, we have a way of making an even better guess than the previous one for\\nwhat this velocity is by taking the average\\nvelocity between the interval of time between\\n12 seconds and 13 seconds. How do we calculate this? Well, if the slope is the change in distance over the\\nchange in time, that's the velocity\\nof the interval, then this is the distance\\nat 13 minus the distance of 12 divided by the time\\nat 13 minus the time at 12. Distance at 13 is 170, distance at 12 is 155 and the time difference between\\n13 and 12 is one second. We get a slope of 15\\nmeters per second. That's the velocity\\nfor that interval. That is a much\\nbetter estimate for the instant velocity\\nat t equals 12.5. Now notice that we still don't have the estimate of\\nthe velocity at 12.5. What would you do to\\nfind this estimate? Well, you can just take finer and finer intervals\\nand the finer the intervals, the better the estimate is. That leads to the derivative\\nwhich is coming next.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous videos, you learned\\nabout some common functions and their derivatives. However, it's not always the case that you\\ncan find the derivative of a function at every point. For these functions where you cannot find\\nthe derivative at every point are called non differentiable functions. So in this video, I'm going to show you\\nseveral non differentiable functions and how they can be identified visually. So far, you've learned that to find the\\nderivative of a function at a point, you draw a tangent line to the point on that\\nline and find the slope to the tangent. So for example, at this point over here,\\nyou would say that a function is differential by the point if\\nthe derivative exists for that point, that means if you can draw a unique\\ntangent to the curve at that point. So for this curve for any point you pick, you can draw a tangent and\\nit's well defined. And we also say that for a function to\\nbe differential at an entire interval, then the derivative has to exist for\\nevery single point in that interval. However, some functions\\ndon't satisfy this property. We call them non differentiable functions. Let's take a look at one. For example,\\nthe function absolute value of x. What's the absolute value of x? Well, it's kind of the positive part of x. So absolute value of 7 is 7, but\\nthe absolute value of -7 is 7, can be defined like this. It's x for x bigger than or\\nequal to 0, and -x for x less than 0. So this function is differentiable\\nalmost everywhere, except for this point in the origin. Because if you try to draw a tangent,\\nwell, it's not very well defined. All of this would work and\\nit's not a well defined tangent. So we say that at x = 0,\\nthe derivative does not exist. And this always happens when\\na function has a corner or a cusp, if that's the case, the function is\\nnot differentiable at that point. That means if you're drawing\\nit with a pencil and at some point you have to stop and\\ndraw a corner, then you have a function that's\\nnot differentiable at that point. So here is a quiz. If I give you this function, can you tell me at which point is\\nthis function not differentiable? So at which point does\\nthe derivative not exist? And so if you said that the derivative\\ndoes not exist at x equals 1, then you were correct, because here\\nwe can draw multiple lines and neither one of them is well defined\\nas the derivative at that point. So we say that the entire function\\nis not differentiable because a derivative does not exist for\\nevery single point in the domain. Now, consider this function. The function is defined as 2 for\\nanything smaller than- 1 and for anything bigger than -1 is this diagonal\\nhere defined by y equals x + 1. So at which point in this function\\ndoes the derivative not exist? This is a step function or\\na piecewise function. It has a jump discontinuity\\nat x equals -1. Over here,\\nas I mentioned is defined as y = 2. And over here is y = x + 1. So we say that at the point -1\\nthere is a jump discontinuity. If you try to draw this function with a\\npencil, what's going to happen is that you have to at some point lift the pencil and\\nstart drawing it somewhere else. So the function is not continuous and\\na function that is not continuous, that has a jump like this is not\\ndifferentiable because the derivative cannot be defined at this point. What line could you draw\\nhere that was the tangent? That is not well defined. So in general, any function that has a discontinuity\\nis not differentiable at that point. And now let's do one more. This one is a little more subtle. Let's say we have the function\\nf of x = x to the one-third. It looks to be differential\\nat every point. But what happens at the origin? The tangent is parallel\\nto the vertical axis. It's a vertical tangent and that is\\nnot good, at x = 0, this has a tangent line that runs straight up to the y-axis\\nand that means it's not differentiable. Why? Well, because a vertical line\\ndoesn't have a well defined slope, it would be some number over 0, because\\nthe rise is any number, but the run is 0. So it's not well defined. One could say it's infinity or\\nsay it's undefined. But the fact is that a function with\\na vertical tangent is not differentiable. So as a small recap, there are three\\ntypes of non differentiable functions. Anything with a cusp or a corner,\\nanything with a jump discontinuity, and anything with a vertical tangent. So corners, cusps are bad,\\njump discontinuities are bad, and vertical tangents are bad because these\\nare all non differentiable functions.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous video, you learned something\\nvery interesting, the Euler's number e.\\nOne very important property of the number\\ne is that if you take the function y equals e^x, this is called the\\nexponential function, and this function is\\nits own derivative. In other words, if\\nyou take a point x, then the point in the\\ngraph is the point x, e^x reference at\\nthat height is e^x. Well, if you take the\\ntangent at that point, the slope of that\\ntangent is actually e^x. So e^x is equal\\nto its own slope. Now let's verify this numerically by taking\\nsome measurements. Here is y equals e^x again. We're going to\\ncalculate the slope of the secant lines in the same way that you've done it before. Let's take for example\\nthe number x equals 2. The point in the graph is 2, 7.39 because 7.39 is e square. Now let's take\\nsome intervals and calculate the slopes\\nof this sequence. For example, if we move\\nto the right one unit, then Delta x is 1, and Delta f is just going\\nto be e^3 minus e^2. That's exactly 12.7. The slope is 12.7 divided\\nby 1, which is 12.7. Now let's reduce the\\ninterval to length one-half. The Delta f is now 4.79 because it's e^2 plus 0.5 minus e^2. Delta x is one-half. The slope is 4.79 divided\\nby a half, which is 9.59. If we do it for 1/4\\nthen Delta f is 2.1 and the slope is 8.39. We do it for an 1/8, then the slope is\\ngoing to be 7.87. For 1/16 it's 7.62. For 1/1000, it's\\ngoing to be 7.39. As you can see, these numbers approach a particular number. The number that it\\napproaches is actually e squared, 7.39. He would verify numerically\\nthat the slope of the tangent line at the point to e squared is\\nprecisely e squared. This is what happens\\nat every point. The tangent at the point x, e^x has slope e^x. That's a fascinating property\\nof the function e^x.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you know,\\nline some quadratics, let's go to cubics. The simplest cubic is the\\nfunction y equals x cubed, seen over here, and here are some of the\\nslopes of the tangents. Again, the formula is\\nvery similar from before. It's x plus Delta x\\ncubed minus x cubed divided by Delta x. Then as delta x goes to zero, then this value becomes the slope of the tangent\\nline or the derivative. Let's do a small example\\nwhere x is equal to 0.5. When x is 0.5, then y is 1/8, because 1/2 cubed is 1/8. Let's take some intervals and calculate the slope\\nof the secant lines. If Delta x is 1, then we have the point 3/2, which is 1/2 plus 1 and 27/8. Why 27/8? Because 1/2 plus 1\\ncubed is 27 or 8. We subtract 1/2 cubed\\nand we get 3.25. The slope of this\\nsecant line is going to be 3.25 divide 1,\\nso that's 3.25. Now, let's decrease\\nthe interval to length 1/2 and do the\\nsame calculation. X plus Delta x cubed\\nis 1/2 plus 1/2 cubed, which is 1 minus 1/2 cubed, which is 1/8, so we\\nhave 7/8 which is 0.86. The slope is 0.86\\ndivided by 1/2, which is precisely 1.75. Now, let's go even smaller. Let's go for 1/4. I'll do the\\ncalculations quickly, but I'd invite you to do them on pen and paper if you'd like. Delta f here is 0.3 and\\nthe slope is 1.188. For 1/8, then Delta f is 0.12 and the slope is 0.12 divided by\\n1/8, which is 0.95. For 1/16, we have\\na slope of 0.85. For 1/1000, we have\\na slope of 0.752. As you see, this values\\ntend to converge to 0.75. That's actually the value\\nf prime of 0.5 is 0.75. That's actually 3\\ntimes 0.5 squared because the derivative of x\\ncubed is actually 3x squared. But let's actually calculate it formally to see\\nthat this is the case. Delta f over Delta x\\nis going to be x plus Delta x cubed minus x\\ncubed as the change in y direction in the vertical direction\\ndivided by Delta x, which is changed the\\nhorizontal direction. Now expanding a cube is a little harder than\\nexpanding a square, but this is what you get. If you cancel out the x cubed, if you divide top and\\nbottom by Delta x, then you get 3x Delta x plus 3x squared plus Delta x squared. Now, as Delta x goes to zero, what happens with 3x Delta x plus 3x squared plus\\nDelta x squared? Well, anything that has\\na Delta x becomes zero. The only thing that stays put is the 3x squared and\\nthat's the derivative. If fx equals x cubed, then f prime of x equals 3x squared and that's the\\nderivative of this cubic.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"In the previous video, you\\nlearned how to calculate the average velocity\\nof an interval, but what is the instantaneous\\nvelocity at a point? Let's look at the point t equals 12.5 that you were\\nlooking at before. Calculating the instantaneous\\nvelocity here may be hard, but estimating it is possible. For example, let's take another\\npoint to the right of t equals 12.5 and calculate the average velocity\\nover that interval. That is the slope of\\nthis line over here, which is exactly Delta x, the change in distance\\ndivided by Delta t, the change in time. But that is not\\nthe instantaneous velocity at t equals 12.5. However, you can\\nget closer to it by making this interval\\nsmaller, for example, take now this point over here\\ncloser to t equals 12.5 and the average velocity\\nof that interval is the new Delta x over\\nthe new Delta t, which is a slope\\nof these new line. Now let's put the\\npoint even closer to 12.5 and even closer. Now imagine putting it so close that you can't even tell the distance\\nbetween the two. You get the limit which\\nis dx over dt and that is precisely the tangent line to\\nthe curve at t equals 12.5. This measure of how\\nfast the distance is changing with respect\\nto time is called the instantaneous rate\\nof change and it is the slope of that tangent line. More generally, the\\ninstantaneous rate of change is a measure of how fast the relation between two variables is\\nchanging at any point. In other words, imagine that\\nyou move a tiny distance, dx in a tiny\\ninterval of time dt. This is the instantaneous\\nrate of change. It's also called the derivative. The derivative of a\\nfunction at a point is precisely the slope of the tangent at that\\nparticular point.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Now that you know lines\\nand some polynomials, let's look at a\\ncomplicated one, 1/x. This is a hyperbola\\nthat looks like this. Let's take a look at some\\nof the tangent lines. The function looks\\nmore complicated, but actually the\\ncalculation is quite simple for the\\ncalculation when we do x plus Delta x inverse\\nminus x inverse divided by Delta x. Let's look at the point x\\nequals 1 and y equals 1. Let's take some interval\\njust like before. If the interval is of\\nlength 1 horizontally, Delta x equals 1, then what is Delta f? Well, Delta f is going to be the change in the\\nvertical direction, so x plus Delta x inverse minus\\nx inverse for x equals 1. That means it's 1/2 minus 1. That's minus 0.5. The slope is minus\\n0.5 divided by 1, which means that the\\nslope here is minus 0.5. Now let's decrease the\\nlength of the interval. Let's make it so\\nthat Delta x is 1/2. If Delta x is 1/2, then what's delta f? Well, is 1 plus 1/2\\ninverse minus 1 inverse, which is 2/3 minus 1, which is minus 1/3. Delta x is simply\\ngoing to be 1/2. The slope is 0.33 divide by 0.5, which is minus\\n0.67 or minus 2/3. Now let's do some\\nmore and again, I'll go a little faster\\nto the calculations. For 1/4, the slope is minus 0.8, for 1/8 is minus 0.89, for 1/16 is minus 0.94, and for 1/1000 it's minus 0.999. As you can see, this\\nconverges to minus 1. The derivative here is minus 1. This is negative 1\\ntimes 1 squared, which is actually the value. Because when f of x\\nequals x inverse, then the derivative is\\nminus 1x to the minus 2. Let's actually do the\\ncalculation to confirm this. Delta f over Delta is going to be x plus Delta x\\ninverse minus x inverse divided by Delta x. Let's actually write this\\nas one over x plus Delta x minus 1 over x over Delta x. Let's expand the one on\\nthe top as x minus Delta x plus x divided by x\\nplus Delta x times x. Some things cancel out, this plus x and this\\nminus x cancel out. We can multiply and\\ndivide by Delta x and we get minus 1 over x\\nsquared plus Delta x. Now as Delta x goes to 0, well, the one on top stays put, the x^2 stays put, but this goes to 0. Therefore, we have 1\\nminus 1 over x squared. If f of x equals x inverse, then f prime of x equals minus\\n1 times x to the minus 2. Let's take a look at what\\nwe've learned so far. The derivative of x squared, x cubed and x inverse, we've already calculated\\nthe first one is 2x^1, the second one is 3x squared, and the third one is\\nminus 1x to the minus 2. Now, can you spot a pattern? These three actually follow\\na really nice pattern. That works for any\\npower function. Take a look at the exponent. The exponent goes here in the derivative as a\\nmultiplication factor. Then you subtract one from the exponent and you\\nget the new exponent. So x squared becomes\\n2x to the minus 1, x cubed becomes 3x\\nsquared, and x inverse, so x to the minus 1 becomes\\nminus 1x to the minus 2. What do you think\\nhappens with x^n? Well, with x to the n, the exponent of n\\ncomes down and it becomes nx to the new\\nexponent is n minus 1, so nx^n minus 1. If you have any power function, for example, x^100, the\\nderivative is 100x^99. If it's x to the minus 100, the derivative is minus\\n100x to the minus 101.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Welcome to this second course in\\nthis specialization on math for machine learning and data science. In this second course,\\nyou learn about calculus. So derivatives, what does that mean,\\nhow does it work, how is this used in machine learning? And you also learn about some advanced\\ntechniques like Newton's method. I'm thrilled that once again we have\\nas our instructor for this course, Luis Serrano. >> Thank very much. Yes, I'm very excited about this course. Machine learning is a lot of\\nminimizing and maximizing functions and that's the main application\\nof this course. And for that we use derivatives. >> In fact, almost everything\\nwe do in machine learning. So the large fraction of what we do in\\nmachine learning is create a cost function and then minimize it. And that's calculus. >> Exactly.\\nWhen you want to train a machine learning model, you calculate how bad it is with\\nthe error and try to minimize that. And so for that, we use derivatives,\\nwe use gradients, and we teach them a lot of techniques. >> Yeah, so often those few lines of\\ncode implementing gradient descent or some other algorithm that\\nis actually using calculus, that someone had derived a derivative and\\nthen implementing that in code. So understanding that seems central\\nto having good intuitions about how the algorithm are doing. >> Absolutely, yeah. If you want to understand\\nhow these optimizers work, that they normally look at black box,\\nif you want to understand them, it's good to know the calculus behind. And at the end of the day,\\nyou're just doing something step by step. >> Right.\\n>> Because gradient descent is just taking small steps to get lower and\\nlower and lower values. And in what direction\\ndo you take that step? Well, the gradient tells\\nyou exactly the direction. >> So one thing that sometimes seem\\nmysterious to people learning this for the first time is when you think\\nabout derivatives in one dimension or two dimensions, that's like the slope. But in machine learning, we often take\\nderivatives in very, very high dimensions, like 10,000 or a million dimensional\\nspace is really hard to visualize. So in this course, you also give people\\nintuitions about what's going on in these very high dimensional spaces. And when you do calculus in these\\nvery high dimensional spaces. >> Absolutely. So just like you saw in course one,\\ndata can be very high dimensional. And when you talk about the derivative,\\nit could be in many directions. And these are captured by long vectors and\\nlong matrices. So we even go for like second derivative. A second derivative would tell you what\\nthe curvature of the space is based on the matrix that comes out\\nof the second derivative. >> And the derivative is derivative. That also leads into the Newton's method. >> Yeah, Newton's method is a really\\ncool method to minimize functions. So gradient descent actually works well. But many times gradient descent\\ncould take hundreds of steps and Newton's method could take just\\na few steps to get the same results. >> Yeah.\\nDoesn't work for all applications. Many ones with not too many parameters. But when it works,\\nit can be blazingly fast. >> Yes.\\nAnd the idea is for you to have a toolkit of\\ndifferent techniques. And if this one works, that's fantastic. But if not, you try this one or\\nthat one, or that one. And that's how we do it in\\nmachine learning, right? >> And in fact, in terms of toolkits,\\none of the cool things about this specialization is also all the notebooks,\\nall the labs. >> Absolutely. So just like in the previous course,\\nyou'll be able to have a lot of code labs where you can see this\\nconcepts working in real time. >> So in this course, you learn\\nthe concepts, run code labs, and hopefully even come away with code snippets that\\nyou can use in your own projects. So I'm excited to have you dive\\ninto this second course and start learning these important\\nconcepts of calculus. So with that,\\nplease go on to the next video.\",\n",
       "  'machine learning::machine learning calculus::01 derivatives and optimization'),\n",
       " (\"Congratulations, you have finished\\nthe calculus course, in this course, you've learned a lot of stuff. First you learn derivatives in one\\nvariable, then you learn gradients and derivatives in multiple variables. You learn how to calculate them and you learn how to use them\\nto optimize problems. When things got complicated, you learn\\nmethods such as gradient descent and Newton's method to speed things up,\\nand most importantly, you learn how to apply all these\\noptimization methods to machine learning. I am so excited you've taken this step\\nwith us and I'm even more excited for all the next steps you're going to be\\ntaking in your machine learning journey.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"In this and the following videos\\nI'm going to show you an alternate method to gradient descent\\ncalled Newton's method. Newton's method is also very fast and\\nvery powerful, in principle Newton's method is used\\nto find the zeros of a function. However, we can adapt it a bit\\nto be used for optimization. Newton's method works in\\nmany variables as well, but let's start by looking at how\\nit works in one variable. So let's say you have this function over\\nhere and your goal is to find the zero. So find this point over here, that's\\nthe point where F of X is equal to zero. So here's a pretty good method to\\napproximate this zero really well that works similar to gradient descent. Let's start with some random point X zero,\\nthat's not the zero of the function, but that's okay,\\nwe're going to get a better one from here. Let's take the tangent at this point,\\nso let's take the derivative and draw the tangent and\\nsee where it hits the horizontal X axis. And at this point we're\\ngoing to call it X1, notice that X1 is much\\ncloser to zero than X0. So all we need to do is iterate over here,\\nso let's do it one more time. Draw the derivative and\\nfind a new point X2, notice how close X2 is from\\nthe zero of the function. So we pretty much almost got there by the\\nthird point where very close to the zero. So we pretty much found it, we didn't\\nexactly found it, but we got pretty close. And that's the point,\\nto be able to approximate the zero, that function really well. Now let's add some numbers, this line\\nover here has slope F prime of X0. And this height over here is F of X0 and the base is X0 minus X1. So this formula over here for\\nthe slope is, rise over run, now rises FX0 and\\nrun is X0 minus X1, so therefore F prime of X0 is\\nFX0 divided by 0 minus X1. We can manipulate this to\\nget that X1 is equal to X0 minus F of X0 divided by F prime of X0. And that's the iterative step,\\nyou have F zero, you simply calculate X1 based on X0 and\\nthen iterate, so how do we iterate? Well, let's go one more step, let's say that you've done it K times and\\nyou've got 2XK. This slope over here is F prime of XK,\\nthis height is F of XK and this horizontal distances\\nXK minus XK plus 1. Therefore doing the same\\nmethods before escape plus one is equal to X K minus FXK\\ndivided by a prime of XK. And since you know, XK, you know F and\\nyou know the derivative, then you can find XK plus 1. So you iterate on the step and you get\\nNewton's method, now the question is, how do you use that for optimization? Because remember gradient descent\\nfound the minimum of a function, Newton's method is only finding the zero\\nof a function, not the minimum. Well, we simply remember that when you\\nwant to minimize the function G of X, you actually have to find\\nthe zeros of G prime of X. So if I can find the zeros of a function,\\nI'm very close to minimizing it. I just have to look at all the zeros and see which ones are minimum because\\nthose are the candidates for minimum. So in other words, if I let F of\\nX be the derivative of G of X, then by finding the zeros of F of X,\\nI am minimizing G prime of X. And the derivative of F prime of X,\\nis simply the derivative of G of X and the derivative of that. So in other words,\\nhere are the algorithms, for Newton's method we start with some X0. Now, if I wanted to find the zero of F,\\nwhat I have to do is update and the step is XK plus 1 is XK minus\\nF of XK over F prime of XK, where XK is my K iteration and\\nXK plus 1 is my K plus 1 iteration. Now in Newton's method for\\noptimization, I have to do the same thing except not with F, but\\nwith G prime, so I have XK plus 1. The K plus 1 iteration is\\nXK minus G prime of XK, divided by G prime of XK prime,\\nthe second derivative. So that's going to be a big deal\\nvery soon, but before that, let's do the iterative step. The iterative step is repeat 2,\\nuntil you find the root, on the left and on the right is repeat 2,\\nuntil you find the minimum.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"In the previous video, you\\nlearned the second derivative and how it's useful in\\noptimization problems. That was all in one variable. Next I'm going to\\nshow you how it works in more than one variable. For multiple variables, the second derivative\\nis actually a matrix full of second derivatives\\ncalled the Hessian. Now I'm going to show\\nyou the Hessian, its properties, and how\\nit's used in optimization. In particular, when we want to optimize a function\\nof many variables, we can use multivariable\\nNewton's method, and that one uses the Hessian. Let's start with a little\\ncomparison between functions of one and two variables. Recall that a function of one\\nvariable is called f of x, and it only depends\\non the variable x, whereas a function of\\ntwo variables depends on variables x and y. The first derivative is\\nsimply f prime x which is the rate of change of f with respect to the only variable x. When you have two variables, you have two rates of change. The rate of change of f with\\nrespect to x called f_x, and the rate of change of f\\nwith respect to y called f_y. If you put them together, you\\nget the gradient nabla f, which is a vector. Now what happens with\\nthe second derivative? You have x prime prime for the second derivative\\nof one variable. That's the rate of\\nchange of the rate of change of effects. For two variables, well, let's see what we're\\ngoing to have. This is what I'm going to\\nshow you in this video. To understand the second\\nderivative in two variables, let's analyze this\\nsimple example f of x, y) equals 2x squared plus\\n3y squared minus xy. We can take the\\nderivative respect to x, which is 4x minus y, and with respect to y, which is 6y minus x. Now we can take the\\nsecond derivative. We can take the derivative\\nwith respect to x, which is 4. We can take the derivative of 4x minus y with respect to y, and that's minus 1. For the second one, 6y minus x, we can take the derivative\\nwith respect to x and with respect to y, and we get minus 1 and 6. These four are the rate of\\nchange of the rate of change. It's the rate of change\\nof f_x with respect to x, of f_x with respect to y, and then f_y with respect to x and of f_y with respect to y. Notice something peculiar. These two are the same thing. That happens almost all the time but let's summarize\\nthis a little bit. The first two are the rate of change with respect to x\\nand then with respect to x, and also with respect to\\ny and with respect to y. These work exactly\\nlike in one variable, because pretty much if you have f as a function of only x, you can take f prime prime of x, and the same thing with y. However, the last two\\nare a bit peculiar. That's the change\\nin the slope along one coordinate axis with respect to tiny changes along the\\northogonal coordinate axis. These two are actually\\nthe same in most cases. You need both partial derivatives\\nto be differentiable. When both partial derivatives\\nare differentiable, then the derivative\\nwith respect to x of the derivative respect to y is the same as the\\nderivative with respect to y of the derivative\\nwith respect to x. In Leibniz's notation,\\nwe can write this as d squared f over dx squared, d squared x over dy squared. For the other two,\\nd squared f over dxdy and d squared f over dydx. Lagrange notation, we get f_xx, f_yy, f_xy, and f_yx. Now, let me show you what\\nthe Hessian matrix is. Let's go back to this\\nlittle graph over here where we took the derivative with\\nrespect to x and y, and then the partial\\nderivatives with respect to x and y of each one of these. When we put these four\\ntogether in a matrix, we get the Hessian matrix. The Hessian matrix here is 4, negative 1, negative 1, 6. The Hessian matrix gives us a lot of information\\nabout second derivatives. It also is very useful in\\noptimization as we'll see soon. In the general case, we\\nhave our function f, the partial derivatives\\nf_x and f_y. They're partial\\nderivatives f_xx, f_xy, f_yx and f_yy. When we put these\\ntogether in a matrix, we get the Hessian\\nmatrix just like before. As I mentioned before,\\nthere's a lot of information in this\\nHessian matrix. Now let's go back to our table. In our table, we have that\\nthe second derivative for one variable is simply\\nf prime prime of x. What is it in two variables?\\nIt's the Hessian. It's the matrix that keeps track of all the second\\npartial derivatives.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"So now that you know Newton's method,\\nlet's put it in action. I want to recall this function you saw\\nin Week 2 when you learn about gradient descent. The function was e to the x\\nminus logarithm of x. And its derivative was\\ne to the x -1 over x. Recall that the minimum for this function\\nwas the omega constant, 0.5671 etc. And so let's approximate this minimum. Recall that in order to use\\nNewton's method for a minimum, you actually have to use it to\\nfind the zeros of the derivative. So we're going to try to find\\nthe 0 of this function which is the derivative of e to the x -1 over x. Now what's the derivative\\nof the derivative? Well, if we take the derivative of\\ne to the x is again into the x. And the derivative of 1 of x\\nis minus 1 over x squared. So the derivative of the derivative\\nis e to the x plus 1 over x squared. That's going to be our f prime of x when\\nwe're taking f of x equals g prime of x. In other words,\\nour new function to find the 0 is f of x. And we're going to use f prime of\\nx to help us with Newton's method. So let's start, let's do a few iterations. We start with the iteration,\\nwith the first value x0 equals 0.05. That's the first iteration. Now let's take the derivative and\\nsee where it hits the 0. So it's at a point x1 not too far from it. x1 is given by this formula using\\nNewton's method we evaluate the derivative at e to\\nthe 0.05 minus 1 over 0.05. That's the derivative divided\\nby e to the 0.5 plus 1 over 0.05 squared, and we get 0.97. So that's our new point x1. So with x1 we're going to iterate again. We draw the tangent see where it hits and\\nthat's at a point x2. How do we calculate x2? By plugging in these values. So x2 is going to be 0.183. And again we take the derivative, see\\nwhere it hits 0 and that's our point x3. How do we calculate x3? Using the derivative again. And the value for x3 is 0.320. We're getting closer,\\nlet's do it one more time. With x3 we draw the derivative. We see where it hits the axis and\\nwe get x4. And to calculate x4 we use the iterative\\nformula again to get 0.477. Notice how close we're getting\\nas to one more iteration with x4 we draw the derivative,\\nwe see where it hits. That's at a point x5. And x5 is going to be calculated like\\nthis and the value is going to be 0.558. And let's just do one more. We draw the derivative again,\\nwe see where it hits and it's at x6. And at x6 is going to be 0.567. And notice that with only six iterations, we actually got really close to\\nthe minimum, which is 0.567. Newton's method is actually really,\\nreally fast, and this example shows it. And as you can see, it's just a simple\\nstep that we can iterate many times.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Let's conclude this week\\nby learning how to apply Newton's method to optimize\\nfunctions of many variables. And as you guessed it,\\nthe Hessian appears here. >> So remember that at the beginning of\\nthis lesson, we saw Newton's method and found an expression for literally finding\\nthe minimum or maximum out of a function. For one variable, the updated equation\\nhas this expression that depended on the current value, the first derivative,\\nand the second derivative. We could just as easily rewrite this\\nlike this, where instead of dividing by f prime prime of xk you multiplied by\\nthe inverse of f prime prime of xk. Now, what happens in 2 variables? Well, in two variables,\\nyou don't have one coordinate, you have two coordinates x and y. So your kth iteration is xk, yk, and\\nyour k + 1 iteration is xk + 1, yk + 1. And how do you obtain the k + 1\\niteration from the kth iteration? Well, let's look at the expression above. This is the second derivative,\\nturns into the Hessian. And this is the first derivative\\nturns into the gradient, and notice that the second derivative is\\ninverse because you're dividing by it. Well, that simply turns into\\nthe inverse of the Hessian matrix. So, this is the expression for\\ntwo variables. And you can see that actually\\nthis works for n variables, I simply have to do a vector of n\\ncoordinates minus an m by n matrix, the Hessian inverse times a vector of\\nn coordinates, which is the gradient. And I have to be very careful. It's tempting to write\\nit like this as well. But this actually won't work. You cannot multiply the gradient\\nat the left of the matrix. You have to multiply at the right. And the reason is that the matrix is two\\nby two and the vector is two one, and you cannot multiply two by one\\nvector to the left of the matrix. You can only multiply it at the right. So we're working with two variables and\\nthe order is pretty crucial. Now, it's a lot of work to\\nactually prove this formula, but we're going to just take it as is and\\nnotice that it actually is a sensible generalization\\nof the one variable case. And now that you know Newton's method for\\nmore variables, let's put it in practice. Here is a function that is concave up, and\\nwe're going to try to find the minimum. So first,\\nlet's take a look at the two partial derivatives with respect to x and y. And then the partial derivatives\\nof those with respect to x and y, and with respect to x and y. So the Hessian is obtained with\\nthese four expressions at the right, and the gradient is formed by\\nthe two expressions in the middle. In other words, here's the gradient and\\nhere is the Hessian. So now, using the Hessian and\\nthe gradient, you're going to use Newton's method. So let's start at some point,\\nlet's say the point x0, y0 equals 4, 4. Here is the gradient at that point. So what we did is in\\nthe expression of the gradient, we simply put the numbers 4 and 4,\\nand now let's find the Hessian. So now notice at this 4,\\n4 is not the 0 of the function, but we're going to get closer and\\ncloser using Newton's method. So what's the first point? Well, the second iteration\\nis going to be 4, 4, the first iteration, minus\\nthe Hessian inverse times the vector. And that's actually going to be 2.58,\\n2.62, and that's the new point. Notice that it's much,\\nmuch closer to the 0, 0 which is the root\\nthat we're looking for. Now, let's iterate again. So we have that, this is the gradient and\\nthat this is the Hessian and the second iteration is this point minus\\nthe Hessian inverse minus the gradient. So we get the 1.59, 1.67. And let's actually continue repeating it. And when you repeat it many times\\nyou end up in the actual zero. So it takes eight steps to get this close,\\ncheck it out. The eighth iteration is 4.15\\ntimes 10 to the minus 17 and minus 2.05 times 10 to the minus 17. This is an awfully small number. Is really, really, really close to 0,\\n0, and 0, 0 is actually optimal point you were looking because\\nthat is the root of the function. So as you can see,\\njust like with one variable, Newton's method in two variables is\\na really, really fast method to be able to approximate really\\nclosely the zero of a function.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"In the previous video, you learned how\\nthe derivative of a derivative can appear naturally in Newton's method. This derivative of a derivative\\nactually has a name. The second derivative, in this video, I'm\\ngoing to show you the second derivative some of its properties and how it can be\\nreally useful in optimization problems. So previously you learn how to use\\nNewton's method for optimization and remember that this was the iterative\\nstep in order to find the minimum or the maximum of the function. You need to find the zeros\\nof the derivative, at some point you use this thing,\\nthe derivative of the derivative. So that's new, that's the second\\nderivative and it's very, very useful. It gives us a lot of\\ninformation about the function, Leibniz notation that can be written\\nas d squared f over dx squared or as d by dx df(x)/dx\\nLagrange is much simpler. If f prime is the derivative, then the\\nsecond derivative is f prime prime of x. Now, I like to understand\\nthings in real life and remember that to understand the first\\nderivative, we use velocity because if x is the distance traveled\\nby a car, then v is velocity. And v is the rate of change of distance\\nwith respect to time or dx/dt. Well, in this case, what's the rate of\\nchange of velocity with respect to time? It's simply the acceleration,\\nacceleration is dv /dt or d square x over dt squared\\nacceleration tells us how much the velocity is changing\\nwith respect to time. So if your acceleration is positive means\\nyour velocity is increasing with respect to time. And if the acceleration is negative\\nmeans the velocity is decreasing with respect to time. And if you have zero acceleration\\nmeans you have constant velocity. So now let's do some plots,\\nimagine that you're driving your car and this is the plot. So in the horizontal\\naxis you have time and in the vertical axis you have distance. And let's say that the plot\\nis this one over here. So let's analyze it bit by bit and let's see how it translates\\nin terms of velocity. So at the start of your trip, your car\\nis stopped and you need to start it and increase your velocity until\\nyou reach a constant speed. So the slope keeps getting\\nbigger as time goes by. Now imagine that this piece of function\\ncan be modeled as 750t squared. And let's say that this behavior\\ngoes on for 2 minutes or 2/60 hours. Now the derivatives for these first\\nminutes is the line 1500 t, why? Because the derivative of\\n750 t squared is 1500 t. Now let's say that for the next couple of\\nminutes you keep going at a steady pace, so you have the same rate of change. Now let's say that this can be modeled\\nby the equation 50t- 5/6, that's a line. A linear equation. Now, since the distance is linear, then you've reached a constant\\nvelocity of 50 kilometers per hours. Why, because the derivative 50t- 5/6\\nwith respect to t is simply 50. Now, let's say that you keep going for\\n5 minutes until you realize that you forgot your water home and\\nyou actually need to go back. So at this point you start\\npressing the brakes and the brakes make you slow\\ndown until you stop. And when you stop, you have a derivative\\nof zero and notice that you've reached the maximum because this is as far as you\\ngot after this, you start going back. So you start turning around and\\ngoing at a negative speed. And I noticed that the slope starts\\npositive, but keeps getting smaller and smaller until the richest zero at\\nthe peak value of the distance. And finally it gets negative when you\\nstart going in the opposite direction. So this one over here is the graph\\nof the speed, you started positive, then you reach zero when you stopped and\\nthen you started going negative because you started going in\\nthe opposite direction as before. Let's say that the expression for the distance was this one minus 1500\\nt squared plus 300 t minus 11.26. So, what's the equation for the line? Well, is the derivative of this,\\nwhich is 300- 3000 t. It's a linear equation. Now, let's say that after\\n2 minutes of going back, you finally set at a constant speed\\nuntil you reach the starting point. So what happens is that over\\nhere you have a constant speed. And let's say the distance\\nfunction is 55/6- 50 t. Therefore the velocity is -50. So now that we have the graph of\\nvelocity based on the graph of distance, let's find the graph of acceleration\\nbased on the graph of velocity. So for your first 2 minutes your speed\\nis aligned that means your derivative is a constant which is a value 1500\\nbecause you have a constant acceleration. Then your speed increases\\nlinearly now between two and five minutes when you travel at constant\\nspeed, the acceleration is simply zero. When you're traveling constant\\nspeed you're not accelerating. There's no change in velocity\\nnow between 5 and 7 minutes, your speed is again in line. But now with negative slope because\\nyour deceleration is -3000 or in other words your acceleration is\\n-3000 because you're decelerating. And finally for the final stretch\\nyou have an acceleration of zero again because you're going\\nat a constant speed of -50. So when we put the first graph and\\nthe third graph together we go from x to a from distance\\nall the way to acceleration. And as you can see for the first 2\\nminutes, when the distance increases at an increasing rate, you have\\na positive acceleration between 2 and 5 minutes when you travel at a constant\\npace, the acceleration is zero. After six minutes to keep going\\nforward but slower and slower and between seven minutes you're going back. So for these transitions you\\nexperience a negative acceleration. Notice that at the maximum distance travel\\nyou had a negative acceleration meaning a negative secondary. This is very important. And finally, until the end of your trip, your acceleration become zero since\\nyou're again going at a steady pace. So as you can see, the second derivative\\ngives a measure of the amount by which a curve deviates from\\nbeing a straight line. This is called the curvature. Let me get a bit more into the curvature\\nwhen you have a positive second derivative, as in the first piece of the\\ndistance you have a concave up function. This is also a convex function, in this function as you can see\\nthe second derivative is positive and the function is increasing\\nat an increasing rate. You're increasing the speed and therefore your distance increases more and\\nmore each time. Over here you have the opposite. Your function is concave down and that happens when the second derivative\\nis negative for the other two, the second derivative is zero for\\nthis, it's inconclusive. So you have no curvature. Now let me get a bit more into curvature. This over here is a concave up function, it looks like a happy face and\\nthis one over here, it's a concave down function,\\nit looks like a sad face. And the way to tell is when\\nthe second derivative is positive, you have a concave up convex function. And when the second\\nderivative is negative, you have a concave down function. And how is this useful for optimization? Well, the second derivative tells you\\nif something is a maximum or a minimum. Remember that the points of derivative\\nzero are the candidates for maximum or minimum, but\\nwe don't know if their maximum minimum. So the second derivative tells us,\\ntake a look at the first one. This one over here has\\na derivative of zero and the second derivative is positive and\\nit's a local minimum. However, this one over here\\nis a local maximum and the second derivative is negative. However, for the other two, it's\\ninconclusive when the second derivative zero, we don't know if we're at\\nthe presence of a maximum or a minimum. Now there's something very\\ninteresting that happens. And is that the first derivative\\ntells us one thing, and the second derivative tells us another\\nthing, when the first derivative is positive, we have an increasing function and when it's\\nnegative, we have a decreasing function. However, when the second derivative\\nis positive, we have concave up and when the secondary is negative,\\nwe have concave down. And as you can see when you\\nhave a concave up point, the point with derivative,\\nzero is a local minimum. Whereas when you have a concave down, the point where the derivative\\nis zero is a local maximum.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Now let me tell you\\none information that the Hessian bring in\\nterms of optimization. Remember that for\\nfunctions of one variable, several things could happen. If the second derivative\\nwas positive, then you have a function\\nthat is concave up, in which case, there\\nwas a local minimum. If the second derivative\\nwas negative, then you had a function that was concave down and so we\\nwould have a local maximum. The case where the second\\nderivative was zero, then it was inconclusive. You didn't know very much. The same thing happens\\nfor many variables, except you have to keep track\\nof a couple more things. What does concave up mean\\nin two or more variables? It means it looks\\nlike this function, 2x squared plus 3y\\nsquared minus xy. Visually, you can tell that the minimum is at this\\npoint over here which corresponds to x equals\\n0 and y equals 0. By analogy, with the\\none variable case, this looks like a happy face. But can we show it numerically? Well, let's look\\ninto the Hessian. Remember, in one variable, if we wanted to know\\nif it was concave up, we looked at the\\nsecond derivative and checked if it was positive. But now we have a matrix. How do we check if a matrix\\nis positive or negative? You actually look\\nat the eigenvalues. Let's calculate the eigenvalue. If you recall from the\\nlinear algebra class, the way to calculate them\\nis by taking a look at the equation determinant\\nof Hessian minus Lambda I, which is this matrix over here, and the determinant is this, and that is this quadratic. When we find the roots\\nof this quadratic, they are 6.41 and 3.59. Notice that they\\nare both positive. For a matrix, this is the equivalent of being\\na positive number. It means the matrix\\nis positive definite. Because all the eigenvalues of this matrix are\\npositive numbers, then the function is concave up and the 0.00 is a minimum. Now, let's look at a\\nvery similar function. This one is minus 2x squared minus 3y squared\\nminus xy plus 15. This one looks like it has\\na maximum point here at 00. Now, we're going to look\\nat the Hessian matrix. Here is the gradient, and here is the Hessian at 0, 0. Now, let's look at the\\neigenvalues of the Hessian. We're going to solve the\\nsame equation as before. The determinant of H minus\\nLambda I, we factor it, and we get the eigenvalues\\nminus 3.49 and minus 6.41. Both of them are\\nnegative so that is like saying that the\\nmatrix is negative. It's actually called\\nnegative definite. When a matrix has all the\\neigenvalues negative, then it's concave down, and therefore the\\n0.00 is a maximum. Now, not every matrix has all the eigenvalues positive or all the eigenvalues negative, something like this can\\nhappen: a saddle point. This is the function\\n2x squared minus 2y squared, which\\nis a saddle point. It looks like a saddle\\nor a potato chip. Over here, well, this point is neither a maximum\\nor a minimum. Now, let's take a\\nlook at the Hessian. This over here is the\\ngradient 4x minus 4y. The Hessian at 0,0 is\\nthis matrix over here. 4, 0, 0, minus 4. What are the eigenvalues here? If we solve for\\nthe determinant of H minus Lambda I equals 0, we get that the eigenvalues\\nare minus 4 and 4. In this matrix, not all\\nthe eigenvalues are positive and not all the\\neigenvalues are negative. Therefore is neither positive definite nor negative definite. So we cannot conclude anything. In this case, we have that because this eigenvalue is negative and the other\\none is positive, then 0, 0 is actually\\na saddle point; a point that is neither\\na minimum nor a maximum. Here is the summary. We're going to have\\none-variable functions, two-variable functions, and\\nfunctions in n variables. What happens with\\na local minimum? In one variable, you have\\nto have a happy face. F prime prime of x\\nis bigger than zero. Obviously, f prime of x has to be zero for this to happen. In two variables,\\nyou have to have an upper paraboloid like the concave-up function\\nyou saw in this video. For that to happen,\\nboth eigenvalues have to be positive\\nof the Hessian. In the general case,\\nyou simply have to have a Hessian with\\nn eigenvalues, all of them bigger than zero. For a local maxima, same thing, you have a sad face\\nin one variable, so the second\\nderivative is negative. For the two-variable case, you have a down paraboloid like the other function you\\nsaw in this video, and both eigenvalues\\nhave to be negative. In the general case,\\nyou simply have to have a Hessian with n eigenvalues, all of them less than zero. For anything else, we\\nneed more information. In one variable case, we have that the second\\nderivative is zero, we don't know if it's a\\nlocal maximum or minimum. It could be either one or none. In the two-variable case, we have a saddle point with one eigenvalue bigger than zero and one smaller than zero. In the general case, you simply have that\\nsome are positive, some are negative,\\nsome are zero, and those cases\\nare inconclusive. Let me repeat, if all of\\nthem are strictly positive, you have a local minimum. If all of them are\\nstrictly negative, you have a local maximum. If anything happens, if\\none of them is zero, or if some of them are positive, some of them are negative,\\nanything like that, then you don't know\\nwhat you have, at least with this test.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Now we know what the problem is. We want to find the prediction function with\\nthe best weights and bias that minimizes the loss\\nfunction L y, y hat. In other words, we want\\nto find the best model that makes the\\nsmallest mistakes. Now, how do we find\\nthis optimum values? We're going to minimize L\\nusing gradient descent. We're going to find\\nthe best w_1, w_2, and b using gradient descent. Recall that the gradient descent\\nformula updates w_1 this way in order to minimize L. Recall that Alpha is\\nthe learning rate, and then this is how it updates w_2 and this is\\nhow it updates b. In other words, you have\\nsome w_1, w_2, and b, some starting values and then all you need\\nto do is calculate these three gradients and you can update them\\nand get new w_1, w_2, and b that are better model because it has\\na smaller loss function. If you do this for a long time, you can imagine that you're\\ngoing to get it ended up with pretty good weights that have a very low loss function and\\nthat implies a good model. Now I'm going to tell you how to calculate these derivatives, there's going be a\\nlot of chain rule. On the left we have\\nthe functions for reference and on the right I'm going to calculate\\nthe partial derivatives. First of all, what\\nis dL over db? Well, take a look at L, L depends on the variable y\\nhat so we need to do dL over dy hat and then y hat depends\\non b so dy hat over db. This is nothing more than the chain rule that\\nwe learned last week. The same thing happens\\nfor dL over dw_1. L depends on y hat so we\\nhave dL over dy hat and y hat depends on w_1 so\\ntimes dy hat over dw_1. The same thing for dL over dw_2, dL over dy hat as L depends on y hat times dy hat over dw_2. Now we know what we have to do and all we have\\nto do is calculate these four derivatives\\nbecause notice that dL over dy hat repeats\\nthree times so we need to calculate\\nthat one and then the three partial derivatives\\nof y hat respect to w_1, w_2, and b. These are much easier than actually plugging\\nin the whole thing. It's breaking the problem into\\neasier to solve problems. Let's calculate each one\\nof these separately. Let's start with dL over dy hat. This is the\\nderivative we have to calculate with respect to y hat. Using the chain rule, this is 1/2 times something squared and its derivative\\nis the something, y minus y hat. But we still need something more because we have to\\ntake the derivative of the inside with respect to\\ny hat and the derivative of y minus y hat with respect to y hat is simply negative 1. We have that minus\\none over there. The first derivative is simply\\nnegative y minus y hat. We could write it as\\nnegative y plus y hat, but we're going to\\nkeep it like this because it's going to make\\nthe math easier later. Now let's move to\\nthe other three, which are much easier actually. What's dy hat by db? Well, notice that this is a\\nconstant with respect to b, has nothing to do with b. Really, it's going to be 1 because it's\\nthe derivative of the remaining which is\\nb by db so that's a 1. What about the next\\none? dy hat over dw_1. Well, notice that this part is a constant\\nwith respect to w_1. This may as well be w_1x_1 plus 7 and its derivative\\nis simply x_1, which is the constant\\nthat accompanies w_1. Similarly, dy hat over dw_2, this part is a constant, so it's just x_2, the constant accompany w_2. Now that we have these ones, let's plug them back in. I'm going to put them here\\non the right as reference. Now let's remember the chain\\nrules we already calculated dL over db is dL over dy hat, which is minus y minus y\\nhat, times dy hat over db, which is 1 so that's\\nthe first derivative. For the second one, dL over dw_1 it's the usual minus\\ny minus y hat, which is dL over dy hat times dy hat over\\ndw_1, which is x_1. For the final one, dL over dw_2 it's going to\\nbe the usual dL over dy hat negative y\\nminus y hat times dy hat over dw_2, which is x_2. These are our three derivatives. Now let's go back and\\nremember our main goal. Recall that the main goal was to find the optimal\\nvalues of w_1, w_2, and b that gives us the predictions y had with\\nthe smallest possible error. We're going to use a\\ntool that we developed last week, gradient descent. Recall that gradient\\ndescent uses these updating formulas\\nto get the new w_1. You start with some\\nw_1 and update it as w_1 minus learning rate\\ntimes the derivative of the loss for we want to\\nminimize with respect to w_1. Now what is this? We\\nalready calculated it. It's going to be negative\\nx_1 times y minus y hat. For w_2, again, this is the gradient\\ndescent step and we know this derivative we already\\ncalculated it we get this. For b, b is going to be updated\\nas b minus Alpha dL over db which has calculated\\nas minus y minus y hat. This is the gradient\\ndescent step. If we do this many times, we will get some\\nreally good weights, w_1, w_2, and b, that\\nare going to have a really small error and\\ntherefore a really good model.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Now that you know\\nwhat a perceptron is, you're just one step from knowing what a\\nneural network is. A neural network is just\\na bunch of perceptrons organized in layers where the information of\\nthese perceptions is passed to the next layer. How to train these\\nneural networks using gradient descent,\\njust like before. Except now we have\\nmany derivatives. We have to take\\nthe derivative of the log loss with respect to each one of the weights and biases of the neural network.\\nLet me show you how. Remember in the previous\\nlesson you worked a binary classification\\nproblem where you had to do sentiment analysis. Use this perceptron shown here, where z was the summation of the previous variables time\\nmultiplied by the weights and then Sigmoid set was the\\nactivation function that got you to find the\\nprediction y hat. Now, turns out that\\nthese models are pretty simple and they\\ncan only do so well. If you remember, what\\nthis thing does, is it builds a linear boundary, a line between the\\nhappy and sad sentence. But maybe language is much\\nmore complicated than that. Maybe it needs\\nboundaries that are much more complex and\\nnot just a line. For that, we use, not just one perceptron, but several perceptrons\\nput together. That's called a neural network. Here we have one perceptron, the red perceptron\\non top and then we put a green perceptron\\non the bottom. Now we have two perceptions that perhaps do\\ndifferent things. What do we do with the outputs\\nof these two preceptons? We plug them into\\nanother perceptron, the purple perception. That is a neural network. You can imagine that a very complex neural\\nnetwork, we have many, many perceptrons\\nplugged into others, and the outputs of\\nthose are plugged into others and then you have\\nmany, many, many layers. You'll be able to solve\\nvery complex problems. For now, we're going to\\ndo it with two layers, the one of the red-green, and the purple one, so that we can actually\\nhandle the math properly. Here is the math. We're going to have weights. Here we're going to\\nhave weights w11, w21, and the bias b1 that goes\\ninto the red perceptron, and the sum, as usual, is z_1 and then you apply Sigmoid z_1 and you're\\ngoing to call that a_1. A_1 is what comes out\\nof the red preceptron. Now, what comes out of\\nthe green perceptron? Similar thing, w_12\\nare the weights, w_22 the bias is b_2. Z_2 is the usual summation\\nof the weights times the features plus the\\nbias and when you apply Sigmoid to\\nthis, you get a_2. A_1 and a_2 are the inputs\\nto the purple perceptron. The weights here\\nare w_1 and w_2, and then z is obtained as a_1, w_1 plus a_2, w_2, and then you obtain z and you apply the\\nSigmoid function to this to obtain y hat. This is a neural\\nnetwork of depth 2 that has one input\\nlayer over here, one hidden layer over here, and the output layer. But as I said, you can\\nimagine neural networks with lots and lots and\\nlots of inputs. However, there's one\\nthing missing, right? Perceptrons normally have a bias and the purple perceptron doesn't have a bias.\\nThat's no problem. We'll just put the bias here and we'll call the\\nbias weight b. Now we have a full\\nneural network. Let's study the\\ninner workings of the red node in\\nour hidden layer. Focusing on this node alone, we've established that a_1, its output is given by\\nthis Sigmoid of z_1, and z_1 is given by\\nthe usual combination of weights and\\ninputs and the bias. So x_1, w_11 plus\\nx_2, w_21 plus b1_. The same thing happens\\nwith the green perceptron. A_2 is given as Sigmoid\\nof z_2 where z_2 is the usual summation of the weights times the\\nfeatures plus the bias. Now, let's go to the\\npurple perceptron. For the purple perceptron, the output is y hat\\nand it's Sigmoid of z. Z is again the summation of a_1 w_1 plus a_2, w_2 plus b. The new inputs to the\\npurple perceptron are the outputs of the green and red\\nperceptrons so a_1 and a_2, and that gets multiplied by\\nthe weights w_1 and w_2, and b gets added to that. That is the output of\\nthe neural network. As a reminder, here\\nare all the variables. Here's a full picture of\\nthe neural network with the inner workings and the math behind each one of the nodes. Now since we're dealing with\\na classification problem, let's not forget that\\nthe error function is the log loss that we saw in our earlier problem to\\nhelp us measure the error, that is l, y, y hat equals negative\\nlogarithm of y hat minus 1 minus y-hat logarithm\\nof one minus y hat, where y hat is the prediction and y is the target in the training data\\nthat we want to hit. Now, that we know how a\\nneural network works, let's get to training them.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"So now that you know how to calculate the\\nderivative of the log loss with respect to all the weights and\\nbiases of the neural network. I'm going to show you how to use this and grading descent to train\\nyour neural network. So let's start building\\nour bigger neural network, this one is going to have three layers. An input layer with inputs x1 and\\nx2, then it's going to have a first layer with some\\nperceptrons and a bias and then this feed into a second layer\\nwith more perceptrons and a bias. And this will feed into a final output layer from which a output of\\ny hats going to come out. Now we need to find some notation for\\nthe weights, so let's add a superscript of 1 for\\nthe first layer and these ones will feed into summations,\\nsuperscript 1 and set 2 superscript 1. So everything with superscript\\n1 is in the first layer and now these are the a's that\\nare obtained after the sigmoid. Any weight and\\nbias of the second layer is going to have a superscript 2 as well as the sets and the a's which are obtained by\\napplying sigmoid on the sets. And finally anything on the third layer\\nis going to have a superscript of 3 and you can imagine that if I continue\\nadding layers we're going to keep having superscripts of 4, 5, 6 etc. And so\\nthat's a pretty consistent notation for all the weights in a neural network and\\nas usual l of y,y hat is the log loss function given by this formula and\\nthat's the one we want to minimize. So now for the purpose of this network,\\nlet's clean up the neural network as bit and let's actually only\\nworry about a few of the weights. And by symmetry everything's\\ngoing to be worked out the same for the other weights, so we're only going to\\ncare about these ones over here. And so the goal of back propagation, back propagation is the method that we're\\ngoing to use to train neural networks. And it's just going to consist on\\ncalculating a lot of derivatives using chain rule and using them to update our\\ngrading the same step just like before except that now we have to\\nkeep track of more variables. So as usual, here is the log loss and we have to see how the log loss changes\\nwith respect to all these weights I'm only circling a few of them\\nit's really a bunch of them. But everything can get calculated\\nusing these green ones over here, so what do we need to calculate,\\nwe need to calculate DL over DY hat and all the DL over D anything else. Because that's what tells us how\\nto move each one of the weights so let's go to this part of the neural\\nnetwork, the part of the end. And as usual we need to build\\na chain because we need distributive over here DL\\nover Dw superscript 3. And that's going to be obtained\\nas the product of these 3 because that's the chain of command, that's the chain of variables that\\nare in between w superscript 3 and l. So once we calculate this ones we can\\ndo the exact same thing for the bias. So these ones we know\\nhow to calculate now, let's move a little further and\\nit's the same thing. We need to calculate the DL\\nby Dw superscript 2 and these are all the variables\\nthat are in between. So we need to do a long,\\nlong chain rule of variables, each one depending on the previous one. But as usual these are either linear\\nfunctions of sigmoid so they're all easy to calculate and the same thing\\nhappens for DL over Db superscript 2. It's just a chain of variables and we need to multiply them in\\norder to get the chain rule. Now this over here have\\nalready been calculated so we don't need to calculate it anymore. So there's a lot of reuse of variables so\\nwe can just store them over here and continue calculating new ones, otherwise\\nwe would have to recalculate many things. So it's good to store the ones we've\\nalready calculated and finally for DL over Dw superscript 2 we\\njust have the product of all of these ones over here that can be\\nalready calculated from before. In our repository of already\\ncalculated derivatives times some new ones over here,\\nAnd as usual, these are easily calculated, so basically\\nthere's a lot of work involved here but you know what the basic steps are. The basic steps are, long chain rule and\\ncalculated separately and these neural networks are built in such a way\\nthat each derivative of the simple ones is either a sigmoid or a linear function\\nso they're very easy to calculate. The good news is if you're a machine\\nlearning practitioner, you don't need to actually do this, there's some great\\npackages that do it very well. But my philosophy, I always like to see\\nit done carefully at least once and then you can use the packages and\\ndo things very fast.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Welcome to week three. In this week, you're going to\\nlearn what a neural network is and how to train them using gradient descent. You're also going to learn a alternate\\nmethod to gradiend descent called Newton's method,\\nwhich is also very fast and very useful. But let's start with the first video. In this video, you're going to learn the fundamental unit\\nof neural networks called a perceptron. That may sound like a foreign term to you,\\nbut you've actually seen one all ready, because linear regression can\\nbe expressed as a perceptron. So let's start with the motivation\\nbehind a regression problem. We'll be using one of the most classical\\nexample used in linear regression, predicting the price of a house\\ngiven the size of the house. So let's say you have\\ndata on three houses. The first one has 1000 square feet and\\nit costs $20,000, the second one, 2000 square feet and\\na cost of $30,000 and the third house has a size of 3000\\nsquare feet and costs $50,000. And let's say that you have more data\\npoints which you plot on this graph. So these are other houses on the market\\nwith their sizes and their prices. And the goal is to take this data of\\ninputs, which is the size of the house and find in this case a line because they\\nlook like they form some sort of a line. So we're going to find a line used\\nto predict the price of the house. And the goal is to find\\nthe best possible line. Now in this case we only have one input\\ntype which is a feature that we're going to use to predict the prices. What if we throw in a second feature? So let's throw in another feature\\nlet's say the number of rooms. Now the houses have 2,\\n4 and 7 rooms respectively. With this added feature,\\nthe problem is not so simple anymore and we need to consider how both\\nfeatures affect the output. Then how do we build and train a model that is able to make\\npredictions of the house prices and more. What if we had more features? What it had like 10 features or\\n100 features? Well here's where the perception comes in. So this regression problem can be\\nmodeled with a single perception. The perception is going to take some\\ninputs into some unknown outputs that we want to predict. So first we're going to start by\\nexploring the perception mathematically. So let's start with the inputs. The inputs here are going to be x1 and x2\\ncorresponding to the size of a house and a number of rooms. But I want you to imagine this,\\nif there were 100 inputs, it would simply be 100 notes\\nx1 all the way to x100. So we start with the inputs and\\nthen they plug into a summation function. I'll explain more about\\nthis in a little bit. Out of the summation function\\ncomes the output y hat and that's going to be what we predict\\nto be the the price of the house. So a good model will come out\\nwith pretty good predictions. And the idea is to build\\nthe best possible model. So what is happening in\\nthis summation step? Let me do it more in detail. Each of these features, x1 and\\nx2 is multiplied by a corresponding weight to determine how\\nimportant it is for the output. So for example, if the size of a house\\nis way more important to predict the price of a house\\nthan the number of rooms, then this will have a higher\\nweight than the number of rooms. So let's call the corresponding\\nweights w1 and w2. And to combine these weights and\\nthe input, we simply add them. So we multiply w1 the weight times x1 the feature and we add w2 times x2. And we're almost there. We're not quite done\\nbecause there's also a bias term that gets added to\\nthe summation function. And now we add this and\\nthat is the output. In this model there's\\nno activation function. I will tell you later what\\nan activation function is. But here the output is simply y hat,\\nthat's w1x1 + w2x2 + b as the predicted\\nprice of the house. And the goal is to find the best w1,\\nw2 and b the weights and\\nbias that will optimize predictions. So the ones that will,\\nwhen we plug in x1 and x2 give us the closest to the actual\\nprice for the entire data set. Now, how do we do this? How do we find this perfect weights? The idea is that we want to\\nminimize some kind of error. The error is basically how far\\nare you from the price of the house? And so if you minimize that error,\\nthen that's all we need. So for that we introduced something\\ncalled the loss function. The loss function is a little\\nfunction that's going to tell us, how far are we from\\npredicting the prices well. So that's on the next video.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Now we know what the\\nproblem is at hand. We want to calculate all those derivatives\\nand be able to do a gradient descent\\nstep in order to find the best weights and\\nbias for our dataset. Now, let's focus on\\nweight w_1 for a moment. The idea is to reduce\\nthe function L(y, ) by moving around with W1. Let's first see\\nhow w_1 affects L. Because if we can find in\\nwhich direction to move w_1 in order to reduce\\nL by a little bit, all we need to do is iterate this step with all the\\nweights many times. Now, there are a lot of\\nvariables in between L and w_1. One of them is y\\nHat because y Hat affects L. We need to find the partial derivative\\nof L with respect to y Hat. Now, y hat is affected by w_1, so we need to find the\\npartial derivative of y hat by w_1. Finally, that tells us how\\nmuch L is affected by w_1. The missing piece is\\nthe partial derivative of L with respect to w_1. That's the one we\\nreally want to find. Why? Because that was\\nthe one that tells us how much w_1 affects L\\nand in what directions. This one is calculated\\nusing chain rule. d L by d w_1 is d L by d y Hat times d\\ny Hat by d W1. All we have to do is\\ncalculate these two in order to calculate\\nd L by d W1. The same thing happens with W2. We want to change W2 to\\naround to reduce the loss. We have this hidden\\nvariable y Hat in between. We want to know how much y\\nHat influences the loss. We also want to know that how\\nmuch W2 influences y Hat. Therefore, we'll be able to find this derivative over here, d L by d W2, which is calculated using the chain rule just like before. The same thing happens with B. Reduce the loss function, we use d L by d y hat, which you've already calculated. This one is influenced by\\nB. d y Hat by d B. That tells us the derivative\\nof L with respect to B. To find this one again\\nwith the usual chain rule. We have a lot of\\nderivatives to calculate, and I'm going to\\nput them over here. The first one is d L by d B, which we write using\\nthe chain rule, then d L by d W1, and d L by d W2. We only have four things to\\ncalculate because d L by d y Hat is there many\\ntimes. What are these ones? Well, let's recall that\\ny Hat is obtained as the sigmoid function\\nof the summation obtained inside W1 X1\\nplus W2, X2 plus B. That the log loss is\\nthis function over here. Let's start calculating things. First, let's do d\\nL by d y Hat, the one that appears most often. Notice that the logarithm of y Hat has as derivative\\none over y Hat, and negative y is simply a constant with respect to y Hat. The first term is minus\\ny divided by y Hat. At the end, this is what\\nwe're going to have. The reason is because the second one has a\\nvery similar derivative, one minus y over one\\nminus y Hat because that y minus y Hat in the denominator comes from logarithm\\nof one minus y Hat. When we expand this, these two cancel out and we get negative y minus y Hat over\\ny Hat times one minus y Hat. That's d L by d y Hat. We're going to use\\nthis one a lot. Now, let's calculate\\nthe other three. How do we calculate\\nthat y Hat by d W1? Well, remember that\\nthe derivative of sigmoid is sigmoid times\\none minus sigmoid. This is going to be sigmoid of the inner part times one minus\\nsigmoid up the inner part, which is the same as y Hat, times one minus y Hat times X1, which is the derivative\\nof the inside by W1. For the same reason, d y Hat by d W2 is this, and d y Hat by d B is y\\nHat times one minus y Hat. Recall that this is using the fact that the\\nderivative of sigmoid of something is sigmoid\\ntimes one minus sigmoid. That's where the y Hat times\\ny minus y Hat come in. Now we've calculated a\\nbunch of derivatives. All we need to do\\nis multiply them. What's d L by d B? Well, it's d L by d\\ny Hat, which is this, times d y Hat by\\nd B which is this. The same thing we do\\nfor d L by d W1. It's d L by d y Hat\\ntimes d y Hat by d W1. Finally, the same\\nthing goes for W2. It's the usual one d L by d y Hat times d\\ny Hat by d W2. Now we need to cancel\\nout a bunch of things. Notice that this two cancel out the y Hat\\none minus y Hat, cancel out on the denominator\\nand the numerator. We end up with some\\nreally nice derivatives. Check it out. d L by d B is simply\\nnegative y minus y Hat. d L by d W1 is negative\\ny minus y Hat times X1. d L by d W2 is negative\\ny minus y Hat times X2. Notice that if y and\\ny Hat are very close, these areas are very small, which means you don't\\nneed to make a lot of changes because you already\\nhave a good prediction. But if you have a\\nbad prediction, y and y Hat are really\\nfar away from each other. Therefore, you need a large derivative to move\\nthings around a lot. Let's go back to where we were. Our main goal was to find\\nthe optimal values for W1, W2, and the bias B. We can use the partial\\nderivative expression. We found our gradient descent\\nexpressions as follows. Here's set, here\\nis sigmoid of set, and the output is something\\nbetween one and zero. The log loss was L (y, ). In order to find the\\noptimal values for W1, W2, and B, we're going to use, you guessed it,\\ngradient descent. This is the updating\\nstep for W1. Now notice that we already know the partial derivative\\nof L with respect to W1, and it's exactly this. This is the similar step for W2. We can also replace the derivative by what\\nwe already found out. This is the step\\nfor replacing B. Again, we can put what we\\nknow to be the derivative. In other words, this\\nis the updating step. You start with some W1, W2, and B, and then you iterate\\nusing this procedure. If you do it as many times, you're going to get to\\nsome pretty good W1, W2, and B values. I'll give you a\\nreally good model that will map your\\ndataset pretty well. That is gradient descent for\\na classification preceptor.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"You've already seen how to use perceptrons as linear\\nregression problems. But a perceptron can also represent a\\nclassification problem, particular a binary\\nclassification problem. All you have to do, is change\\nthe activation function. In this video, I show you how. Now that you know how to solve the regression problem with a perceptron and with\\ngradient descent, classification is\\nactually very similar. It's only has one\\nlittle complication, but it's not that bad. Let's look at an example that you've already seen\\nearlier in the class. You made an alien\\ncivilization and you started listening\\nto the way of speaking. You take note of\\nfour sentences in their language that\\nare these four. They only have two words\\nin their language, so it's pretty simple. But what you want\\nto do is be able to tell the mood of\\nthe each sentence. Now when the aliens said\\nthe first sentence, you can notice that\\nthey were happy. For the second sentence,\\nthey were sad. For the third one,\\nthey were sad and for the fourth one, they were happy. The idea is to be able to infer from new sentences the mood, if they're happy or sad. We're going to make a\\nmodel that will predict the mood based on the sentence. Now, since we want numbers, because models take\\nnumbers and not words, then we're going to make\\nthe following table where the number of times the word aack and\\nthe number of times the word beep are recorded. The first sentence has three times the word aack and\\nzero times the word beep. So it's 3, 0, for\\nthe second one is 0, 2, for the third is 1, 3, and the fourth one is 2, 1. That's the dataset for the sentiment analysis\\nclassification problem that we're going to\\nsolve in this lesson. The first thing we do\\nis plot the points. Here are the points. As you can see, the happy\\npoints tends to be on the bottom right and the\\nsad points on the top-left. That's probably what the\\nmodel is going to tell us. But let's turn it\\ninto a perceptron. For the perceptron,\\nwe have our inputs labeled x_1 and x_2, and x_1 is the number of\\ntimes a word aack appears and x_2 is the number\\nof times beep appears. Again, this is going to go through a prediction\\nfunction just like before with a summation\\nand a bias, etc. The prediction y hat\\nis going to tell us if the model thinks the\\nsentence is happy or sad. Now what goes in the\\ncenter node of the highlighted in red indicates\\nthe placement of this line. That's going to classify\\ndifferent sentences are happy or sad based on the number of times a particular word appears. Decenter is the one that\\ndivides the happy zone in the bottom right from the\\nsad zone in the top-left. Let's explore this center\\nnode more carefully and the entire classification\\nperceptron further, it's going to be very similar to the regression perceptron, but with a little addition. Again, we're going\\nto have weights W_1 and W_2 and determining how important any\\nother features are. Say, the word aack is really\\ncorrelated with happiness, then W_1 is going to\\nbe a large number. If beep is very irrelevant, then the W_2 is going\\nto be a small number. However, if beep happens to be inversely correlated\\nwith happiness, say it's a very sad word, then W_2 is going to\\nbe a negative weight. Again, we're going to\\nhave a bias input b. What's going to happen\\ninside the node? We're going to have a summation. The summation is again\\ngoing to be x_1, W_1 plus x_2, W_2. The features times the weights\\nadded plus the bias b. That's just like before. This produces a\\ncontinuous number that's going to be called set. As I said, that's a\\ncontinuous number, that can be anywhere\\nin the number line. This number can be\\nminus a million, it could be plus 7,000, it could be zero, could be one, it\\ncould be whatever. However, that's not\\nthe output we want. That's the output we\\nwant in any regression because we want any\\nnumber to be the price. But now we want a number\\nbetween one and zero because we want it to\\ntell if it's happy, that's a one or if it's\\nsad, that's a zero. How do we turn the\\nentire number line into the numbers 1, 0 or at least some\\nnumber in between 1, 0. We're going to use what's\\ncalled the activation function, denoted by the\\nletter sigmoid of z. The activation function,\\nit's actually very useful, is going to take all the\\nnumbers in the number line and crunch them into\\nthe interval 0, 1. Now you can still get numbers in between\\n0, 1. That's okay. If you get a 0.5 can be that the model doesn't know if the\\nsentence is happy or sad. If you get 0.9 means\\nthe model things, the sentence is happy. If you get 0.01, then it means that\\nthe model of things, the sentence, it's pretty sad. The sigmoid function is a huge component of\\nthis perceptron. Now, let me explore the sigmoid function a\\nlittle more in detail. The sigmoid function is\\ngiven by the formula 1 over 1 plus e to the z and that's going to\\ngive us a number 0 and 1. In the next video,\\nwe're going to get into the sigmoid\\nfunction in more detail. But for now, just know that it's a very important piece of the perceptron and\\nthe summation in the left gets passed through\\nthe sigmoid function in order for our output to be something\\nbetween zero and one, which is exactly what our\\ndataset is asking for.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"So now that we know how a neural\\nnetwork looks like, all we have to do is find these derivatives in\\norder to do gradient descent. So recall that the goal is to adjust\\neach one of the highlighted weights and bias in order to reduce\\nthe loss function L y, y hat. And so basically, we need to see how each\\none of these weights affects the loss. And that is what the partial derivative\\nof L with respect to Wij tells us. Same thing with these biases. How do these biases affect the loss? Well, dL over dbi tells us that. Now, how do the weights W1 and W2 affect\\nthe loss with this partial derivative, and how does this bias affect the loss\\nwith this partial derivative? In other words, this partial derivatives\\ntell us exactly in what direction to move each one of the weights and\\nbiases in order to reduce the loss. So that's exactly what we're going to do. We're going to calculate each one of these\\nin order to reduce the log loss function. So let's simplify and\\nonly look at these ones over here. Del L over del W11, W21 and b1. And let's also look at del L over\\ndel W1 and del L over del b. So let's recall that the way\\nwe calculated the output of the red note was first we\\ndid set 1 is this summation, then we applied sigma to get a1, and then we did the summation\\nof the inputs a1 and a2 times the weights W1 and\\nW2 and the bias b. And we applied sigmoid to\\nthis set to get y hat. And then with y hat,\\nwe found the log loss of y, y hat. So there's a lot of variables\\nin between L and W11. And we're just going to keep track of all\\nof them to do a humongous chain rule. So in order to reduce this log loss,\\nwe need del L over del y hat. And the reason is because L\\nthe loss depends on y hat. Now y hat depends on that. So we need this derivative of 2. Del y hat over del z. Now z depends on a1. So we need del z over del a1,\\nand a1 depends on Z1. So we need del a1 over del z1. Now, z1 depends on W11 so\\nwe need to del z1 over del W11. That's a huge chain roll. This is how it looks,\\ndel L over del W11 which is the one we wanted to find is equal to del\\nz1 over del W11  del a1 over del z1  del z over del a1  del\\ny hat over del z  del L over del y hat the product of all this\\nis a really long chain rule but that's what gives us del L over del 11,\\nthe one that we really want. Now let's calculate each\\none of these separately. What's del L over del W11? Well, this one's easy because it's del\\nz1 over del W11 is the one over here. Each one of these terms\\nis actually really easy. What is this derivative? Well, it's simply X1 because\\nW11 is the variable, X1 is a constant accompanying it and\\neverything else is a constant. So has derivative 0. Now let's move to del a1 of our del z1. That one is easy too\\nbecause it's a sigmoid and the direction sigmoid\\nsigmoid times 1- sigma. So this is a1  1- 81. Now what is del z over del a1? Again, this is a linear equation. So W1 is a derivative and\\nwhat's del y hat over del z. This is again a sigmoid. So it's y hat  1- y hat. And finally, what's del L over del y hat. That's the harder one but we've already\\ncalculated this one plenty of times. So our derivative del L over del W11\\nis the product of all these things. These two cancel out. And so we get this expression over here. So in order to find the optimal value\\nof W11 that gives the least error, we performed gradient\\ndescent with this formula. And what's del L over W11. It's simply this over here. So that is how we change W11. And as you can imagine,\\nthat's how you change any Wij, you just have to change\\nthe subscript here. But I'll tell you how\\nto do it in a minute. But for the sake of redundancy and to\\nreally nail this down, let's do the bias. So in order to reduce this again,\\nwe're going to go a little faster, when a del L over del y hat. And that depends on set,\\nwhich in turn depends on a1 which in turn depends on Z1\\nwhich in turn depends on b1. So the long chain rule\\nis this one over here, product of all these derivatives\\nis equal to del L over del b1. So let's calculate one\\nof these separately and quickly because we've already\\ndone it before for most of them. What's del L over del b1 is\\nthe product of del z1 over del b1. Which now it's easy. It's 1. Why is it 1? Because b1 is the only variable here. So this may as well be\\nb1 plus some constant. And the derivative of that\\nrespect to b1 is simply 1. And the rest, we already know\\nthis is the signal function. This is a linear function. This is again the sigmoid function, and\\nthis one we've already calculated before. So things cancel out, and the way we update b is in order\\nto find the optimal value b1. Then we perform grading descent\\nwith this del L over del b1 with calculator already and it's this. So let me summarize,\\nthe way we update W11 is like this. For W12, it's a slightly different. For b1 is this and then for the other\\nthree weights and biases, it's this. So in short, if you do these updates for a learning rate alpha,\\nyou get better weights and biases. So that's how you update the first\\nlayer of the neural network. Now let's update the second layer. But this one is much easier because\\nwe have to keep track of less things. So here is how you do it. You want to minimize L, y, y hat so\\nyou want to actually decrease it. Del L over del y hat is the first training\\nthat you need because L depends on y hat. Y hat depends on z. Z depends on the W1. And therefore you need this\\nchain rule over here in order to calculate del L over del W1. And let's calculate it\\nfrom its separately again. So what's del L over del W1? It's distributed over here\\nwhich is a linear function. So it's a1. This one over here is a sigmoid so\\nthe derivatives y hat  1- y hat, and this one over here we calculated already. So things cancel out, and\\nwe get something very simple. That to find the optimal value of W1\\nyou perform great in descent with this formula over here and since you've\\nalready calculated derivative, you get this, this is high update W1. I'm going to rewrite it like\\nthis by turning a negative, a double negative into a positive. And this is how you update W2, and\\nthis is how you update the bias. So in short, we've seen how to\\nupdate each one of the weights and bias of the neural network. So when we do this, we actually get\\na much better set of weights and bias. And that's how you train a neural network.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"In the previous video, you\\nsaw a dataset with a bunch of houses and their prices and also their sizes\\nin square feet. The idea was to fit the best possible line\\nthrough these points. The line that passes the\\nclosest possible to this point, and that would be what\\nmakes a good prediction. This line over here is precisely\\nthe model and we need to find a way to evaluate the model to see how\\nwell it's doing, and based on that,\\nto see how can we improve it to get a better one. First, let's see what\\nthe model predicts. The model predicts\\nthis column, how? Well, the point corresponding to the first house is at\\nheight 15,000 and the one corresponding for\\nthe second house at high 30,000 and for the\\nfirst one is at 45,000. This looks like a\\npretty good model. We may be able to do better, but this so far it looks\\nlike a pretty good model, at least you can\\nsee it correctly predicted the price\\nof the second house, and it was not so far from\\nthe first and the third. But let's quantify\\nhow far it was. By that, we need to subtract the predictions from\\nthe actual value, and we're going to\\ncall that the error. That error is indicated by these vertical broken\\nlines over here. Clearly, the shorter the\\nline, the better the model, because that means\\nthat predictions are close to the actual values. However, there's a\\nsmall problem with y minus y-hat and the\\npoint is above the line, then this is positive\\nand if the point is below the line, then\\nthis is negative. A negative error is also bad as well as\\nthe positive error. But if we add them, we can\\nget zero or small numbers. This can get confusing,\\nso for that, we square the error. y minus y-hat squared is the\\nactual measure that we want. In machine learning, we\\nactually multiply it by a half. The reason is really more\\ncosmetic because in reality, when you take the derivative\\nof y minus y-hat squared, then you get a lingering two. We put a 1/2 there\\nto cancel with that two and then we have less\\nnumbers to keep track on. But at the end of the\\nday, you can multiply zero by any constant you want, and you still don't change\\nanything at the end. Now we have a prediction\\nfunction that outputs some prediction y and\\nis given by this formula. The loss function\\nwe're going to call L, and for every point is 1/2\\ntimes 1 minus y-hat squared. Here's the distance\\nbetween the prediction and the value squared\\nmultiplied by half, and we're going to call\\nthat L of y, y-hat. Our goal now is\\nto minimize this. To find the w_1, w_2, and b, that gives y-hat with\\nthe least error. In the last week, you learned a lot about\\nminimizing functions, including methods like\\ngradient descent. As you can imagine, this is\\nwhat we're going to use next. We're going to use gradient\\ndescent to minimize L, the loss, and thus\\nfind the best w_1, w_2, and b. That implies we found the\\nbest model for the dataset.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"In the previous\\nvideo, you learned the sigmoid function and how it's super useful\\nin machine learning and the reason is\\nbecause it takes the entire number line and crunches into the interval zero, one, which is what we\\nwant for the output. The formula is 1/1\\nplus e^minus z. Here is the graph. The graph is this one over here. Notice that the input is the number line and the output is the\\ninterval zero, one. The input is the horizontal axis and the output is\\nthe vertical axis. Notice that it has\\nasymptotes at zero and one, which means it never\\nreally touches the numbers zero and one but every number has\\nan output strictly between zero and one\\nand for example, sigmoid of zero is 1/2, because if you plug in a\\nzero and 1/1 plus e^minus 0, you get 1/1 plus\\n1, which is 1/2. If a number is very large\\npositive like 1,000, then sigmoid of 1,000\\nis very close to one. If it's a very large negative\\nnumber like minus 1,000, then sigmoid of minus 1,000\\nis very close to zero. However, the sigmoid\\nfunction has a very nice property other than the fact that it crunches the entire number line\\ninto the interval zero, one and is that its\\nderivative is really nice. Now we're going to\\ncalculate the derivative. Why is it important that\\nit's a nice derivative? Because in machine learning, we're going to use\\nderivatives a lot. It's very important that our function is easy\\nto differentiate. Let's take the derivative\\nin great detail. Sigmoid of z is 1/1\\nplus e^negative z, which can be written like this, 1 plus e^negative z^negative 1. Now we're going to take\\nthe derivative of this. We're going to use\\nthe chain rule. For the chain rule\\nwe have d/dz of something to the minus 1 is minus 1 times\\nsomething to the minus 2. Then we take the derivative\\nof the something inside, that's exactly chain rule. Let's rewrite it as negative 1, 1 plus e^negative z to\\nthe negative 2 times, now, d/dz of 1 plus e^negative z is d/dz of 1 plus the\\nd/dz of e^negative z. That's going to be rewritten as zero because the derivative of a constant that's one is zero, and here we have the\\nderivative of e^negative z, which is e^negative z derivative of negative\\nz by the chain rule, and that last part is\\ngoing to be minus one. We're about halfway there. Notice that this negative one gets canceled with\\nthis negative one, and we have 1 plus e^negative\\nz^negative 2 times e^negative z, which is this. Now this can be\\nrewritten even nicer. Let's write it as\\ne^negative z over 1 plus e^negative z squared. This is where we are. Now, notice that we can\\nadd and subtract one. Why do we want to add\\nand subtract one? You'll see, but let's\\nwrite it like this. Now let's split it as the\\nfirst 1 plus e^negative z over the denominator minus 1\\nover the denominator. Now this cancels with one\\nof these two and we get 1/1 plus e^negative z minus 1/1 plus\\ne^negative z squared. That can be written as this minus 1/1 plus\\ne^negative z times itself. Now we're just\\ngoing to factor the 1/1 plus e^negative z times 1 minus 1/1\\nplus e^negative z. Why do we want to do that? Well, recall that this\\nis exactly the sigmoid. The first term is sigmoid, and the second term is 1 minus sigmoid.\\nIsn't that beautiful? The derivative of sigmoid is sigmoid times 1 minus sigmoid. The fact that this\\nderivative is so easy to calculate means that it's going to be really useful\\nin machine learning.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " (\"Now that you know\\nall there is to know about the sigmoid function, let's go back to the perceptron. Let's take for example\\nthe sentence Aack, beep, beep, beep as an example. This one has the word\\nAack one time so x_1 is one and the word beep three\\ntimes so x_2 is three. Let's say that\\nwe're in the middle of training the algorithm and the weight that we\\nhave are 4.5 for w_1, 1.5 for w_2, and the bias of two. What the perceptron does\\nis it takes this input, multiplies them by\\nthe weight functions and applies the sum and then the sigmoid function in order\\nto get a prediction y-hat. The idea is that\\nwe're going to use y-hat together with\\na loss function that tells us how far\\ny-hat is from what it should be in order to\\nupdate the weights. Let's say that y is zero, so the sentence is set. Then we're going to use\\nthat to update w_1, w_2, and b. Basically, we're\\ngoing to measure how far y-hat is from y\\nand that's the error. Now you may think y-hat\\nminus y works really well, y-hat minus y squared, or one-half times\\none-half minus y squared. All the other functions we've\\nseen before, those work. But for classification, the one that works best\\nis called the log loss. Next, I'm going to show you how the log loss is calculated. Log loss is going to\\nbe called now L of y, y-hat and we're going\\nto use that error to update the three weights\\nand bring the error down. Let's recap. We have a prediction function\\ny-hat with an activation of sigmoid that is applied on\\nthe summation coming out of the perceptron and we have a loss function that\\nis the log loss. Now the log loss is something\\nyou've already seen before. Do you remember that\\nexample we saw in the previous section where\\nwe were trying to find the ideal coin to be able\\nto fit some dataset? It was a coin that\\nyou need to flip 10 times and you\\nneeded to obtain heads seven times\\nand tails three times and you need to find\\nthe perfect coin for that. The perfect coin was obtained by minimizing some loss\\nfunction with a logarithm. It's precisely this. This is actually the\\nloss function for this problem and\\nyou can see that if y is zero then this is small when y-hat is small and\\nlarge when y-hat is large. If y is one then the\\nopposite happens. When y-hat is close to one\\nthis function is small and when y-hat is close to zero\\nthis function is large. In other words this L of y, y-hat is a large number if y\\nand y-hat are far away from each other and small number if they are close to each other. There are many reasons why I want to use log loss\\nfor this function. One is that the math works\\nout really nicely but the other one is that\\nthis function has a probabilistic nature\\nto it and the reason is the same reason that he came out of that example\\nwith the coins. Classification problems are\\nhighly probabilistic because you can think of the output\\ny-hat as a probability. Let's say that y-hat\\nis 80 percent or 0.8, that means that the model gives the sentence an 80 percent\\nprobability of being happy. Let's go back to the main goal. The main goal is to find\\nthe perfect weights w_1, w_2, and b. That will give us y-hat with the least amount of log\\nloss L(y, y-hat) error. In order to be able to do this, we need to go back\\nto gradient descent. We call the gradient\\ndescent formula that updates the weights w_1 as the old w_1 minus\\na learning rate times the derivative of the log\\nloss with respect to w_1 and w_2 as w_2 minus\\nlearning rate times derivative of log\\nloss by w_2 and the same thing for\\nb bias equals bias minus Alpha derivative of\\nlog loss by derivative of b. Now in order to start the\\nalgorithm we simply start with some arbitrary values\\nfor the weights and biases and we're going\\nto start the design process. We start with random\\nvariables then we calculate the partial derivatives\\nand then we update. In the next video, I'm\\ngoing to show you how to find these partial\\nderivatives and you're going to see that the\\nsigmoid and the log loss actually conspire really nicely to get some very\\nnice derivatives.\",\n",
       "  'machine learning::machine learning calculus::03 optimization in neural networks and newtons method'),\n",
       " ('Congratulations. This\\nweek you learned how derivatives and gradients work in two or more dimensions, how to turn that into an\\noptimization problem, and how this works\\nin machine learning in methods such as\\nlinear regression. Then you also learned a\\npretty useful and fast method called gradient\\ndescent that will help you optimize\\nfunctions quickly. See you in the next lesson.',\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"So far you learn how to solve optimization problems using\\nderivatives and gradients. However, when you try to get exact solutions to these\\nproblems analytically, you're going to\\nstart noticing that these problems can get really\\ncomplicated really fast, especially in higher dimensions. In this lesson,\\nI'm going to show you a method that is iterative and that is really powerful for minimizing or\\nmaximizing functions, especially in many variables, the method is called\\ngradient descent. But let's start with\\ngradient descent in one variable in order to work our way up the gradient\\ndescent in several variables. Let's look at the\\nfollowing function, f of x equals e to the\\nx minus logarithm of x. This is the plot\\nof the function, so it's nice and smooth. The question is, can we\\nfind the minimum here? The minimum is\\nsomewhere around here. We know how to do it\\nfrom previous lessons. All we have to do is calculate f prime of x and set\\nit equal to zero, and then solve for x. What is the\\nderivative of f of x? It's e to the x minus 1 over x. The reason is because\\nthe derivative of e to the x is e to the x and the derivative of\\nlogarithm is 1 over x. All we have to do\\nis solve for e to the x minus 1 over x equals 0. However, this is pretty\\nchallenging, let me show you. Solving for e to\\nthe x minus 1 over x equals 0 is equivalent\\nto solving for e to the x equals 1 over x.\\nI challenge you to try it. It's actually pretty\\nhard to do analytically. The solution is 0.5671, etc. Which is a famous number, is known as the Omega constant. However, the fact that this\\nis really hard to solve analytically should not stop\\nus from trying something. The question is, is\\nthere any other way? Here is one method. Let's pick some random value, let's say somewhere around here. Now, let's give it a try and move to the\\nleft by a little bit, and move to the right\\nby a little bit. Now we have two\\nnew points to try. Which of these two\\npoints is better? Well, since we're trying\\nto minimize the function, let's go for the\\none on the right, which has a smaller\\nvalue in the function. Now we are at this\\npoint, and let's repeat. So we move in both directions and check to\\nsee which point is smaller, and this one wins. Let's try it one more time. We move in both directions, and now something\\ninteresting happens, which is that both points on the two sides have\\na higher value. Therefore, let's\\nsettle for this point. We say that this point it\\nmay not be the minimum, but it's close enough\\nto the minimum. Now this method is not\\nbad but we can actually improve it a lot to get\\nreally close to the minimum. Let me show you in\\nthe next video.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"In the previous video, you learned the learning rate. Learning rate is a big\\ndeal in machine learning and finding a good learning\\nrate can be pretty hard. It can also be\\npretty influential to how well your model does. Let's say you have a learning\\nrate that's too large, then you may miss the\\nminimum and never be able to find it because your\\nsteps are just too big. What if the learning\\nrate is too small? Well then you may\\ntake forever to actually reach the minimum\\nor perhaps never reach it. What you want is a learning\\nrate that is just right. However, finding a\\nlearning rate that is just right can\\nbe pretty hard. It's actually a research problem to find a good learning rate. There are a lot of really\\ngood methods that change the learning rate based on\\nhow the problem is doing, but there's no definite method to find a good learning rate. Now, here's another problem that gradient descent may have. Let's say that you have\\nthis function over here, so the minimum is\\nactually this one. However, let's say\\nthat you start running your gradient descent\\nalgorithm over here, well, it's never going to take\\nyou to that minimum because it takes you towards\\na local minimum, to point and looks like\\na minimum, but it's not. How do you overcome\\nthis problem? There's actually no secure\\nway to overcome it. But a way to get pretty\\ngood results is to run the gradient descent\\nalgorithm many times with many different\\nstarting points. Once you run it with this, chances are one of\\nthem will get you to the minimum or at least\\nto a pretty good point.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"In the previous\\nvideo, we learned an okay way to approximate\\nthe minimum of function. However, here we're going\\nto try something smarter. Let's say you're\\nover here and you want to get closer\\nto the minimum, what would you tell\\nthis point to do? You would tell it to step to\\nthe right a small amount. What if the point is over here? Well, you would tell it\\nto step to the left. Now, is there any\\ninformation on the function that could help you make\\nthis decision quickly? Well, let's look\\nat the derivative. Over here, the slope is negative and you're\\nmoving to the right, and over here the slope is positive and you're\\nmoving to the left. That's the information you need. You actually need to\\nsubtract the slope. Because if you are at a point where the\\nslope is negative, you need to add a little bit to the coordinate of the point. Whereas if you're in a place\\nwhere the slope is positive, then you need to subtract a little bit to the\\ncoordinate of the point. In short, if you want a new point that is\\ncloser to the minimum, then that should be the\\nold point minus the slope. Why? Because if you're to\\nthe left of the new minimum, the slope is negative. You're subtracting\\na negative number and moving to the right. If you were to the\\nright of the minimum, then you subtract\\na positive number, the slope, and move to the left. Now, if the new point is X_1\\nand the old point is X_0, then the formula is X_1 equals X_0 minus f prime of\\nX_0, which is the slope. However, there is\\na small caveat. Imagine that you're over here at a very steep\\npart of the curve, because the function is steep, then the derivative\\nis very large. You're making a\\nreally long jump. Now, long jumps can\\nbe pretty chaotic. The reason is because\\nyou may actually miss the minimum and go pretty\\nfar away and get lost. We want to take small steps and to be more\\nsecure in our path. How do we take into\\naccount a small step? Well, you simply modify the formula and put\\na small number, multiplying the slope,\\nfor example, 0.01. You can take any small\\nnumber you'd like, and that's called the learning\\nrate and is denoted Alpha. There's a whole science in\\npicking good learning rates. Now, this formula\\nhas an added bonus. Imagine if you're over here far away from the local minimum, and in a steep\\npart of the curve. You'll be taking a big step, obviously not big because\\nyou don't want to miss the point and go too\\nfar, but relatively big. However, if you're\\nreally close to the minimum in a part\\nthat's more flat, you want to take a small step. Now, this is included here in the formula because if you're in a steep\\npart of the curve, the derivative is B, and\\nyou're taking a large step. However, if you're\\nin a flat land, then the derivative is small and you're\\ntaking a small step. Imagine like in golf, if you're far away\\nfrom the hole, you want to hit it really\\nhard to get there closer. But if you're\\nreally close to it, you want to hit it with\\nprecision. Not a lot of strength. This method is called gradient\\ndescent and it's really, really useful in\\nmachine learning and that's why it's so popular. It only consist in iterating this procedure. You\\nstart with X_0. From X_0, you can get a\\npoint X_1 using the formula. Then you can get another\\npoint X_2 based on X_1 by just plugging X_1 in the\\nformula now. You can continue. You can iterate in\\nmany times, 20 times, 100 times, thousands of times. This is a pretty fast algorithm. So it can be iterated\\nthousands of times quickly and its\\nresults are amazing. It can get us really, really close to the minimums\\nof functions. That's why it's so widely\\nused in machine learning. Here is the algorithm in short. You start with a function f of x and the goal is to find\\nthe minimum of f of x. First you define a learning rate and you choose a starting point, and then you do\\nthe updating step, which is X_k equals X_k minus 1 minus Alpha times the\\nderivative at X_k minus 1. Then you repeat this step until you're close enough\\nto the true minimum. You can tell that\\nyou're close enough to the true minimum when your steps don't really\\nchange that much. Now let's try it on the example. Let's do a couple of iterations. Recall that the function is e\\nto the x minus logarithm of x and the derivative of e\\nto the x minus one over x. Let's pick a starting\\npoint of 0.05 and our learning rate of 0.005. The first iteration is we find the derivative and we move by learning rate\\ntimes derivative. We get a new point, 0.1447, which is closer to the minimum. Then we take another iteration. We find the derivative and\\nsubtract learning rate times the derivative to get 0.1735. Then we can repeat many times until we get\\npretty close to the minimum. Notice something interesting, which is that in this algorithm, you never need it to solve e to the x minus 1 over x equals 0. You never need to solve\\nfor the derivative zero. You only do know the\\nderivative and then apply it in the algorithm when you're\\ntaking the updating step. That's a huge improvement.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Now that you learn gradient descent for\\none variable, let's learn it for more variables. We're going to start in the bottom and work our way up to the complete\\ngradient descent algorithm. Now earlier this week used\\nan example of temperature and a sauna to introduce\\noptimization with two variables. The goal was to move from your initial\\nstarting point to the coolest place in the room. Now in the previous lesson you\\nlearn how to solve a problem analytically by calculating the partial\\nderivatives and equating them to 0. The goal now is to find some alternative\\nway to solve this problem using gradients, just as you did in the previous video for\\na one variable function. So let's say that you start\\nhere in the sauna and you're going to take some random steps. So you take four random steps in four\\ndirections and you're going to see which one of\\nthe four gets you to the coldest place. So let's say it's this one, so\\nyou move a step in that direction. Now again you take four random steps and see which one of the four takes\\nyou to the coldest place. And you take that step and\\nyou can iterate, you can continue doing this until you\\nget either to the coldest place or at least somewhere close\\nto the coldest place. However, there's some small caveats, how do you pick the directions\\nin which to take steps? Is there any smart way to\\ntake these directions? Well just like in the previous\\nvideo with one variable, there is a smart way to do it and\\nit is using derivatives, except this time it's\\ngoing to be using gradients.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Again, in the previous video, you saw an okay way to take you closer to the\\ncoldest place in the sauna. However, there was a lot of random steps in that algorithm. We're going to do it\\nmore exactly and more mathematically just like we\\ndid before with one variable, we're going to define\\ngradient sending two variables.\\nIt's very similar. You start with an\\ninitial position, except now the\\ninitial position has two coordinates, x_0 and y_0. Before you used to\\ndraw the tangent or the derivative but now\\nwe're in two variables, so we have two tangents, this one over here. Notice that it's pretty steep. We're going to draw\\nthat magnitude in the floor in the direction that the derivative\\nis increasing. Because it's steep,\\nit's a big vector. Now we're going to\\ndo the same thing with the other component\\nof the gradient. We get a smaller vector\\non the floor because this second gradient is much flatter than the previous one. Now, let's take the sum\\nof these two vectors or the vector with\\nthe two coordinates, and that is the gradient. Now the gradient has an\\ninteresting peculiarity, is the direction of\\ngradient ascent. If you want to take a\\nsmall step and get to the hottest place you can on a small step you want to take in the direction\\nof the gradient. But we don't want to get hotter, we want to get colder\\nso we take it in the direction of the\\nnegative gradient. The direction of the\\nnegative gradient is the direction that\\nif you take one step, takes you to the\\ncoldest possible place, that you can get two\\nin one tiny step. We're going to take that step in the direction of the\\nnegative gradient. It's going to be a\\nsmall step so we multiply it by the\\nlearning rate again. We get x_0y_0 minus learning rate times gradient\\nand that's our new point, x_1, y_1, and that's\\na better point. Notice that this is exactly the same as gradient descent\\nin one variable, except instead of\\nthe derivative, we simply take the gradient. Now let's see how this works\\nfor the previous example, this is the formula\\nfor the temperature. It's 85 -1/90 x squared times x minus 6 times y\\nsquared times y minus 6. Let's run a couple of steps of the gradient\\ndescent algorithm. Let's start with the 0.50, 0.6. Now we have to\\ncalculate the gradient, which is the partial derivative\\nwith respect to x and the partial derivative\\nwith respect to y put together in a vector. We've already calculated\\nthese derivatives previously so here they are. All we need to do to\\nfind the gradient is to put them\\ntogether in a vector. Now, let's plug x\\nequals 0.5 and y equals 0.6 into this vector to get the gradient over\\nhere at this point. Now, simply take a step. You move in the direction of the gradient times\\nthe learning rate with a learning rate of 0.05. That gives you a new point\\nis not 0.50, 0.6 anymore. Now it's 0.5057 and 0.6047. You move in that direction. Now you're just a little bit\\ncloser to the minimum point. Now let's run this\\nalgorithm again. We calculate the gradient and we move in the direction\\nof that gradient. We add a new point. You simply repeat\\nthis many times. If you repeat this many times, then you are going to get really close to the minimum\\nreally fast. In short, the gradient\\nascent algorithm is very similar to the one we\\ndid in one variable. Now we have a function f of xy. The goal is to find the minimum. The first step is to define\\na learning rate Alpha, and to choose a starting\\npoint x_0, y_0. Step two is to update the\\nkth position by taking the k minus first\\nposition and subtracting the learning rate times the gradient and the k\\nminus first position. Then step three says repeat step two until you're close enough to\\nthe true minimum. As usual, you know when\\nthe true minimum is happening because it's when you move really slowly or maybe\\nnot move basically at all. Now, just like gradient descent in one variable grade decent, two or more variables still\\nhas the same drawbacks. For example you want to get\\nto this global minimum, but you may accidentally get into these local\\nminimums over here. The way to overcome this, at least with high probability, is to start at very\\ndifferent places. Notice that here we start at three different places\\nand each one of them took us to a different local minimum with one actually reaching\\nthe global minimum. But just like with one variable, there's no way to tell for sure if you've got\\nglobal minimum. But if you run in many times, probably what you got as the optimal solution\\nis pretty good.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"You may remember from earlier this week, that you solve a linear regression\\nproblem using power lines. But even with two variables and\\na simple problem, this turned out to be pretty cumbersome,\\nin this video. I'm going to show you how to solve the\\nexact same problem using gradient descent, and you're going to see how fast and\\neasy it is. So the power lines example\\nwas the following. You wanted to try to find a best fiber\\nline that reduces the cost of connecting to the power lines, and the problem boiled\\ndown to minimizing this function. So finding the optimal M and B, that will\\nfind the smallest value for E of MMB. And analytically you found that\\nit was one half and seven thirds, and it gave a value of 4.167. However we can also do this,\\nusing gradient descent. The idea is to minimize the sum of\\nsquares cost, and this was the gradient. So all you need to do is\\nfind the optimal M and B. For this, you have to first started\\nsome initial starting point, and then the points MB such that\\nthe cost is minimum is over here. So what you do is you take tiny steps\\ndescending until you find the minimum, mathematically the steps\\nare the following. First, you start with some point M0,\\nB0, that's some random slope, and some random wine to step,\\nand then, you iterate MK plus 1B. K plus one is the K plus one iteration. And that's the K iteration\\nminus alpha times that gradient on top, evaluated at MK, BK.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Let's take a look at the\\nparallel problem again. Here are the three\\nparallel lines and here is the line that tries to\\nfit them really close. The equation is y\\nequals mx plus b. Now we're going to relate that\\nto this plot at the right. The plot on the right is the cost of the power\\nlines. Let me show you how. Over here, we're\\ncalculating the areas of the squares and\\nrecall that the sum of these areas was the\\ncost of that fiber line. Now let's say that this was a numerical value\\ngiven by this segment. What's going to happen is\\nthat the plot on the right, is going to measure\\nthe square loss here in the vertical axis. The way it's going to\\ndo it is the following. This line is parametrized by the slope and the\\ny-intercept m and b. Let's put m and b in the\\ntwo axis here in the floor. Now, in the point mb, we're going to plot the\\nheight of the cost function. This point over\\nhere is going to be corresponding to the\\nline on the left. Now, this is a pretty\\ngood line fit. It's going to have\\na small error. Let's look another point. Let's look at an awful line. This one doesn't fit\\nthe points really well because it has a\\nreally high loss and therefore is going\\nto be at a point over here in the\\nerror function plot. The idea is the following. In the plot on the right, you're going to use gradient\\ndescent to get to the very bottom and to be able to\\nminimize the cost function. As you do that, you're going\\nto be changing m and b. As you change m and b, then the plot on the left, it's going to have\\na better line fit. That's how you solve linear regression using\\ngradient descent. Now let's see another example, one related to advertising. Let's say that you work\\nin an agency and you have a TV advertising budget. That relates to number of sales. Of course, if you\\nincrease the budget, the number of sales increases and if you decrease the budget, the number of sale decreases. You would like to be able\\nto predict the number of sales given a certain\\nadvertisement budget. You start looking at your\\ndata and the goal would be to predict the sales in\\nterms of the TV budget. The first tool you would try\\nto use is linear regression. In other words, try to find\\na line y equals mx plus b, such that if you plug in x, the TV budget, then you get y, or at least something\\npretty close. The idea is that you have\\na lot of observations. Let me show you how to do gradient descent with\\na lot of observations. Let's go back to our plot and let's say the TV budget is in the horizontal axis and number of sales in\\nthe vertical axis. You have a bunch of points,\\nlet's say n of them. Here we have x_1, y_1, x_2, y_2 up to x_n, y_n. Let's calculate\\nthe cost function. Let's look only at the\\nfirst point, x_1, y_1. What is the cost at that point? Well, is this length squared? Now what is this length? Well, the point on the line has the form x_1 and x_1 plus b. The reason is that the x-coordinate should be\\nthe same as in x_1, y_1. What is that distance? Well, it's simply y_1\\nminus mx_1 plus b. Or we can also write it as mx_1 plus b minus y_1 because\\nat the end of the day, we're going to be squaring it, so it doesn't really matter\\nif it's negative or positive. When we square it, we get the loss at that point. Now, we're going to add all the losses of\\nall the points. Let's actually take the average, so we divide it by n. There's\\nan extra two there and the reason that two is there is that when we take\\nthe derivative, the two in the exponent\\ncancels with this one, but it doesn't really matter. That could be a\\ntwo there or not. Because if we multiply the function by two\\nor divided by two, the minimum point is still going to be at\\nthe same m and b. Our total cost\\nfunction is L of m and b given by this average\\nof all the losses. Now all we're going to\\ndo is gradient descent. We started at point m_0, b_0. That gives us some random line that may or may\\nnot be a good fit. Then using that, we calculate the next iteration, m_1 b_1, which is m_0, b_0 minus the learning rate,\\ntimes the loss. That gives us a\\nslightly better line. Now we can iterate. We can now find m_2, b_2 using the same thing\\nexcept plugging m_1 b_1, and then m_3, b_3, and do it many times. Let's say we do it\\ncapital N times until we get a pretty good fit. That's the formal version of the gradient\\ndescent algorithm.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"In this video, I'm going to show you a nice way to condense all the partial derivatives into a vector known as the gradient. I'm going to show you what notation is used\\nfor the gradient. Now that you know what a\\npartial derivative is, you pretty much know\\nwhat a gradient is. Recall that if you have a\\nfunction in two variables, you can slice it in two ways, treating y as a constant and\\ntreating x as a constant. Each one gives you a different\\npartial derivatives. The first one gives you the partial derivative\\ndf over dx, which is 2x, and the second one gives you\\nthe partial derivative df over dy, which is 2y. So the gradient is simply the vector containing these\\ntwo partial derivatives, so basically the vector 2x, 2y. In general, we call\\nthe vector using the nabla letter nabla f, and that's just\\nthe collection of all the partial derivatives with respect to all the\\nvariables in the function. In this case a vector\\nof two entries because the function\\nhas two variables. But if the function\\nhas 17 variables, it would simply be a\\nvector of 17 entries, each one corresponding to\\nthe partial derivative of the function with respect to each one of the variables. These gradient, in\\nthis case 2x, 2y, is a pretty good description of the tangent plane\\nbecause it describes the slopes of the two lines\\nthat form that tangent plane. Now let's do a small exercise. Please find the\\ngradient of f of x, y at the point 2, 3. Feel free to think about it. The answer is nabla f equals 2x, 2y, except for x and y, you replace 2, and 3. This is 2 times 2, and 2 times 3, which is the vector 4, 6. That is the gradient of\\nthe function f of x, y equals x squared plus y\\nsquared at the point 2, 3.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Now that you've learned\\nhow derivatives work in more than one dimension, I'm going to show you how to\\nuse them in optimization, and the example we're\\ngoing to use is the same one we used\\nbefore of the sauna, except now the sauna\\nhas two dimensions. In Week 1, you saw a simple example where\\nyou are in a sauna and wanted to get to\\nthe coolest points since you're starting\\nto feel hot. But in this simple example, you are in the bench\\nin the sauna and you can only move\\nin two directions, to the left and to the right. Now let's look at a\\nmore realistic scenario where you can move in any direction because\\nyou're standing anywhere in this room. It's a two-dimensional sauna. Now, let's consider\\nthat the sauna is this five-by-five room\\nand you're standing here, but you can walk in pretty much any direction\\nthat you want. Again, you're starting to feel\\nhot so you want to get to the coldest place in the room and the temperature\\nlooks like this. The red points are hot and\\nthe blue points are cold. For each position xy, where x and y are\\nthe two coordinates, the temperature is the value of the function at that point, which is represented\\nby the height. The red points are high\\nbecause they represent hot points and the\\nblue points are low because they\\nrepresent cold points, and the goal is to find\\nthat coldest place, that blue point over\\nthere at the right. First, let's try the\\nsame approach you did on Week 1 to\\nget to the minimum. Let's say that\\nyou're standing over here and you're going to basically move around\\na little bit and see where does it get colder. Let's say you want to take\\na step in this direction, and in this direction, and in this direction, and from those you select\\nwhich one was colder. Clearly, it's the\\nblue direction, so let's move one step here. Now let's iterate. Let's continue doing\\nthat and moving in some random steps and\\nchecking out those steps which one took us\\nto a colder place and continue doing this. It's believable that if\\nyou do this for a while, you will be able\\nto take steps that take you to the coldest\\nplace in the room. How do you recognize the\\ncoldest place in the room? Well, just like before, it's a place where\\nif you walk in any direction, you get hotter. There's no way to get colder by taking a step in any direction. Another way to look at it\\nis if you were to take the tangent plane to the\\ntemperature function, this tangent plane is\\nparallel to the floor because the two partial\\nderivatives are both zero, and these are given by dT\\nover dx and dT over dy, where T is the temperature\\nfunction and x and y are the two\\ncoordinates in the sauna. Let's put some numbers here\\nand see how things go. Let's say that the\\ntemperature is this function 85 minus 1 over 90 times x squared times x minus 6 times y squared\\ntimes y minus 6. It's a little more\\ncomplicated than the previous one we had of\\nx squared plus y squared, but the math is very similar. Here is an exercise. Given this function,\\ntry to calculate the partial derivatives\\ndf over dx and df or dy. I encourage you to\\npause the video and take pen and paper and\\ntry to calculate them, and I will give you the answers. The answers are df over\\ndx is minus 1 over 90 times x times 3x minus 12 times y squared times y minus 6. That's the one you obtain by letting y behave\\nas a constant, and df over dy is minus 1\\nover 90 times x squared times x minus 6 times\\ny times 3y minus 12. Those two are the\\npartial derivatives. Now to find that minimum point, remember that you\\nhave to set them both equal zero and then\\nsolve for x and y. Now this is going to\\ngive us a lot of points, and many of them\\nwon't be the minimum, only one of them will be. We have to go through this slightly tedious\\napproach of looking at all the pairs xy for which these two partial\\nderivatives are zero, but that can be done. Take a look at df over dx. It's a product of\\nseveral things. For a product of several\\nthings to be zero, one of them at least\\nhas to be zero. The possibilities here\\nare that x is equal to 0 or that 3x minus\\n12 is equal to 0, in which case x is equal 4, or that y squared is equal to 0, in which case y is equal to 0, or that y minus 6 is equal to 0, in which case y equal 6. Similarly for df over dy, either x is equal to 0\\nor x is equal to 6 or y is equal 0 or 3y\\nminus 12 is equal to 0, in which case y equals 4. Let's put all these\\ncandidates together. For this candidate, if x is equal to 0\\nor y is equal to 0, then you have one point\\nwhere the gradient is zero, and the other points are\\nall denoted over here. Notice that some points\\nare outside of the sauna, so we don't care about those. Now, for x equals\\n0 or y equals 0, those are all maximum\\nbecause as you can see, all these points give\\n85 as the temperature, so those are not the minimum. The only minimum is\\nthis one over here, x equals 4, y equals 4, which is the point\\nwe were looking for. For this point, the\\ntemperature is 73.6, so this is the minimum. As you can see, just like\\nwe did for one variable, you set the derivative\\nequal to zero, that gives you a\\nbunch of candidates and then you check\\nthem individually. Over here, we simply said\\nboth derivatives equal to zero and get a lot of candidates and then\\ncheck them individually.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"So in this video, I'm going to show\\nyou something very interesting. The gradient is pretty useful to minimize\\nfunctions of two or more variables in the same way as the derivative was useful\\nto minimize functions of one variable. So let's go back to\\na function of one variable. This , f of x equals x squared. And now let's look at the function\\nwe've been studying in two variables, which is f (x,\\ny) equals x squared plus y squared. Now ,let's go back to\\nthe function of one variable. When you want to minimize this function,\\nthe minimum point is this point over here. And the way to find it is to find the\\npoint in which the slope is equal to 0. So we take the derivative,\\nsolve that derivative equal 0, and the solution is this\\nminimum point over here at 0. So what happens with more variables? Well, something very similar. The minimum point is this point over here. And notice that the tangent plane\\nis actually parallel to the floor, and that's going to happen in\\nevery local minimum and maximum, that the plane is parallel to the floor. Now, since we're not dealing with planes,\\nbut we're done with partial derivatives, let's actually draw the two\\npartial derivatives. Notice that they are these\\ntwo lines over here, and both have slopes of 0 because\\nthey're parallel to the floor. So in other words, the minimum of the\\nfunction f on the right happens when both of the slopes of the tangent lines\\ngiven by the partial derivatives are 0. And if you have a function of many,\\nmany more variables, say a function of 12 variables, then you're just going to\\nneed for 12 slopes to be 0. Those coresponding to all\\nthe partial derivatives. So now let's do a little bit of math. On the left, you want to solve the slope\\nequal 0, so the derivative equal 0. That means you want to solve for\\nf prime of x equals 0. Now, f prime of x for x squared is 2x, because that's\\nthe derivative of x squared. And when you solve for 2x = 0, you're\\nbasically getting the solution x = 0. And x = 0 is precisely\\nthis point over here. Now, what happens when we\\ndo this in more variables? Well, you want the two\\npartial derivatives to be 0. That means data for dx and dy are both 0. Now, what's df over dx, that's 2x. And df over dy is 2y. So you want 2x = 0 and 2y = 0. That's the same as the point, x, y equals\\n0, 0, which is this point over here. Now, this time, solving the system\\nof equations, 2x = 0 and 2y = 0,\\nwas pretty easy to get the point x, y = 0. Sometimes you may have to be\\nable a little bit of work, but basically, to find the minimums and\\nmaximums, all you have to do is set all the partial derivatives to 0 and\\nsolve that system of equations.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Welcome to Week 2. In Week 1, you learn everything about\\nfunctions with one variable. Now you're going to\\nlearn everything about functions with two\\nor more variables. For example things like a\\ntangent line in one-dimension will generalize to a tangent\\nplane in two dimensions. This can also be used in optimization in the\\nexact same way, and it also applies to machine\\nlearning. In the same way. However, optimizing functions in two or more variables can get very complicated\\neven for a computer. We're going to introduce methods\\nthat will speed this up. One method that we'll learn\\nis called gradient descent. Then I will show you how to use gradient descent to\\noptimize several functions. But for now, let's start with a very important concept,\\nthe tangent plane. In Week 1, you learned\\nabout derivatives of functions in one variable, for example this one, f of x equals x squared, which is a parabola. Visually you can identify\\nthe derivative at each point as the slope of the\\ntangent line of the point, for example at the\\n0.2 comma four, the slope of the\\ntangent is four. The tangent is this\\none over here. Now what happens when we have a function of two variables? Well, let's think\\nof a simple one. Let's think of f of x comma y equals x-squared\\nplus y-squared. Now the function has two\\ninputs and one output. Therefore, it needs to be\\nplot in three-dimensions. We're going to\\nplot it over here. X and y are the two horizontal\\naxis and the floor. Z, or f of x, y is plot as the height. Now the tangents are not lines. They're placed like\\nthis plane over here. That is called the\\ntangent plane. How do we get this\\ntangent plane? Well, essentially it's\\nthe same as the one DKs, except we have to\\ntake two cases. We have to actually\\ncut the space into planes and then calculate the\\ntangents on these planes. Let's say we fix y equals four. That means we're cutting\\nthe space here and we have the red parabola. Now let's cut the space\\nwith x equals two. Now we have another\\nred parabola. This parabola, we can\\nfind a tangent line. Once we have the two tangent\\nlines, then we have a plane. Because any two lines that cross actually represent\\na unique plane. That one over there\\nis the tangent plane. What were the\\ncalculations we did here? Well, when we fixed\\ny equals four, then we cut the space with a slice and we also\\nlet y equals four. That's why this function f of x4 became x squared plus four\\nsquared because y is four. That function is easy to\\ndifferentiate with respect to x, the derivative is two x, that's the slope of\\none of the lines. When we fixed x\\nequals two is when we slide the space with\\nthe other direction. Now you get the\\nfunction f of 2, y, which is two squared plus y squared by replacing\\nx equals two. You can differentiate\\nthat with respect to y. You get to y. Now this calculation seems\\nconfusing, don't worry. We're going to elaborate them a little more over the\\nnext few videos.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"So in order to really nail down\\nthe concept of a partial derivative, let's do one more example. Let's say that the function now is f(x,\\ny) = 3x squared times y cubed. So the task is to find the partial\\nderivative of f with respect to x. So in the following quiz,\\ncalculate the partial derivative of f with respect to x,\\nor die f over die x. So the way to do this is with the two\\nprevious steps that we learned. Step 1 is treat all the other\\nvariables as constants. So in this case, it's the variable y. That means this y cubed is a constant, so we're going to cover it with a box\\nto remember that it's a constant. Step 2 says, differentiate the function\\nusing the normal rules of differentiation. So how do we do this? Well, this 3 is a constant, so if you remember the multiplication\\nby scalar rule, we put this constant here and\\nthat will multiply the derivative. The next factor is x squared. So what's the derivative of\\nx squared with respect to x? It's simply 2x. And finally,\\nthis gray box is another constant, so it multiplies in the derivative\\nas the same constant. Now you remember that\\nthe gray box was y cubed. So all of a sudden we have the derivative\\nis 3 times 2x times y cubed. Simplifying it, its 6xy cubed, and\\nthat's the partial derivative. Now, let's do the other one. What is the partial derivative\\nof f with respect to y? That's what the next quiz is about. So for this one you do the exact same\\nthing except now you have to treat the x as a constant because you're taking\\nthe derivative with respect to y. So let's cover x squared with a box\\nto remember that it's a constant. And now let's take\\nthe derivative just like before. 3 is a constant, so it goes here,\\nthen the constant is a constant, so it goes here, and then the derivative\\nof y cubed is 3y squared. So now you remember that the constant\\nwas an x squared all along. So the derivative is 3x squared times 3y\\nsquared, which is 9x squared y squared, and that is the partial derivative\\nof f with respect to y.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"Before we get to partial derivatives\\nI wanted to have the following mental picture. Imagine that you have this function of\\ntwo variables plotted in 3D space and that you cut it with a plane like this,\\nwhat do you get? You get something like this? And in the cut you can see this red\\nparabola with a tangent over there, that's the partial derivative. Now you can also cut it in another\\ndirection, you can cut it like this and then you get also a red parabola and that\\nred parabola is a function of one variable. So you can draw tangents like\\nthe black tangent over there. So let me elaborate a little\\nmore if we take our function in the variables X and\\nY which is plot in 3D. And let's say we fix a value of Y in\\nthis case its 4, but it can be anything. So we treat y as a constant, all of\\na sudden if you treat y as a constant, this is no longer a function of two\\nvariables is now function of only one variable represented by this\\nred parabola over here. Now let's take some point in that red\\nparabola and let's draw a tangent like it's drawn here, that black line,\\nwhat is the slope of this line? The slope of this line is\\nthe partial derivative. So how do we calculate\\na partial derivative? Well, it's very simple, let's take\\nthe formula for the function f (x, y) equals x squared plus y squared,\\nsince we fixedy and it doesn't really matter\\nwhat value we fixed it too. We are treating y as a constant. So when you differentiate this function\\nnow this function is only a function of x. So when you take the derivative with\\nrespect to x which we're going to call di fover di ym where di is this symbol\\nover here, that's partial derivative. Then it's going to be 2x which\\nis the derivative of x squared with respect to x plus zero because\\n0 is the derivative of a constant. Remember that y squared is now a constant\\nso its derivative with respect to x is 0. So therefore the slope of\\nthis line over here is 2x, so you can take two partial derivatives\\nwith respect to x and with respect to y. So what happens when we\\ntreat y as a constant? Well, the partial derivative is 2x and\\nif we were to treat x as a constant, then the partial derivative is 2y because\\nthe derivative of x squared 0 and the derivative and y squared is 2y. In other words,\\nif you have a function f(x, y) you can take two partial derivatives. There's two ways to denote it,\\none is f sub x and the other one is di f over die x. And you can also take the partial\\nderivative respect to y and that's fy or di f over di y. So these two are the two partial\\nderivatives you can take of this function and as you can imagine if I have\\na function not of two variables, but let's say 10 variables,\\nthen I can take 10 partial derivatives, one with respect to each of the variables. Now those calculations weren't super\\nclear, let me do them in detail here. So let's say you have the function f of\\n(x, y) equals x squared plus y squared. And the task is to find the partial\\nderivative of x with respect to x and y. So let's do di f over di x first,\\nwe're going to try to find this two and as I said before, that's the partial\\nderivative notation over here. So when you want to find the partial\\nderivative of f with respect to x, there are two steps. The first step is to treat all the\\nvariables as constants, so in this case, y is the other variable so\\nwe're going to treat y as a constant. And step two says differentiate\\nthe function using the normal rules of differentiation. So let's start,\\nthis y squared over here is a constant. So it might as well be a 1,\\nwe can think of this as f(x, y) equals x squared plus 1 because the derivative\\nof 1 is 0 because it's a constant. So di f over di x is\\nsimply 2x because that's the derivative of x squared and\\nwhat's di f over di y? Well, now x is the one that\\nbehaves as a constant. So we can think of this as 1 plus\\ny squared, or 7 plus y squared, or anything you want. And the derivative of 1 plus y\\nsquared with respect to y, is 2y. So those are the partial derivatives.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent'),\n",
       " (\"In this video, we're going to go back to the power\\nline example from a previous lesson. However, this time, the example is\\nin two dimensions instead of one. The reason I really like this example\\nis because it introduces one of the most important machine learning models\\nout there called linear regression. And then, you're going to see how to solve a linear\\nregression problem using optimization. So in a previous lesson, you saw a power\\nline problem in one dimension where you need to determine where to place\\nyour house in order to connect to three power lines with the least cost. In this lesson, we work with a similar\\nproblem, but in two dimensions. So now the power lines\\nhave coordinates and they're going to be located\\non an xy plane like this one. So, here are the three positions\\nof the power lines and the problem is the following. You now want to find the optimal place for a fiber line connection that\\ngoes in a straight line in such a way that you reduce the total cost\\nof connecting to the three power lines. And how do you connect to the power lines? You can connect to each power line by\\nconnecting a wire to touch the fiber line, but the wire needs to be\\nparallel to the y-axis. And the cost for connection is the square of the length\\nof the wires just like before. So here are the three connections and\\nthe three costs, so the costs are the area\\nof the three square and the total cost is the sum of these areas. Now, the point is to find the perfect line\\nthat reduces this cost and minimize it. And that becomes a mathematical problem\\nbecause if you look at the line, the line can have an equation such as y\\nequals mx plus b, where m is the slope and b is the y intercept. And the goal is to find the optimal m and\\nb to minimize the sum of squares. Now, before we had a function with x and\\ny, now we have a function with and b, but it's the same thing. We're minimizing a function\\nof two variables, the function is the sum of the areas and\\nthe two variables are m and b. So let's actually calculate\\nthe function that we want to minimize. So, focus your attention\\non the blue square. Now the blue power line is\\nlocated on the coordinate 1,2 and it needs to connect to the fiber line. Where does it connect? In the 0.1 m plus b. Why is that the 0.1m plus b? Because if we let x equals 1,\\nthen mx plus b is equal to m plus b. So, the y coordinate of the point\\nof connection is m plus b and the y coordinate of the power line is 2. Therefore, the distance is m plus b minus\\n2 and the cost is m plus b minus 2 square. In a similar way, the cost for\\nthe yellow power line is 2m plus b minus 5 squared and for the green power\\nline is 3m plus b minus 3 squared. And therefore, the total cost is\\nthe sum of these three numbers. Now let's expand this, the first one,\\nm plus b minus 2 squared expands as this. The second one expands has this and\\nthe third one expands as this. And if we join similar terms,\\nwe get that E of m,b is 14m squared plus 3b squared plus 38 plus 12\\nmb minus 42m minus 20b. And that is the cost function\\nthat we want to minimize, so we want to find the m and b that make\\nthis expression e of mb the minimum, but we know how to do this already. All we have to do is calculate the two\\npartial derivatives  over m and  over  set them both equal to 0 and\\nthen solve for m and b. So let's start with the quiz. Please help me find the partial\\nderivative of E with respect to m. And the answer is 28m plus 12b minus 42. Now let's do another quiz, help me find the partial\\nderivative of E with respect to b. And the answer is 6b plus 12 and minus 20. So now that you have these\\ntwo partial derivatives, all you have to do is set them to be\\nequal to 0 and solve for m and b. You know how to do this from\\nthe linear algebra class. So, let's actually work it out and\\nonce we work it out that m and b are going to be the optimal\\nvalues to minimize E of m,b and they're going to give us the perfect line. So, how do we solve this? Well, let's start with\\nthe second equation and let's multiply it by 2 to get\\n12b plus 24 minus 40 equals 0. Now let's subtract it from the first\\nequation to get 4m minus 2 equals 0. This is equivalent to m equals two\\nfourth which is 0.5 or one half. Now let's take that m equals one half and\\nplug it back into the second equation to get 6b plus 12\\ntimes 0.5 minus 20 equals 0 or 6b plus 6 minus 20 equals 0 or\\n6b minus 14 equals 0. Which gives us the solution,\\nB equals 14 over 6 or 7 over 3. If this seemed a little confusing, please check the linear algebra class\\nwhere this explained in detail. So now we have the solution to the system, the solution to the system is m equals\\none half, and b equals 7over 3. Those are the two values that make\\nthose two partial derivatives 0. And if you plug them back to the equation,\\nyou get that E of one half, seven thirds is 4.167,\\nthat is the minimum value of the cost. And let's see how this looks in the graph. If you plot the line of\\nequation one half x plus seven thirds you get this line over here, and notice that indeed this line\\ngives some pretty small squares. So that's it,\\nthat's how you minimize the square loss. This problem is actually\\ncalled linear regression. Linear regression is a very, very important problem in machine learning\\nwhere you have a bunch of points and you try to find the closest line to them\\nand that's exactly what we did here. Now, notice one problem here, when we solve this equation of\\ntwo variables and two unknowns, that can be pretty hard, especially\\nif you were to have many variables. Solving an m by m system of\\nequations can be pretty expensive. Is there a faster way to do it? So the answer is yes, and\\nyou're going to see it next in the class. It's a method called grading\\ndescent which actually helps minimize functions in a pretty fast way. So stay tuned for the next few videos where you're\\ngoing to learn grading descent.\",\n",
       "  'machine learning::machine learning calculus::02 gradients and gradient descent')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"../Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# prompt_1 = \"\"\"Review Text: \"%s\"\n",
    "\n",
    "# Task: Your task is to convert this text into Basic Note Type (front/back) Anki flashcards. Prioritize information regarding the imaging features of diseases, unique imaging findings, and methods of differentiating similar disease entities. Ensure that each flashcard is clearly written, and adheres to the specified formatting and reference criteria.\n",
    "\n",
    "# Formatting Criteria:\n",
    "\n",
    "# - Construct a table with three columns: \"Front\", Back, \"Number\".\n",
    "# - Each row of the \"Front\" column should contain a single question testing the imaging features of disease, unique imaging findings, and methods of differentiating similar disease entities.\n",
    "# - The Back column should contain the succinct answer to the question in the corresponding row of the Front column.\n",
    "# - The \"Number\" column will serve to number each row, facilitating feedback.\n",
    "\n",
    "# Reference Criteria for each \"Statement\":\n",
    "\n",
    "# - Each flashcard should test a single concept.\n",
    "# - Each flashcard MUST be able to stand alone. Include the subject of the flashcard somewhere in the text.\n",
    "# - Keep ONLY simple, direct questions in the \"Front\" column.\n",
    "# - Clear concise language but if required give plenty of context.\n",
    "# - Output csv format like the example below.\n",
    "# - Output at least %d rows of question/answers.\n",
    "\n",
    "# Example:\n",
    "\n",
    "# Front;Back;Number\n",
    "# \"How is necrotic tissue identified in acute pancreatitis on a CT scan?\";\"Lack of contrast enhancement.\";1\n",
    "# \"Why should people create their own examples?\";\"Because Jake is too tired to think of good examples.\";2\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# text, tag = all_text[0]\n",
    "\n",
    "# # Split the text based on space and newline\n",
    "# split_text = re.split(r\"[\\s\\n]+\", text)\n",
    "\n",
    "# n_flashcards = int(len(split_text) / 65)\n",
    "# n_flashcards = max(3, n_flashcards)\n",
    "\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a teacher who is writing anki cards for his students.\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         # \"content\": f\"Please write an anki card on the following text:\\n{text}\",\n",
    "#         \"content\": prompt_1 % (text, 20),\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# print(model.device)\n",
    "\n",
    "# terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=1024,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# response = outputs[0][input_ids.shape[-1] :]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddalton/miniconda3/envs/flan-t5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-15 14:31:50,238 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:02<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Write Questions\n",
    "\n",
    "Structure: \n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Model\n",
    "    3. Parse Lecture Material \n",
    "    4. Generate Questions\n",
    "    5. Save to DataFrame\n",
    "    6. Save to File\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os, pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# variables\n",
    "\n",
    "\n",
    "prompt_1 = \"\"\"Review Text: \"%s\"\n",
    "\n",
    "Task: Your task is to convert this text into Basic Note Type (front/back) Anki flashcards. Prioritize information regarding the imaging features of diseases, unique imaging findings, and methods of differentiating similar disease entities. Ensure that each flashcard is clearly written, and adheres to the specified formatting and reference criteria.\n",
    "\n",
    "Formatting Criteria:\n",
    "\n",
    "- Construct a table with three columns: \"Front\", Back, \"Number\".\n",
    "- Each row of the \"Front\" column should contain a single question testing the imaging features of disease, unique imaging findings, and methods of differentiating similar disease entities.\n",
    "- The Back column should contain the succinct answer to the question in the corresponding row of the Front column.\n",
    "- The \"Number\" column will serve to number each row, facilitating feedback.\n",
    "\n",
    "Reference Criteria for each \"Statement\":\n",
    "\n",
    "- Each flashcard should test a single concept.\n",
    "- Each flashcard MUST be able to stand alone. Include the subject of the flashcard somewhere in the text.\n",
    "- Keep ONLY simple, direct questions in the \"Front\" column.\n",
    "- Clear concise language but if required give plenty of context. \n",
    "- Output csv format like the example below. \n",
    "- Output at least %d rows of question/answers.\n",
    "\n",
    "Example:\n",
    "\n",
    "Front;Back;Number\n",
    "\"How is necrotic tissue identified in acute pancreatitis on a CT scan?\";\"Lack of contrast enhancement.\";1\n",
    "\"Why should people create their own examples?\";\"Because Jake is too tired to think of good examples.\";2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# functions\n",
    "def clean_response(response_text):\n",
    "    \"\"\"Clean Response Text\n",
    "    Args:\n",
    "        response_text (str): response text from model\n",
    "    Returns:\n",
    "        response_df (df): cleaned response df\n",
    "    \"\"\"\n",
    "\n",
    "    # clear response text\n",
    "    # delete everything before Front;Back;Number\n",
    "    response_text = response_text[response_text.find(\"Front;Back;Number\") :]\n",
    "\n",
    "    # Use StringIO to create a file-like object from the string\n",
    "    _data = StringIO(response_text)\n",
    "\n",
    "    # Read the data into a pandas dataframe\n",
    "    df = pd.read_csv(_data, delimiter=\";\", quotechar='\"', on_bad_lines=\"skip\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 2. Load Model\n",
    "model_id = os.path.join(\"..\", \"Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  2%|         | 1/60 [00:05<05:15,  5.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  3%|         | 2/60 [00:11<05:28,  5.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  5%|         | 3/60 [00:16<05:15,  5.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  7%|         | 4/60 [00:23<05:48,  6.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  8%|         | 5/60 [00:28<05:09,  5.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 10%|         | 6/60 [00:33<04:58,  5.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 12%|        | 7/60 [00:36<04:02,  4.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 13%|        | 8/60 [00:42<04:20,  5.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 15%|        | 9/60 [00:47<04:10,  4.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 17%|        | 10/60 [00:51<03:58,  4.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 18%|        | 11/60 [00:55<03:43,  4.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 20%|        | 12/60 [01:00<03:41,  4.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 22%|       | 13/60 [01:04<03:31,  4.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 23%|       | 14/60 [01:09<03:39,  4.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 25%|       | 15/60 [01:15<03:38,  4.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 27%|       | 16/60 [01:24<04:30,  6.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 28%|       | 17/60 [01:28<04:00,  5.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 30%|       | 18/60 [01:33<03:52,  5.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 32%|      | 19/60 [01:38<03:31,  5.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 33%|      | 20/60 [01:42<03:11,  4.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 35%|      | 21/60 [01:47<03:10,  4.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 37%|      | 22/60 [01:53<03:16,  5.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 38%|      | 23/60 [01:57<03:07,  5.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 40%|      | 24/60 [02:02<03:01,  5.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 42%|     | 25/60 [02:09<03:15,  5.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 43%|     | 26/60 [02:14<03:00,  5.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 45%|     | 27/60 [02:18<02:43,  4.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 47%|     | 28/60 [02:23<02:39,  4.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 48%|     | 29/60 [02:27<02:27,  4.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 50%|     | 30/60 [02:33<02:33,  5.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 52%|    | 31/60 [02:39<02:30,  5.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 53%|    | 32/60 [02:47<02:49,  6.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 55%|    | 33/60 [02:50<02:24,  5.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 57%|    | 34/60 [02:59<02:43,  6.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 58%|    | 35/60 [03:05<02:33,  6.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 60%|    | 36/60 [03:10<02:22,  5.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 62%|   | 37/60 [03:15<02:08,  5.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 63%|   | 38/60 [03:20<01:57,  5.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 65%|   | 39/60 [03:23<01:40,  4.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 67%|   | 40/60 [03:29<01:40,  5.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 68%|   | 41/60 [03:34<01:35,  5.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 70%|   | 42/60 [03:42<01:48,  6.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 72%|  | 43/60 [03:48<01:41,  5.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 73%|  | 44/60 [03:54<01:37,  6.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 75%|  | 45/60 [03:59<01:25,  5.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 77%|  | 46/60 [04:01<01:05,  4.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 78%|  | 47/60 [04:06<00:59,  4.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 80%|  | 48/60 [04:10<00:52,  4.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 82%| | 49/60 [04:13<00:45,  4.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 83%| | 50/60 [04:17<00:41,  4.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 85%| | 51/60 [04:21<00:37,  4.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 87%| | 52/60 [04:24<00:30,  3.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 88%| | 53/60 [04:31<00:33,  4.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 90%| | 54/60 [04:38<00:31,  5.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 92%|| 55/60 [04:44<00:27,  5.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 93%|| 56/60 [04:48<00:20,  5.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 95%|| 57/60 [04:54<00:16,  5.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 97%|| 58/60 [04:59<00:10,  5.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 98%|| 59/60 [05:06<00:05,  5.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "100%|| 60/60 [05:12<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 3. Parse Lecture Material\n",
    "errors = list()\n",
    "first_iteration = True\n",
    "for i, (text, tag) in tqdm(enumerate(all_text), total=len(all_text)):\n",
    "\n",
    "    # Split the text based on space and newline\n",
    "    split_text = re.split(r\"[\\s\\n]+\", text)\n",
    "\n",
    "    n_flashcards = int(len(split_text) / 180)\n",
    "    n_flashcards = max(2, n_flashcards)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a teacher who is writing anki cards for his students.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # \"content\": f\"Please write an anki card on the following text:\\n{text}\",\n",
    "            \"content\": prompt_1 % (text, n_flashcards),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 4. Generate Questions\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1] :]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    # print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "    # 5. Save to DataFrame\n",
    "    if first_iteration:\n",
    "        df_responses = clean_response(response_text=response)\n",
    "        df_responses[\"tag\"] = tag\n",
    "        first_iteration = False\n",
    "    else:\n",
    "        try:\n",
    "            df_response = clean_response(response_text=response)\n",
    "            df_response[\"Deck_ID\"] = tag\n",
    "            df_responses = pd.concat([df_responses, df_response])\n",
    "            del df_response\n",
    "        except:\n",
    "            print(f\"Error in response {i}\")\n",
    "            errors.append(response)\n",
    "    # 6. Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.drop([\"Number\"], inplace=True, axis=1)\n",
    "df_responses.to_csv(\"questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['machine learning::machine learning calculus::01 derivatives and optimization',\n",
       "       'machine learning::machine learning calculus::03 optimization in neural networks and newtons method',\n",
       "       'machine learning::machine learning calculus::02 gradients and gradient descent'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[\"tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mathematics for ml::machine learning calculus::01 derivatives and optimization',\n",
       "       'mathematics for ml::machine learning calculus::03 optimization in neural networks and newtons method',\n",
       "       'mathematics for ml::machine learning calculus::02 gradients and gradient descent'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[\"Deck_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of levels 3\n",
      "N of levels 3\n",
      "N of levels 3\n"
     ]
    }
   ],
   "source": [
    "for id in df_responses[\"Deck_ID\"].unique():\n",
    "    print(f\"N of levels {len(id.split('::'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean df from nans\n",
    "df_responses.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anki package created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import genanki\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Example DataFrame with questions and answers\n",
    "output_path_decks = os.path.join(\"..\", \"generated_decks\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path_decks, exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to generate unique model and deck IDs\n",
    "def generate_id():\n",
    "    return random.randrange(1 << 30, 1 << 31)\n",
    "\n",
    "\n",
    "# Define the model for the notes\n",
    "model_id = generate_id()\n",
    "my_model = genanki.Model(\n",
    "    model_id,\n",
    "    \"Simple Model\",\n",
    "    fields=[\n",
    "        {\"name\": \"Question\"},\n",
    "        {\"name\": \"Answer\"},\n",
    "    ],\n",
    "    templates=[\n",
    "        {\n",
    "            \"name\": \"Card 1\",\n",
    "            \"qfmt\": \"{{Question}}\",\n",
    "            \"afmt\": '{{FrontSide}}<hr id=\"answer\">{{Answer}}',\n",
    "        },\n",
    "    ],\n",
    "    css=\"\"\"\n",
    "    .card {\n",
    "      font-family: arial;\n",
    "      font-size: 20px;\n",
    "      text-align: center;\n",
    "    }\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create decks\n",
    "generated_decks = list()\n",
    "d_decks = dict()\n",
    "for deck_id in df_responses[\"Deck_ID\"].unique():\n",
    "    n_levels = len(deck_id.split(\"::\"))\n",
    "    for level in range(0, n_levels, 1):\n",
    "        deck_name = \"::\".join(deck_id.split(\"::\")[: level + 1])\n",
    "        if deck_name not in generated_decks:\n",
    "            created_deck = genanki.Deck(generate_id(), deck_name)\n",
    "            generated_decks.append(deck_name)\n",
    "            d_decks[deck_name] = created_deck\n",
    "\n",
    "# Add flashcards to decks\n",
    "for deck_id in df_responses[\"Deck_ID\"].unique():\n",
    "    # Query df\n",
    "    query = f\"Deck_ID == '{deck_id}'\"\n",
    "    df_query = df_responses.query(query)\n",
    "\n",
    "    # Add flashcards to deck\n",
    "    for index, row in df_query.iterrows():\n",
    "        front = str(row[\"Front\"]) if row[\"Front\"] is not None else \"\"\n",
    "        back = str(row[\"Back\"]) if row[\"Back\"] is not None else \"\"\n",
    "        note = genanki.Note(model=my_model, fields=[front, back])\n",
    "        d_decks[deck_id].add_note(note)\n",
    "\n",
    "# Create a package and write it to an .apkg file\n",
    "all_decks = list(d_decks.values())\n",
    "package = genanki.Package(all_decks)\n",
    "\n",
    "# Write to file\n",
    "\n",
    "if specialization is not None:\n",
    "    package.write_to_file(\n",
    "        os.path.join(output_path_decks, f\"{specialization}.{course}.apkg\")\n",
    "    )\n",
    "else:\n",
    "    package.write_to_file(os.path.join(output_path_decks, f\"{course}.apkg\"))\n",
    "\n",
    "print(\"Anki package created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
