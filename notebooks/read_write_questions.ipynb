{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\n",
    "\n",
    "Structure:\n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "import sys, os, numpy as np, pandas as pd\n",
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# variables\n",
    "\n",
    "specialization = \"mathematics_for_ml\"\n",
    "course = \"machine-learning-linear-algebra\"\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\", specialization, course)\n",
    "\n",
    "\n",
    "# functions\n",
    "\n",
    "\n",
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 15:39:02,114 - INFO - Retrieving data for course machine-learning-linear-algebra \n",
      "2024-08-12 15:39:02,115 - INFO - Number of files: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mathematics_for_ml/machine-learning-linear-algebra/01_week-1-systems-of-linear-equations\n",
      "../data/mathematics_for_ml/machine-learning-linear-algebra/03_week-3-vectors-and-linear-transformations\n",
      "../data/mathematics_for_ml/machine-learning-linear-algebra/02_week-2-solving-systems-of-linear-equations\n",
      "../data/mathematics_for_ml/machine-learning-linear-algebra/04_week-4-determinants-and-eigenvectors\n"
     ]
    }
   ],
   "source": [
    "all_text = list()\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    # week folders\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    print(folder_path)\n",
    "    lecture = folder.split(\"/\")[-1]\n",
    "    lecture = re.sub(r\"week-\\d+-\", \"\", lecture)\n",
    "    tag = f\"{specialization.replace('_', ' ')}::{course.replace('-', ' ')}::{lecture.replace('-', ' ').replace('_', ' ')}\"\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        # lesson folder\n",
    "        for lesson in os.listdir(folder_path):\n",
    "            lesson_path = os.path.join(folder_path, lesson)\n",
    "            for file in os.listdir(lesson_path):\n",
    "\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(lesson_path, file)\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        text = f.read()\n",
    "                        all_text.append((text, tag))\n",
    "logging.info(f\"Retrieving data for course {data_path.split('/')[-1]} \")\n",
    "logging.info(f\"Number of files: {len(all_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\">> As I mentioned before, equations\\nbehave a lot like sentences as they are statements that give you information. In this video, you will learn\\nwhat a linear equation is and what a system of linear equations is. As a matter of fact, you will be solving\\nyour first system of linear equations, which is extracting all the possible\\ninformation from that system. Just like with systems of sentences,\\nsystems of linear equations can also be singular or non singular based on\\nhow much information they carry. And as you already learned these\\nconcepts with real life sentences, you are more than ready to\\ntackle them with equations. In the previous video, you saw sentences\\nsuch as between the dog and the cat, one is black. For the rest of the course, youll focus on sentences that carry\\nnumerical information, such as this one. The price of an apple and a banana is $10. This sentence can easily be\\nturned into equations as follows, if a is the price of an apple and\\nb is the price of a banana, then the equation stemming from\\nthe sentence is a + b = 10. Of course, you might wonder why an apple\\nand a banana together cost a whopping $10. To keep our example simple, im going to use these hypothetical\\nprices with nice whole numbers. So while the thought of a $10 apple and\\nbanana might make your wallet shutter, rest assured it's all in the name\\nof mathematical simplicity. Now here's the first quiz in which you\\nwill be solving the first system of linear equations in this class. The problem is the you\\nare going to a grocery store. But this is a very peculiar grocery store. In this store, the individual items don't\\nhave information about their prices. You only get the information about the\\ntotal price when you pay in the register. Naturally, being a math person,\\nas you are, you're interested in figuring\\nout the price of each item. So you keep track of of the total prices\\nof different combinations of items in order to deduce the individual prices. So the first day that you go to the store,\\nyou bought an apple and a banana and they cost $10. The second day you bought an apple and\\ntwo bananas and they cost $12. And the question is,\\nhow much does each fruit cost? So several things may happen. You may be able to figure out\\nthe price of the apple and banana, or you may conclude that you don't have\\nenough information to figure this out. Or even more, you may conclude that there's a mistake\\nwith the prices giving this information. All of these are options in the quiz, and\\nthe solution is that apples cost $8 and bananas cost $2 each. Why? Well, from day one, you can see\\nthat an apple plus a banana is $10. From day two, you can see that\\nan apple plus two bananas is $12. So what was the difference\\nbetween day one and day two? While, in day two, you bought\\none more banana than in day one. Also, in day two,\\nyou paid $2 more than in day one. Thus, you can safely\\nconclude that that extra banana you bought on day two cost $2. The extra $2 you paid on day two were\\nbecause of that extra banana you bought on day two. And now that you know that bananas cost\\ntwo, well, how much do apples cost? Well, from day one, you can see that\\nan apple and a banana cost $10. So if a banana costs $2, then the\\nremaining $8 must correspond to the apple. Thus, each apple costs $8,\\nand each banana costs $2. To start, you'll be solving your\\nfirst system of three equations with three unknowns. And the problem is a familiar one. You're in a similar store as before. But now your goal is to\\nfind the prices of three. An apple, a banana, and a cherry. So you go to the store\\nthree days in a row. On the first day, you bought an apple,\\na banana, and a cherry, and paid $10. On the second day, you bought an apple,\\ntwo bananas, and a cherry, and paid $15. And on the third day, you bought an apple,\\na banana, and two cherries, and paid $12. Now, the question is,\\nhow much does each fruit cost? So, the way to solve this is very similar\\nto the way to solve the previous systems, with two equations and two unknowns. The first equation says that an apple,\\na banana, and a cherry cost $10. The second one says that the same\\narrangement plus an extra banana costs $15. Therefore, that extra banana\\nmust cost those extra $5. And the third one says that the same\\narrangement of day one plus an extra cherry costs $12. Therefore, that extra cherry\\nmust cost that extra $2. So banana is 5 and cherry's 2. We still need to find\\nthe price of the apple. Now, look at the first equation. If the banana is five and\\nthe cherry is two, and the three of them cost ten,\\nthen the apple must cost three. So our solutions are the price of an apple\\nis three, the price of banana is five, and the price of a cherry is two. The system of equations you just\\nsolved is this one over here, system of equations 1,\\nwhich has as equations a + b + c = 10, a + 2b + c = 15, and a + b + 2c = 12. And the solution you got was a = 3,\\nb = 5, and c = 2. Now, here's quiz two. The scenario is the same, except\\nthe prices in the store are a little different, and you also bought\\ndifferent quantities of fruits. On day one, you bought an apple and\\na banana and they cost $10. On day two, you bought two apples and\\ntwo bananas and they cost $20. The question is,\\nhow much does each fruit cost? Remember that the options of not\\nhaving enough information or having a mistake in the information\\ngiven are both valid as well. For this problem, the solution is that there is not enough\\ninformation to tell the actual prices. And why is this? Well, you can use a similar\\nreasoning than before. From day one, you can deduce that\\nan apple and a banana cost $10. From day two, you can deduce that\\ntwo apples and two bananas cost $20. But these two equations\\nare the same thing. They may not look the same, but\\nin disguise theyre the exact same thing. Because you see, if one apple and\\none banana cost $10, then twice of one apple and one banana\\ncosts twice of $10, which is $20. So two apples and two bananas cost $20. Therefore, the system is redundant because\\nit basically has the same equation twice. Its like that system of sentences where\\nboth sentences stated that the dog was black. The system didn't carry\\nenough information. Now, what are the solutions to the system? Well, because the system doesnt\\ncarry enough information, the system has infinitely many solutions. Any two numbers that add to ten\\nare a solution to the system. So, for example, if the apple's 8 and\\nthe banana's 2, then that works, because apple plus banana is 10,\\nand 2 apples plus 2 bananas is 20. But if they're 5 and 5, that also works. If they're 8.3 and 1.7, that also works. And even saying that the apples are free\\nand the bananas are ten works too. So this system has infinitely\\nmany solutions because you simply don't have enough information. You don't have the two equations to narrow\\nit down to one single solution like you had with the complete system. And now you're ready for a final quiz. Similar scenario, except the first day\\nyou bought an apple and a banana and they cost $10. And the second day you bought two apples\\nand two bananas and they cost $24. Can you figure out how\\nmuch each fruit costs? And remember, there are still\\nthe options of not enough information or I'm mistaking the information. And the answer here is that\\nthere's no solution, why? Well, in the same fashion as before,\\nif one apple and one banana cost $10, then two apples and\\ntwo bananas must cost $20. But the store charged you $24 for\\ntwo apples and two bananas. Where are those four extra dollars? If you assume that there are no extra\\nfees for buying more than one fruit or discounts or anything of that sort, then you must conclude that that extra\\nmoney must be due to a mistake with the register when you checked out\\nin at least one of the two days. This means that these two equations\\ncontradict each other, just like the two sentences, the dog is black and\\nthe dog is white contradicted themselves. And this concludes that\\nthe system has no solutions. So here's our recap. You solved three systems of equations. The first one has the equations a,\\nb ten and a two b twelve, because the price of an apple and\\na banana was ten, and the price of apple and\\ntwo bananas were twelve. The second one has the equations\\na + b = 10 and 2a + 2b = 20. And the third one has the equations\\na + b = 10 and 2a + 2b = 24. The first one had a unique solution,\\nwhich was a eight and b two for a is the price of an apple and\\nb the price of banana. The reason this system has a unique\\nsolution is because both equations give you one different piece of information. Thus, you're able to narrow down\\nthe solution to one unique solution. For this reason,\\nthe system is complete and non singular. The second system has\\ninfinitely many solutions, which are any two numbers that add to ten. In this system,\\nthe two equations are the exact same one. So you never had a second equation to\\nhelp you narrow down the solution to a unique one. This means the system is redundant and\\nsingular. And finally, the third system has no solution because\\nthe two equations contradict each other. Therefore, the system of equations\\nis contradictory and singular. So, as you see, we are using the same\\nterminology as with systems of sentences and everything works\\nin the exact same way. Now you're ready to solve more systems. Here are three systems to solve. Remember that just like before,\\nsome of them may not have a solution. Some of them may have\\nan infinite number of solutions. So, if that's the case, please state it, even if any of the options already\\nshows a solution to the system. And so here are the solutions. System two has infinitely many solutions,\\nwhy? Well, you look at equation one. It says a + b + c = 10. And from equation two,\\nthere's an extra c and an extra five, so that c must be equal to five. The price of a cherry is five. However, when you go from equation two\\nto three, there's also an extra five and also an extra c. So equation 3 brings\\nnothing new to the table. When you replace c = 5 on the first\\nequation, you get a + b = 5. And actually, any triplet where\\nthe third number is five and the first two add to five works. So all of these are solutions\\nto that system. That system has infinitely many solutions. Now, let's look at system three. System three has no solutions, why? Well, from the first and\\nthe second equation, c is equal to five. However, from the second and the third, c = 3, because from the second\\nequation to the third one, you bought an extra cherry and\\nyou paid an extra $3, so c = 5, but c = 3. That's a contradiction. So system three has no solution. What about system four? Well, system four has\\ninfinitely many solutions. Why?\\nBecause, as you can see, the second equation is two\\ntimes the first one, and the third equation is\\nthree times the first one. So the first one is really the only\\nequation here that matters. Or equivalently the second or third. But only one of them matters. So any three numbers that\\nadd to ten would work. So, for example, 001-07-1910. Any three numbers that add\\nto ten are a solution. So we have infinitely many solutions. And finally, some clarification. You may have noticed the word\\nlinear equation several times. What does that mean? Well, linear equation can be anything\\nlike a + b = 10, 2a + 3b = 15, 3.4a- 48.99b + 2c = 122.5,\\nanything like that. And notice that it can have\\nas many variables as we want. But there's a special rule that must\\nbe followed in a linear equation, variables a, b, c, etc. We're only allowed to have numbers or\\nscalars attached to them. And there's also an extra\\nnumber all by itself, like the 122.5 here allowed\\nto be in the equation. So in short, you can multiply the variable\\nby scalars and then add them or subtract them, and then add a constant. And that's it. So what's an equation that's non-linear? Well, non-linear equations\\ncan be much more complicated. They can have squares like a squared,\\nb squared. They can have things like sine, cosine,\\ntangent, arctan, anything like that, powers like b to the five. They can have powers like two to the a or\\nthree to the b. And furthermore,\\nyou can actually multiply the a's and b's. In linear equations,\\nyou can only add them. But in a non-linear equation,\\nyou can have ab squared, b divided by a, 3 divided by b. Things like logarithms,\\nanything along those lines. So, linear algebra is the study of linear\\nequations like the ones in the left. And since they're much simpler, then there\\nare many things you can do with them, such as manipulating them and\\nextracting information out of them. So we're only going to worry about\\nthe linear equations in the left. And the reason it's called linear algebra\\nis because it's the study of linear equations.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Now that you learned what a system\\nof linear equations is and when they are singular or non singular,\\nit is time for some visualizations. It turns out that linear equations\\ncan easily be visualized as lines in the coordinate plane. Well, this is because\\nyou have two variables. If you have three variables,\\nthey are planes in space. And if you had more variables,\\nthey look like high dimensional things. But let's not worry about that yet. So since linear equations can be\\nrepresented as lines, then systems of linear equations can be represented as\\narrangements of lines in the plane. This way you can visualize their\\nsolutions and their singularity or non singularity in a much clearer way. Okay, so how can you visualize, for\\nexample, the equation a + b =10 as a line? First, let us get a grid in which\\nthe horizontal axis represents a, which is the price of an apple, and\\nthe vertical axis represents b, which is the price of a banana. Now let's look at solutions\\nto this equation a + b =10. In other words,\\npairs of numbers that add to ten and what you'll do is put them in this plot. So two obvious solutions are the point\\n(10,0) so the a coordinate, the price of an apple is 10. And the b coordinate, the price of\\na banana, is 0 because 10 + 0 is 10. Another obvious solution is the point\\n(0,10) where a is 0 and b equals 10. Other solutions are the point\\n(4,6) because 4+6 = 10. So this is a equals 4 and b =6. Or the (8,2), where a =8 and b =2. Notice that you can also have negative\\nsolutions, for example, (-4,14). Now this makes no sense in the word\\nproblem because an apple cannot cost minus four. But these are two numbers that add to ten,\\n-4 + 14 =10. So this is a legitimate\\nsolution to the equation. And you can also have negative\\nsolutions like (12, -2). Now notice that all these\\npoints form a line. In fact, every single point in this\\nline is a solution to the equation. So you can then associate the equation\\na + b =10 with this line. Now let's do another equation. Say the equation a + 2b =12. That means points for which the horizontal coordinate plus two\\ntimes the vertical coordinate add to 12. So some solutions for\\nthis equation are the point (0,6), since zero plus two times six equals 12. The (12,0) because 12\\nplus 2 times 0 is 12. The (8,2),\\nbecause eight plus two times two is 12. And again, negative solutions like (-4,8),\\nfor example, because minus four plus\\ntwo times eight is 12. And again, these points form a line and every point in the line is\\na solution to this equation. So the line is associated\\nwith the equation a + 2b =12. One small aside, you may be familiarized\\nwith the notions of slope and y intercept in a line,\\nthe slope is the ratio of rise over run, which in the line of the left is -1. As for every unit you move to the right,\\nthe line moves one unit down. So the down is the minus the negative. For the line on the right,\\nthe slope is minus 0.5 because for every unit you move to the right,\\nline moves half a unit down. For the y intercept for\\nthe line on the left, it is 10, as this is the height of the intersection\\nbetween the line and the vertical y axis. And for the line on the right, it is 6. Now here's what's interesting. Each equation is associated to a line. So what happens with\\nthe system of two equations? Well, the system of two equations is\\nsimply associated with the two lines in the same plane. Notice that these two lines\\ncross at a unique point. The point (8,2) for a =8 and b =2. The point is precisely the unique\\nsolution to that system of equations. This is exactly what we got\\nbefore algebraically but now we can see it geometrically. Now that we know how to plot\\nthe equation of the line a + b =10, let's try another one. How about 2a +2b = 20? We'll notice that that line still\\ngoes to the points (0,10) and (10,0). As the line is defined by only two points,\\nthen the line is exactly the same, and as the one with equation a +b =10. Recall that a few lessons ago you\\nlearned that equations a +b =10 and 2a +2b =20 carry the same information. This is a visual confirmation for that. And now when we want to find\\nthe solution to this set of equations, there is no single intersection point. Instead, the two lines overlap each other. They are the same line. And what happens now is that\\nevery point that belongs to both lines is a solution to the set of\\nequations a +b =10 and 2a + 2b =20. That means we have\\ninfinitely many solutions, because every point in\\nthat line is a solution. And finally,\\nlet's look at another system of equations. The one with equations a +b =10 and\\n2a + 2b =24. So, let's plot the one on the right. Notice that the line's equation 2a + 2b\\n=24 goes through the points (0,12) and (12,0). Because 0 times 0 plus 2 times 12 is\\n24 and 2 times 12 plus 2 times 0 is 24, and therefore it has to\\nbe this line over here. It's very similar to the original line,\\nexcept it's translated up by two units. So when we try to get the solutions to\\nthis set of equations, take a look. The system of two equations is simply\\nassociated to these two lines in the same plane that are parallel. Parallel lines never meet. So there are no solutions to this system. There's no point that\\nbelongs to both of the lines. So the system has no solutions. Let's now summarize what\\nyou've seen in this video. There are three systems of equations. The first one has equations a + b =10 and\\na +2b =12. The second one has equations a + b =10 and\\n2a + 2b =20. And the third one has equations\\na + b =10 and 2a +2b =24. And here are the plots for\\nthe three the first one corresponds to two lines that intersect\\nat the unique (8,2). So that's the unique\\nsolution to the system. The second one corresponds to two\\nlines that are exactly the same line, corresponding to a system that\\nhas infinitely many solutions. And the third one corresponds to\\ntwo parallel lines that never meet, which means the system has no solutions. So we can use the exact same nomenclature\\nas we used with equations and with systems of sentences. Since the first system has a unique\\nsolution, it is complete and it is non singular because every line\\nbrings something new to the table. The second system has infinitely many\\nsolutions because the second line is exactly the same as the first one. So the system is redundant and singular. The second line brings nothing new to\\nthe table because it's exactly the same as the first line. And finally, since the third system\\ncorresponds to two lines that never meet, it means the second equation\\ncontradicts the first one. We have no solutions. Therefore, the system is contradictory and\\nsingular. Now you're ready for another quiz. Problem one says, which of the following plots correspond\\nto the systems of equations three? a +2b =8 and 2a- b =3. And problem two says,\\nby looking at the plot for problem one, do you conclude that the system\\nis singular or non singular? And the answer is this. In order to plot these lines, you can\\nnotice that the line with equation three, a + 2b, goes to the (0,4) and (8/3,0). And the line with equation 2a- b\\n=3 goes to the (0,-3) and (3/2,0). And notice that the two lines cross\\nat the (2,1), which is precisely the unique solution to the system\\nof equations that's a=2 and b =1. Since the two lines intersect at a unique\\npoint, then the system is non-singular. In a similar way, a linear equation\\nin three variables is represented by a plane in three dimensional space. At the right you have threedimensional\\nspace with three axis, the axis a, which is a horizontal axis,\\nthe axis b which is vertical. And the axis c which should stem from the\\nscreen and go all the way to your nose. So how would the plot of, for example, the\\nequation a + b + c=1 look like in space? Well, let's look at some points\\nthat would belong to this plot. For example, the (1,0,0) belongs\\nto this plot because 1+0+0 = 1. That's the point where the a coordinate\\nis 1 and the other two are 0. The point (0,1,0) also belongs\\nto here because 0 +1 +0 is 1. And finally, the point (0,0,1)\\nalso belongs to this plot. Now, three points define a plane. And actually the entire plane\\nthat passes through those three points is the set of solutions\\nof the equation, a+ b + c =1. So in the same way as a linear equation\\nwith two variables correspond to a line in the plane, a linear equation on three\\nvariables correspond to a plane in space. Now in the particular case where the\\nconstant equation is zero, for example, in the equation 3a -5b +2c =0, the plane must go through the origin\\nwhich is the point (0,0,0). And the reason for this is because if\\nwe set a equals zero, b equals zero and c equals zero,\\nthis is a solution to the equation. Because the sum of three times zero\\nplus five times zero plus two times zero is equal to zero. So now in the same way as we used\\nto intersect lines to get points as the solutions to systems equations,\\nyou can also intersect planes. Check this out. So here we're not so concerned about\\ngetting the right visualization because these things are hard to\\nvisualize in two dimensions. However, we're going to be concerned\\nabout how the intersections of these planes appear. So for this system of equations, a + b +\\nc =0, a +2 b + c =0, and a + b + 2c =0. Let's look at the first equation that\\ncorresponds to some plane that goes to the point (0,0,0). The second equation corresponds to some\\nother plane that goes to the point (0,0,0). And these two planes intersected a line. And the third one corresponds to another\\nplane that passes to the point (0,0,0). And the three planes intersect at a single\\npoint, which is precisely the (0,0,0). So this is important because\\nthis is a non-singular system, it has a unique solution, and\\nthat unique solution is the (0,0,0). Now, let's look at system two. The first equation is again a plane, and\\nthe second equation is another plane, and both of them go through the origin and\\nthey intersect at a line. Now, this system is singular. So what happens is that the third\\nplane also crosses the other two and the origin, but it actually\\ncrosses the other two at a line. So these are three planes that all\\nof them go through the same line. So the set of solutions is not\\njust a point, it's an entire line. So there are multiple solutions\\nto the system of equations, which means that the system is singular. And finally, we have this other system\\nwhere the equations are a + b + c =0, 2a +2b +2c =0, and 3a + 3b +3c =0. The first one corresponds to a plane. Now, as you've seen before, the second equation is just\\na multiple of the first one. So it's actually the same equation. So it's not surprising that it\\ncorresponds to the exact same plane. And the third equation also\\ncorresponds to the exact same plane. Therefore, the set of solutions to this\\nsystem is every single point in the plane. There are multiple solutions, and\\ntherefore this system is again, singular. Coming up next, you'll have an opportunity\\nto use some interactive tools that let you explore these concepts in a handson way. The first one allows you to build and manipulate two by two\\nsystems of equations. Visualize them as lines in the plane, and see how changes to the system impact\\nthe number of solutions to your system. In the second tool, you can choose\\nbetween several systems of equations in threedimensional space and then rotate\\nthose equations in three dimensions. You're more than welcome to explore these\\ntools on your own, but I've also included instructions on how to use them and some\\nsuggestions of activities to complete. Enjoy trying them out, and\\nI'll see you once you're done.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Welcome to Week 1 of the\\nlinear algebra course. This week, you will\\nlearn what a system of linear equations is and some\\nof its representations. One of them is as a set\\nof lines in the plane. Another one is as an array\\nof numbers called a matrix. Furthermore, you will learn what singular or non-singular\\nsystems are, and why is this so important\\nin linear algebra? Linear algebra has\\napplications in many fields of science\\nand technology, and machine learning is\\ncertainly not the exception. In fact, it is not a stretch to say that\\nlinear algebra is the most useful and widespread math field in machine learning. In this video, you'll\\nlearn about some of the best applications of linear algebra in machine learning starting from the most\\npopular application that you know by now,\\nlinear regression. This course is first and\\nforemost, a math course, but I'm guessing you're\\nhere because you're interested in further\\nstudies in machine learning. You may have already explored some machine learning topics, or maybe even taking some\\nmachine learning courses. If you haven't yet\\nstudied machine learning, I can recommend the machine\\nlearning specialization from DeepLearning.AI,\\nto provide you with great foundation in the field as a complement\\nto the specialization. Throughout these\\ncourses, I'll be using machine learning\\nexamples to provide context for the math\\nyou're studying and how these ideas are\\napplied in practice. In these machine\\nlearning courses, sometimes Andrew says, don't worry about the math\\nin this course. I'll sometimes do the\\nopposite and say, don't worry about the\\nmachine learning. You'll see many examples of\\nmachine learning techniques, but if you don't\\nunderstand all the details of how they work, that's okay. The goal is to build a strong\\nmathematics foundation, continue to build your interest in machine learning topics, and prepare you for further\\ncourses in machine learning. With that approach in mind, let me introduce this\\nweek's main topic, systems of linear equations, using an example from\\nmachine learning. A common machine\\nlearning approach to modeling systems is\\ncalled linear regression. Linear regression is a supervised machine\\nlearning approach, which means you've\\nalready collected data on many inputs\\nand an output, and your goal is to discover the relationships between them. For example, suppose\\nyou want to predict the electrical power output\\nfrom a wind turbine. If you had just one\\nfeature, like in this case, wind speed shown\\nhere on the x-axis, which is the horizontal\\naxis and you plot your target of power\\noutput on the y-axis, which is the vertical axis. Then the data points here are representing real\\nmeasurements of wind speed and power output. Clearly, there's a pattern, and the goal of linear\\nregression would be to find the line of best\\nfit for the data, for example, this one. With a model like this, you're\\nmaking the assumption that this relationship is\\nliterally linear. That it can be\\nmodeled by a line. In other words that if\\nyou know the wind speed, you can multiply it by a\\nconstant at a second constant, and make a reasonable\\nestimate of the power output from\\nthe wind turbine. For example, with this model, I can say that if the\\nwind is blowing at five meters per\\nsecond then I predict the power output from\\nthe wind turbine is going to be 1,500 kilowatts. Now, this model is not perfect. You can see the\\nactual data scattered about the line\\nrepresenting the model, but it's doing a reasonable job. The model here is the familiar\\nlinear equation, y=mx+b, where y is the power output, x is the wind speed. Your goal is to find\\nthe best values for m and b that fit the data. In machine learning,\\nyou'll often see the equation of this\\nmodel written as y=wx+b, because the number multiplied\\nby x is called a weight, the b is called a bias, and so luckily, doesn't\\nneed to change. Now, linear regression\\nwith just one feature like this it's easy to visualize, but in many machine\\nlearning problems, you'll be considering\\nmore features. In the case of predicting power output from\\na wind turbine, you may want to include\\nnot only wind speed, but temperature as well. In order to account\\nfor the new input, the equation of your line\\nwould need to change. Now y=w_1 times wind speed plus w_2 times\\ntemperature plus b, with a new weight added for\\nthe second input variable. If you graph this equation, it no longer would form a line. Instead it would be graphed as a plane in three\\ndimensional space. But what if you wanted to consider more features\\nlike pressure, humidity, or anything\\nelse that might affect the performance\\nof the wind turbine? The idea is exactly the same\\nas with one or two features. You simply add a new weight\\nfor each new feature, even as the equation of\\nthis model gets longer, conceptually, it works the same. By finding the right values for the weight and\\nthe biased terms, it should be possible to make accurate predictions\\nof the output or target under the assumption that this is a\\nlinear relationship. If we write that out explicitly, then you have w_1*x_1+w_2*x_2, and again, for as many features as you have up to w_n*x_n. Then you add b and set that\\nall legal to y, your target. If you imagine this equation as being one row in a dataset, then you already know\\nthe values of the x's and the y's and your goal is to find the values of the w's and b to make this\\nequation true. Of course, in reality, you have many records\\nin your dataset. You have many equations\\nyou could write down like this one for each\\nrecord in your data. If you add a superscript 1 with parenthesis up here on everything in this first\\nequation I wrote down, then I can write down the same thing for\\nthe second example in the dataset and denote that with a superscript 2 in\\nparenthesis like this, and then so on down to the superscript m in parenthesis\\nfor the last example, in a dataset\\ncontaining M records. Notice that these red\\nsuperscripts are not exponents. They simply act like subscripts except that we\\nput them on top for clarity. Ideally, you'd\\nfind the values of the weight and biased\\nterms that solve all these equations\\nat the same time or at least get as\\nclose as possible. From this very common machine\\nlearning model appears a fundamental concept of linear algebra called a\\nsystem of linear equations. This will be a fundamental topic that you'll study this week. Join me in the\\nnext video to take a closer look at systems\\nof linear equations and test your knowledge\\nof linear algebra before we dive into\\nthe weeks materials.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Now it turns out that there is a much faster way to tell if a matrix is singular\\nor non-singular, and in this video,\\nyou will learn it. It's called the determinant, and it is a quick formula that returns a zero\\nif the matrix is singular and a number different from zero if the matrix\\nis non-singular. Take a look back at the matrix\\nin the previous example, and let's focus on the\\nsingular matrix on the right. In this matrix, you can\\nmultiply the first row by two to obtain the second row, and therefore, the rows\\nare linearly dependent. In contrast, for the\\nnon-singular matrix on the left, there's no number that you\\ncan multiply the first row by to get the second row because the rows are\\nlinearly independent. Let us look at this\\nmore carefully. If a matrix has entries\\na, b, c, and d, then the matrix is\\nsingular if there exists a number k for which the first row times k is\\nequal to the second row. That means ak = c, and bk = d for the same\\nvalue of k. This is equivalent to c/a = d/b = k, and that's the same thing\\nforgetting about k, that ad = bc, or equivalently, ad-bc = 0. This value of ad-bc\\nis very important; we're going to call it the\\ndeterminant of the matrix. The determinant of\\nthe matrix is ad-bc. By construction, this\\ndeterminant is zero if the matrix is singular and\\nnonzero if it's non-singular. Now, a way I like to look\\nat the determinant is like this: ad is the product of the numbers in the main\\ndiagonal and bc is the product of the numbers\\nin the antidiagonal. Let's calculate the\\ndeterminants of these two matrices in the ongoing example and\\nsee what you get. The first matrix has\\ndeterminant 1*2-1*1 = 1, and the second one, 1*2-2*1 = 0. Notice that the first matrix, which is non-singular, has a determinant of\\none which is nonzero, and the second matrix, which is singular, has\\na determinant of zero. This is no coincidence, this is always going\\nto be the case. Non-singular matrices have\\nnonzero determinants; they don't need to be\\none, but they're nonzero, and singular matrices\\nhave zero determinant. To summarize, the determinant of a matrix with entries a, b, c, and d is ad-bc, and it is zero precisely when the matrix is singular and nonzero when the matrix\\nis non-singular. Now you're ready for\\na quiz. In Problem 1, you have two matrices and you're asked to find\\ntheir determinants. In Problem 2, determine\\nif the matrices are singular or non-singular based on the results of Problem 1. The answers are: for\\nthe first matrix, the product of the terms in the main diagonal is 5*3 = 15. The product of the terms\\nin the main antidiagonal is 1*(-1) = -1, and their difference is\\n15-(-1) = 15+1 = 16. Since the determinant\\nis different from zero, then the matrix is non-singular. For the second matrix, the\\ndeterminant is 2*3-(-1)*(-6). The three minuses means\\nthat's one minus, so determinant is is 6-6, which is precisely zero. Since the determinant is zero, then the matrix is singular. The determinant for\\n3 by 3 matrix is a bit more complicated than\\nthat for 2 by 2 matrix, but it is mostly the\\nsame. Here it is. Before calculating\\nthe determinant, recall that for a 2 by 2 matrix, you consider the main diagonals, add the products of the\\nelements of one of them, and subtract the products of the element in the other one. In a larger matrix,\\nthis is the same thing, except you'll need\\nsome more diagonals. The first diagonal is\\nthe main one over here. Now, consider the\\nnext diagonal here. This one is incomplete,\\nso you can complete it by wrapping around the matrix\\nand finishing it like this. The same thing happens\\nwith the next diagonal. For the determinant,\\nyou're simply going to add the products\\nof the elements in these diagonals and\\nsubtract the products of the elements in these diagonals that go the other way around. Let's do an example. Consider this 3 by\\n3 matrix over here. In order to calculate\\nthe determinant, consider the main\\ndiagonal with entries 1, 2, and 2. Its product is 1*2*2 = 4. Now consider the next\\none with entries 1, 1, 1, so the product of\\nthese entries is one. Now, the next one, again with product\\nof entries one. Now we're going to subtract the diagonals that go\\nin the other direction, so this one is 1*2*1 = 2, this one over here is 1*1*1 = 1, and this one over\\nhere is 1*1*2 = 2. We're adding 4+1+1-2-1-2, and the determinant becomes one. A little more calculations, but basically the same thing\\nas with 2 by 2 matrices. Now you're ready for a quiz. Find the determinant of\\nthe following matrices, and verify that those\\nwith determinant 0 are precisely the\\nsingular matrix. You can recognize that these are the exact same matrices\\nfrom the previous quiz. Here are the answers. The\\nfirst one has determinant 0, and so does the second one. The third one has determinant 6, and the fourth one\\nhas determinant 0. Therefore, the first\\ntwo are singular, the third is non-singular, and the fourth is\\nsingular because the same thing happens\\nas with 2 by 2 matrices. Singular matrices\\nhave determinant 0 and non-singular matrices have determinant nonzero. Now let's look more carefully\\nat the third matrix, the one with determinant 6. This six was obtained\\nat 6+0+0-0-0-0. Why? Well, let's do it. You first have the main\\ndiagonal which is 1*2*3 = 6. The next one is zero,\\nthe next one is zero, and the negative ones\\nare 0, 0, and 0. Notice something peculiar here. The only term that is\\nnonzero is the first one, because this matrix\\nis very special, is upper triangular, means that everything below\\nthe diagonal is a zero. Now, since everything below\\nthe diagonal is zero, all the terms in the\\ndeterminant are going to contain one of these\\nelements below the diagonal, except for the one that takes\\nthe entire main diagonal. Whenever you have a matrix where anything underneath\\nthe diagonal is zero, the determinant is going to be the product of the elements\\nin the main diagonal, so that's a quick\\nshortcut you can use. They can still be singular, for example, look at this one. In the same way, this determinant is going to\\nbe the product of 1*2*0, the elements in the diagonal, and that happens to be zero. That's the last new\\nconcept this week. After this, you'll have a few different opportunities to deepen your understanding. There are two labs that will help you get\\nfamiliar with NumPy, the Python package\\nyou'll be using on labs and assignments\\nthroughout this course. The first lab is\\nan introduction to the NumPy library overall, and in particular, NumPy arrays. If you're already familiar\\nwith this library, feel free to just\\nskim this first lab. The second lab\\nintroduces the ways NumPy can be used to\\nrepresent systems of equations and show you how to use plotting libraries to\\nvisualize those systems. Both labs are ungraded, but provide valuable background for the rest of the course. Finally, the week concludes\\nwith a graded quiz on all the topics you've studied\\nthis week. Good luck.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Now that you learned\\nhow systems of linear equations are represented\\nby lines in the plane, and that non-singularity equates to these lines intersecting\\nat a unique point, I'm going to show you\\nan even simpler way to visualize singularity\\nand non-singularity. It involves slightly\\nsimplifying the system. Recall that these are\\nthe three systems you've been studying where system\\n1 has a unique solution, so it's complete\\nand non-singular. System 2 has infinitely\\nmain solutions, so it's redundant and singular. System 3 has no solutions, so it's contradictory\\nand singular. Now, forget for a moment about the system being complete, redundant or\\ncontradictory, and let's focus on the singularity\\nand non-singularity. The reason for this is\\nthat these are the terms we're going to be studying\\nduring the whole class. Note that system 2 and\\n3, the singular ones, are very similar since both\\nconsist of parallel lines. Difference lies in system 2, these lines are the same, and in system 3,\\nthey're spaced apart. But both are fundamentally\\ndifferent from the non-singular system\\n1 since in system 1, the two lines are not parallel. Can you somehow\\ncontract systems 2 and 3 into one in order to really separate singular and\\nnon-singular systems into two buckets? The answer is yes. The way to do this is to look at the constants of the\\nsystems of equations. These constants\\nare the numbers in the equations that are not accompanying the\\nvariables a or b. For instance, in system 1, they are 10 and 12; in system 2, they are 10 and 20; and in system 3, they\\nare the 10 and the 24. Let's turn all these constants for all these three systems into zero and see what\\nhappens to the plots. When you turn these\\nconstants to zero, the plots now become these. Why? Because the new systems always have the point\\n0,0 as a solution, so they must pass by the origin. Notice that if I\\nwere to set a and b to be zero in any\\nof these equations, the equations would work, so 0,0 is a solution\\nto all of them. Now notice that system 1 is still a pair of\\nintersecting lines, so it still has a\\nunique solution. It's still complete\\nand non-singular. System 2 is still a pair\\nof identical lines, so it still has\\ninfinite solutions. It's redundant and singular. But notice what\\nhappened to system 3. It went from a pair of\\nnon-intersecting parallel lines to a pair of identical lines. Now, it has infinitely many\\nsolutions instead of none. It went from contradictory\\nto redundant. However, it stayed\\nsingular as before, and that's what matters. In conclusion, the constants in the system don't matter when it comes to determining if the system is singular\\nor non-singular. Now, for the rest of the course, singularity and\\nnon-singularity will be the important concept, and completeness,\\nredundancy, and contradiction will\\nbe used much less. For this reason, you can now start considering systems\\nof equations where the constants are always zero\\nand these are much simpler. Furthermore, the geometric\\ninterpretation of these systems will be pairs of lines that go\\nthrough the origin.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Now that you know\\nthat the constants in the system of\\nlinear equations are not important in\\norder to determine if the system is singular\\nor non-singular, you're about to jump into one of the most important and\\nfundamental objects of linear algebra, the matrix. Matrices have lots of\\nvery important properties and they arise from many\\ndifferent places in math. In this case, they will arise\\nfrom the coefficients in a systems of equations\\nin a very natural way. Let us go back to\\nthe two systems of equations you previously saw. Recall that systems\\n2 and 3 collapse into one system when you turn\\nthe constants into zero. Now, since the constants are zero, you can\\nforget about them. If you take the coefficients\\nof A and B and now put them in a two by two shaped box, which is called an\\narray or a matrix, that's the matrix\\ncorresponding to the system. The matrix corresponding to the first system is the\\none with entries 1, 1, 1 and 2. Why these numbers? Because the first\\nequation, a plus b, can be considered the equation\\n1 times a plus 1 times b. Thus, the one and one\\nequation, a plus 2b, can be considered equation\\n1 times a plus 2 times b, thus the one and the two. In this matrix, each row\\ncorresponds to each equation, and each column to the coefficient of each\\none of the variables, in this case a, for the left column and b\\nfor the second column. Similarly, the matrix\\ncorresponding to the second system is this\\none, with entries 1, 1, 2 and 2 A matrix is simply an array of numbers inside a rectangle. These ones are\\nsimple as they're in a two by two rectangle, but later in the course, you're going to see larger matrices. Matrices just like systems\\nof linear equations, can also be singular\\nor non-singular. Since the first system\\nis non-singular, because it has a\\nunique solution, then we say that its corresponding\\nmatrix is non-singular. Since the second\\nsystem is singular, as it has infinitely\\nmany solutions, we say that its corresponding\\nmatrix is singular. Of course, there\\nare quicker ways to tell if matrix is singular or non-singular without\\nhaving to find the solutions to its corresponding\\nsystems of equations. In the previous\\nvideo, you solved four systems of three\\nequations and three unknowns. You found that the first\\none has a unique solution, the second one has\\ninfinitely many solutions, the third one has no solutions, and the fourth one has\\ninfinitely many solutions. Using the same\\nterminology as before, the first one is complete, the second one is redundant, the third one is contradictory, and the fourth one is redundant. The first one is non-singular, while the other three\\nare all singular. Now, just like with systems of two equations\\nand two unknowns, an easy way to find if a system is singular or\\nnon-singular is to turn the constants into zeros\\nand to study that system. Now that you have simplified the system with zeros\\nas the constants, then let's look\\nat the solutions. The first system,\\nwe know that it has a unique solution since\\nit is non-singular. Furthermore, 0, 0, 0 is a solution\\nbecause setting a, b, and c equal to zero is\\na solution to the system. Therefore, the unique\\nsolution is the point 0, 0, 0, and the system is\\ncomplete and non-singular. Now notice that systems\\n2 and 3 are now the same system because we\\nset this constant to be zero. What are the solutions\\nto the system? Well, take a look at\\nequations 1 and 2. The only difference\\nbetween equations 1 and 2 is that equation\\n2 has an extra c, and c must be equal to zero because that c doesn't change\\nthe result which is zero. Therefore, c=0. If you plug\\nthat into the first equation, you get that a plus b\\nmust equal to 0, so a=-b. So all the points satisfy that the first\\ncoordinate is equal to negative the second coordinate and the third one\\nhas to be zero. The system is redundant\\nand singular. The fourth system, well, its solutions are all the points for which the three\\ncoordinates add to zero. That means a can be anything, b can be anything, and c needs to be minus a minus b for the\\nthree to add to zero. The system is redundant\\nand singular. Just like before,\\neach of the system's associated with a matrix that keeps track of\\nits coefficients. Again, in each row you have each equation and each column, you have the coefficients of\\neach of the variables a, b, and c. The matrices\\nare these ones. Using the same\\nnotation as before, these matrices are singular or non-singular based on if the systems are singular\\nor non-singular.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"In this video, I will\\nshow you a way to tell if a matrix is singular or\\nnon-singular directly, without having to solve the\\nsystem of linear equations. But first, let us look back\\nat systems of sentences. Recall that a system of\\nsentences is singular if the second sentence carries the same information\\nas the first one. Similarly, a system of\\nequation is singular if the second equation carries the same information\\nas the first one. This is the concept\\nof linear dependence. Let's look back at the two\\nsystems of linear equations that you've seen so far with these\\ncorresponding matrices. And let's focus on the\\nsingular system on the right. The reason this system\\nis singular is because the second equation is a\\nmultiple of the first one. In particular, it's two\\ntimes the first one. That means if we take the left\\nhand on the right hand and multiply everything by two\\nin the first equation, you get the second equation. Now if you look at the\\ncorresponding matrix, then the second row is a\\nmultiple of the first one. By that, we mean\\nthat if you take every element in\\nthe first row and multiply them all by two, you get the second row. That means that the second row can be obtained\\nfrom the first one, so the second row is\\ndependent on the first one. Notice that you\\ncan also say that the first row is dependent on the second one by\\ntaking the second row and multiplying\\neverything by a half. In any way, either\\none depends on the other one or the other\\none depends on the one, they're both dependent, so\\nthey're linearly dependent. In contrast, in the non-singular\\nsystem on the left, the second equation\\nis not a multiple of the first one, or vice versa. There's no constant\\nthat I can multiply the first equation to get the second equation,\\nor vice versa. This is why the system\\nis non-singular. Each equation tells you\\nsomething completely different. So in the corresponding matrix, the same thing happens. No row is a multiple\\nof the other one. I cannot take a\\nnumber and multiply one row entirely by the\\nnumber to get the other row. That means the rows are\\nlinearly independent. As you may imagine,\\nthe same thing happens with columns and\\nrows and one can define these concepts of\\nlinear dependency between rows and between columns and they determine the singularity and\\nnon-singularity of the matrix. For a larger matrix, the concept of linear\\nindependence and dependence is a bit more complex but\\nstill very intuitive. Follow along for the details. In order to understand\\nlinear dependence, consider the following example. Take a look at the system with three equations and\\nthree unknowns. The equations are a=1, b=2, and a+b=3. Now, notice that c doesn't appear, but\\nthat doesn't matter. Now, this system is singular\\nfor the simple reason that the third equation is the\\nsum of the first two. To see this more in detail, the first equation can be\\nwritten as a+0b+0c = 1, the second one is 0a+b+0c\\n= 2 and if we add them, we get a+b+0c = 3, which is precisely\\nthe third equation. Now, consider the matrix\\nof the coefficients of the system that the matrix obtained by forgetting\\nthe constants. The matrix has entries 1,0,0, 0,1,0, and 1,1,0. In the same way that the sum of the first two equations\\nis the third one, the sum of the first two\\nrows in the matrix is equal to the third\\none because 1+0 is 1, 0+1 is 1, and 0+0 is 0. Therefore, Row 3 depends\\non Rows 1 and 2. Row 3 in the same\\nway as Equation 3, doesn't bring anything\\nnew to the table. It's the sum of the first two. We say that the rows\\nare linearly dependent. Because from the\\nfirst and the second, you can form the third one. Therefore, this matrix and\\nthe system are singular. Now let's look at another system which you've already\\nseen that it's singular. The corresponding matrix\\nis this one over here, 1,1,1, 2,2,2, 3,3,3. Notice that there\\nare many relations between the equations\\nof the system. Here's one, the\\nfirst equation plus the second equation equals\\nthe third equation. In the matrix, Row 1 plus\\nRow 2 is equal to Row 3, so Row 3 depends on Rows 1 and 2 meaning that the rows\\nare linearly dependent. Now if you look closely, this system and this matrix have a lot of other\\ndependencies. For example, the second row is twice the first row\\nand the third row is three times the first row. This is a highly singular system with a lot of dependencies\\nbetween the rows. Now in order to\\nreally understand the concept of linear\\ndependence and independence, let's take a look at\\nthis other system, which you've also\\nseen is singular. Now, this one is a\\nlittle more subtle. What are the relation between the equations? Well, here's one. Let's take the first equation, and let's add it to\\nthe third equation. What do you get? You get 2a+2b+4c = 0. Now, that doesn't\\nlook like one of the equations perhaps,\\nor perhaps yes. If you take this equation\\nand divide it by two, you get a+b+2c = 0 which is precisely\\nthe second equation. In other words, the second\\nequation is the average of the first and third\\nequation and in the same way, the average between Row 1\\nand Row 3 of the matrix is Row 2 so Row 2 depends\\non Rows 1 and 3. Therefore, the rows\\nof this matrix, as well as the equations in the system, are\\nlinearly dependent. Now, in contrast, take a look\\nat the system of equations. You've seen before\\nthat this system has a unique solution, so\\nit's non-singular. Now, no matter how much you try, it is impossible to find any linear relations\\nbetween these equations. There's no way you can obtain the third one out\\nof the other two, or the second one of the\\nfirst and the third. There's nothing you can do here. And the same thing\\nhappens in the matrix. No row can be obtained as a linear combination\\nof the other two rows. Thus, the rows are linearly independent and this implies that the matrix is non-singular. Now as you can see,\\nit's not easy to see and to verify that\\nthere's no relation. Don't worry, soon I'm\\ngoing to show you some methods to be\\nable to verify this. But for now, you're\\nready for a quiz. Determine if the following\\nmatrices have linearly dependent or independent\\nrows. The answers are in. For the first\\nmatrix, notice that the first row times three plus the second row times\\ntwo is equal to the third row and so the\\nrows are linearly dependent. This implies that the\\nmatrix is singular. For the second matrix, notice that if you subtract the second row from\\nthe first one, you get the third row thus the rows are again\\nlinearly dependent, showing that the\\nmatrix is singular. The third matrix\\nhas no relations. Notice that no matter\\nhow much you try, there's no way to get one\\nrow out of the other two. For example, the first row\\nhas that leftmost one. There's no way you\\ncan obtain that from linear combinations of the second and third\\nrow, and so on. This matrix is non-singular because the rows are\\nlinearly independent. Finally, for the fourth matrix, notice that the first row times two is equal\\nto the third row. These rows are dependent\\nand the matrix is singular.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"The first topic you learn in linear\\nalgebra is systems of linear equations. However, before you delve into equations, it is important to understand\\nthe language of mathematics. When I think of equations,\\nI think of sentences. Sentences that are giving you\\ninformation about things in the world. And when there are many sentences or\\nsystems of sentences as I call them, these sentences combine themselves\\nto give you more information. When looked at from a proper angle, the way sentences combined to give you\\ninformation is very similar to the way equations combined to\\ngive you information. In other words, systems of sentences\\nbehave a lot like systems of equations as you'll see in the following example. So let's start with some\\nexamples of systems of sentences. Now, for the sake of this example, assume\\nthat you only have one dog and one cat and they're both of only one color. You are given some information and your goal is to try to figure out\\nthe color of each of the animals. So here's system 1 with the sentences,\\nthe dog is black and the cat is orange. Then you have system 2 with sentences,\\nthe dog is black and the dog is black. And finally you have system 3 with\\nthe sentences, the dog is black and the dog is white. The sentence is yours here are simple\\nsentences with one piece of information each. So sentence such as the dog is black and\\nthe dog is white or not allowed and they separately contain\\ntwo pieces of information. And the goal for a system is to convey as much information\\nas possible with these simple sentences. With regards to achieving that goal notice\\nthat these systems are quite different. In particular, the first system of\\nsentences contains two sentences and two pieces of information. This means the system contains as many\\npieces of information as sentences and that's called a complete system. System 2 is a bit less informative\\nas it has two sentences, but they're exactly the same. Therefore the system only carries one\\npiece of information even though it contains two sentences,\\nthe sentences he repeat themselves and therefore the system is called redundant. And finally system three strange as\\nthe sentences contradict each other. This is because the dog can't be black and\\nwhite at the same time remember that we have one dog and\\nit can only have one color. So the system is called\\nthe contradictory system. The more information a system carries,\\nthe more useful it will be for you. For this reason will introduce some\\nterminology that you'll be using throughout the whole course. When a system is redundant or\\ncontradictory, it's called a singular system. And when a system is complete,\\nit's called a non singular system. In a nutshell, a non singular\\nsystem is a system that carries as many pieces of information as sentences. So it's the most\\ninformative system can be, and a singular system is less\\ninformative than a non-singular one. Systems of sentences can carry\\nmore than two sentences. In fact they can carry as many as we want. Here are some examples of\\nsystems with three sentences. In this new example,\\nyou have three animals and are again trying to determine their color. The first system has the sentences,\\nthe dog is black, the carrot is orange and the bird is red. The second one has a system says\\nthe dog is black, the dog is black and the bird is red. The third one says the dog is black,\\nthe dog is black and the dog is black. And finally the foreman has the sentences,\\nthe dog is black, the dog is white and the bird is red. So the first system is complete as\\nit carries three different piece of information using three systems,\\nso it's complete and non-singular. The second system is redundant and\\nsingular as the 1st and 2nd sentences, say the exact same thing. Can you guess what the third\\nsystem is going to be? So if he gets redundant, Well done. The third system is redundant is all\\nthe sentences say the same thing. And finally, the fourth system is\\ncontradictory because the dog can't be black and white at the same time. Notice that the third system is more\\nredundant than the second system, which has two sentences\\nthat say the dog is black. Is there a measure of how\\nredundant a system is? And the answer is yes, and\\nit's called a rank, but you'll learn this a bit later this week. And in terms of singularity and\\nnon-singularity, the terminologies is exact as before. The first system is non-singular as it\\nis complete and the other three systems are singular as they are either\\nredundant or contradictory. Now system can be a bit more complicated\\nthan the ones who previously saw. Consider the system of sentences. Sentence one says between the dog,\\nthe cat and the bird, one of them is red. Sentence to says between the dog and\\nthe cat, one of them is orange and sentence three says the dog is black. So problem one of the squids says,\\ncan you figure out what color is the bird? And problem too says,\\nis this system singular or non-singular? Great job. And the solution for\\nquestion one is that the bird is red, why? Well, if you look at the third sentence,\\nit says that the dog is black. So now you know that the dog is black\\nin the entire system of sentences. When you look at the second sentence,\\nit says that between the dog and the cat, one of them is orange, since the dog\\nis black, then the cat must be orange. And finally the first sentence\\nsays that among the three animals, one of them is red, since the dog\\nis black and the cat is orange, then we must conclude that\\nthe bird must be red. For question two, since you figure\\nout the color of the three animals, That means the system carries three pieces\\nof information with three sentences. In other words, it has no redundancies and\\nno contradictions. It carries as many pieces of\\ninformation as sentences. Therefore it is a complete system and\\nit is non-singular.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"In the previous\\nvideo, you looked at a linear regression scenario for the problem of predicting\\nelectrical power outputs from a wind turbine. In this case, you had a dataset containing a series of features, things like wind\\nspeed, temperature, atmospheric pressure,\\nhumidity, and so on. I call this x_1, x_2 and so on, up to x_n for a dataset\\nwith n features. Then I added a superscript\\nto the dataset to denote which row of data a\\nset of features belong to. Then you have the model weights\\nmultiplying it feature, which we wrote as w_1, w_2 and up to w_n. Then we also added\\na biased term b. We said that equals\\nto the target y, which in this case is power\\noutput from the wind turbine. An important thing to\\nnote about this system is that while the xs and ys\\nare unique in each row, all of these x^1 are different\\nfrom these x^2 and so on and all the y^1\\nis different from this y^2 on down to y^m. The w values and b are all\\nthe same across all rows. Again, what you're\\nsaying here with a linear model is that there exists some set of values w_1, w_2 and so on, up to w_n, as well as some\\nvalue b that when multiplied by any of these rows of features\\nand added up like this, will be able to provide you with an estimate of your\\ntarget y for that row. In other words, with this\\nmodel you are saying, give me a set of xs and I\\ncan estimate a value for y because I have a\\nmodel that tells me what all the w's and b's are. Instead of writing this model\\nout in long form like this, I can instead say I have a\\nvector of weights called w, that is made up of w_1, w_2 and so on and\\nI multiply that by each row of features x in\\nmy matrix of features, which I now call capital X. Then I add a biased term and\\nset that all equal to y, which is a vector of\\nmy target variable. Just like that, we're back to a nice simple equation that looks just like the\\nequation of a line. I just jumped ahead a lot. Don't worry if\\nyou're not already familiar with the terms\\nvector and matrix. For now, it's fine to\\nthink about them as just lists of numbers\\nor grids of numbers. Linear algebra is all\\nabout manipulating vectors and matrices to\\ndo powerful calculations. As you just saw, this math is the backbone of many machine\\nlearning techniques. Now, if you're already a\\nlinear algebra ficionado, you may have noticed\\nthat I'm being a little imprecise with\\nthe notation here. Like I might need to indicate whether this is\\nreally a transpose of w or x to make the math\\nwork out depending on how these vectors\\nand matrix are defined. But I'm not going to\\nworry about that for now. Instead, I want to\\nemphasize that when you're using linear regression as\\na machine learning model, you are representing\\nthe system you're interested in as a system\\nof linear equations. In fact, if there were\\na set of w and b values that allowed you to perfectly predict y given a\\nset of x features, then this would be a system\\nof equations you could solve analytically without\\nany machine learning. By that I mean you could\\nsolve this by just applying basic algebra\\nwith a pencil and paper, provided that you have\\na dataset containing all the x and y\\nvalues and you have at least as many example\\nrecords as you have unknowns, the ws and the bs\\nthat is to solve for. With linear regression, you're solving the system empirically, which is to say iteratively and approximately by finding the best fit linear\\nsolution to the system. In this week of materials, I'm going to start super\\nsimple and walk you through common vector and\\nmatrix operations from the basis of\\nlinear algebra. If you study linear\\nalgebra before, it is possible you will have seen some of these\\nconcepts already. In order to give you\\na sense of what's coming in each week\\nin this course, I will pose a series of questions\\nfor you to think about. If you can answer all these\\nquestions successfully, then congratulations,\\nyou are ready to skip right to the end of\\nthe week and take the quiz. If you have some uncertainty about any of these questions, then you'll find some\\nvaluable learning in this week of materials. This week, we'll begin with\\nsystems of linear equations. How you represent those systems\\nof vectors and matrices, and how you manipulate\\nthose systems to compute the determinant or other\\ncharacteristics of the system. Here's your first\\nset of questions. This specialization\\nhas three courses, Linear Algebra, Calculus, and\\nProbability and Statistics. Suppose I recorded a score for you in each of these\\nthree courses, but I don't tell you\\nthe actual scores. Instead, I tell you the following information\\nabout your scores. Your linear algebra score\\nadded to your calculus score minus your probability and statistics score is equal to 6. Your linear algebra score\\nminus your calculus score plus double your probability\\nand statistics score is equal to 4. Four times your linear\\nalgebra score minus double your calculus\\nscore added to your probability and statistics\\nscore is equal to 10. Now, of course, no structure is ever going to give\\nyou scores like this. That would be ridiculous. But take a moment to think about these sentences and\\nsee if you could represent these statements as a system of linear equations. If we let a represent your\\nlinear algebra score, c represent your calculus score, and p represent your probability\\nand statistics scores, here is what that\\nwould look like. Now, in the context of what\\nwe looked at before with the linear regression for predicting the\\nwind power output, what would be the equivalent\\nof the weights w, the features x and the target y? In this case, your scores a, c, and p are the weights. The thing that is consistent\\nacross all the statements, the features are the numbers next to the weights.\\nIn that case. The numbers on the other side of the equal sign are\\nthe targets y. Now, could you tell me\\nwhether this system is singular or non singular\\nor in other words, do these equations\\neither contradict one another or is there\\nredundant information here? Could you solve this\\nsystem of equations? Which is to say,\\ncould you now solve for what your score in each\\nof these three courses was? Now, could you\\nrepresent the system of linear equations as a\\nmatrix and a vector? Can you calculate the\\ndeterminant of that matrix? If you could easily answer all these questions,\\nthen congratulations, you are ready to go\\nahead and take the quiz for this week and complete\\nthe labs for this week. If not, then also\\ncongratulations, you are in the right place. Join me through this\\nweek's materials as we go through systems\\nof linear equations. Step by step, we'll start super simple with\\nsystems of sentences that will then turn into equations and then from there\\nwe'll get into solving these systems\\nand the properties of such systems like singularity\\nand the determinant. As you can see,\\nlinear algebra is used all over the place\\nin machine learning, and that is a great reason\\nfor you to master it. In the remainder of the scores, you'll be learning some\\nvery important concepts and techniques in linear\\nalgebra involving matrices, linear equations,\\nlinear transformations, and much more.\\nReady? Let's begin.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Let's talk quickly about\\nprogramming experience. This course is\\ndesigned to both provide you a theoretical\\nbackground in the mathematics underlying machine learning and show you\\nhow those concepts are applied in practice. That means you'll need\\nto do some programming. This course includes graded programming\\nassignments and ungraded programming labs that focus on the application of the\\nskills and concepts you're learning. These exercises are written in Python\\nand will appear as Jupyter notebooks, an interactive\\nweb-based interface that allows you to read, run, and edit those programs. You won't need to be an expert Python\\nprogrammer to be successful in the exercises, but you should be comfortable with\\nthe concepts that would typically be taught in an introductory\\nPython programming course. This includes things like\\ndifferent data types and data structures, control flow using conditional\\nstatements, loops and functions, as well as importing and\\nusing different Python libraries. You should be comfortable reading and\\nediting Python code using these concepts, writing and debugging your own code, and occasionally referring to\\nthe documentation of new packages. If you are a confident programmer\\nin another programming language, you should be just fine learning the\\nPython you need for this course as you go. If you're new to programming, however, I would recommend you take an introductory Python course before\\nyou begin this course. In the next reading item, you will find some great resources for\\nwhere to get started learning Python.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Hi, and welcome to math for\\nmachine learning and data science. Concepts like vectors, matrices,\\ngradients, optimization, priority distributions,\\np-values occur a lot in machine learning. When you learn about these concepts,\\nyou gain a deeper understanding of how and why machine learning algorithms work. And when you understand the math\\nbehind machine learning, you'd be able to better go beyond\\njust using out of the box machine learning algorithms to build and\\ncustomize the models more. And you also be better able to judge\\nwhen to apply which technique. Through learning the math\\nof machine learning, you also become more effective at\\ndebugging machine learning algorithms. When I train a machine learning algorithm, it pretty much never works the first\\ntime or the first few times I run it. And so, the math has frequently helped me,\\nand I'm sure help you too, to make better decisions about how to\\nefficiently improve the performance of your machine learning algorithms. And who knows, maybe with the mathematical\\nfoundation that you gain from the specialization, I hope that you\\nmight someday use that skill to even invent new machine learning algorithms. I'm thrilled to introduce to you\\nthe instructor for this specialization, Luis Serrano. When DeepLearning.AI set out\\nto create this specialization, I hope to find an instructor who\\ncould bring math to life with visual examples that conveyed the intuition\\nbehind these math concepts. We spoke with many people to try to find\\nthe best instructor for these concepts, and I was thrilled when we found Luis,\\nwho turned out to be a fantastic fit. Luis is a machine learning engineer,\\nresearcher and an educator with a PhD in pure math. He was a machine learning engineer at\\nGoogle, where he worked on the YouTube recommended system, and\\nhe was also lead AI educator at Apple. Luis also holds a popular YouTube\\nchannel called Serrano Academy, where he put together math and\\nmachine learning concepts in a really illustrative,\\nreally delightful visual style. He's also the author of a best-selling\\nbook, Grokking Machine Learning. With Luis teaching this specialization,\\nI'm confident you're in good hands and you'll enjoy learning these materials\\nabout math for machine learning and data science. So I'm thrilled to have you with us, Luis. >> Well, thank you very much, Andrew. It's great to be here and it's actually\\nfunny because it was about ten years ago that I started hearing\\nthe term machine learning. And I remember, got my computer and Google searched\\nintroduction to machine learning courses. I thought it was going to be a very\\ncomplicated thing, but I looked and the first hit was your class. So I started taking your class and I was blown away by how wonderful and\\nsimple it was. So thank you for that. And for me, it's very interesting that\\nten years later, I'm on the other side of the camera talking to you about\\nmachine learning courses. So it's great to be here. Thank you. >> Thank you. I'm so glad to have you here. And in fact,\\nI did not know that story, but actually makes me reflect when you're\\nwatching this learning math from Luis. Maybe a few years from now, maybe ten\\nyears, maybe even faster if you find it in you to teach something online,\\nI think that could be very cool too. >> Yeah,\\nyou'll be on the side of the camera. We have another chair. >> So this specialization has\\nthree courses, and the first one, which is just four weeks long,\\nis on linear algebra. Do you want to say a bit more about what\\nthe first course of the three is about? Yes, so the first course is linear\\nalgebra, and in this course, we cover anything regarding vectors,\\nmatrices, linear transformations, systems of equations, determinants, etc. But we like to see it\\nin a different light. So for example, a matrix can be\\nseen as an array of numbers, but that's like seeing a book\\nas an array of letters. Books have a lot of interesting stories\\nand can take you to place in your mind. And matrix are the same,\\nthey can be seen in much deeper ways. >> In fact, one thing I've heard you\\nsay is I think that if you look at how a neural network or\\nsome learning algorithm manipulates data. A lot of a neural network is built on top\\nof matrix operations to take the data set, rotate it, turn it, warp it, in a multiple times with\\nmultiple layers of a neural network so that eventually you get out some answer\\nlike predicting, is this a cater at all? >> And that's what neural networks do,\\nright? They're just a bunch of matrices with\\nactivations in the middle that warp space. >> Yeah, so I think given that so many of\\nthe most important learning algorithms, including neural networks. But many others are built on top of matrix\\noperations, gaining that deeper intuition about how it all works, well, give you\\na better sense of how neural networks and many other learning algorithms\\nactually do the magic that they do. >> Exactly. >> And\\nthen the second course is on calculus. Do you want to say a bit more about that? >> Yes, so calculus is very important in\\nmachine learning for many things, but one that's fundamental is maximizing and\\nminimizing functions. >> Yeah, in fact, vast majority of\\nlearning algorithms are created by creating some cost function and\\nthen minimizing. So knowing how to do that is fundamental\\nto a lot of learning algorithms. And so, I've actually found that when\\nI'm using a minimization algorithm, be a gradient descent or some more\\nadvanced algorithm like atom, I think that if you can picture in your head, what's\\nthe derivative calculation doing then? If it isn't working as well as you want,\\nI find that I'm better able to tune the algorithm to make it do a better job\\nsolving this critical minimization task. >> Yeah, absolutely. Knowing a lot about the calculus and\\nthe derivatives can really help demystify these optimizers that at\\nfirst may look obscure. >> And then finally, the third course\\nof the specialization is probably in statistics and even things like\\nhypothesis testing and p-value. So tell us more about the third course. >> Yeah, the third course is very\\ninteresting because a lot of machine learning happens to be probabilities,\\nright? Many times a model outputs a probability,\\nand probability can be used to train models. For example, maximum likelihood\\nestimation is very important. In maximum likelihood estimation, what you\\nwant to do is, let's say you have some evidence and you want to find the scenario\\nthat most likely generated this evidence. That's machine learning. Your evidence is your data, and you want to find the model that\\nmost likely generated this data. So you want to maximize the probability\\nthat the model generated that data. >> So sometimes I've said,\\nwhen it comes to math, I sometimes said don't worry about it. And I actually stand by that. When you're learning machine learning for\\nthe first time to get it to work, sometimes you don't need to worry\\nabout the intricacies of exactly how the math works. But as Luis was saying,\\nto get to that next level of expertise, when you can then start to gain that\\ndeeper understanding of that math as well. You can then learn a better, deeper mastery of the algorithms\\nthat you're using and building. And then one exciting element of this\\nspecialization is learners don't just learn the math, they also get to\\nsee it in Python code and see it run. Do you want to say more about that? >> Yes, that's absolutely right. You will be able to put these\\nalgorithms in practice. And for that,\\nwe have a bunch of Python labs. So we assume some basic knowledge\\nin Python to be able to run them, but we're also going to have\\nthe resources for you to get up to speed. >> Awesome.\\nSo, this specialization will go from high school, not very advanced\\nlevel of math, all the way up to the core concepts of math for\\nmachine learning and data science. And so with that, I'm excited to have\\nyou jump into this specialization. So please go on to the next video\\nto dive into linear algebra.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"In this course,\\nyou will encounter many useful applications\\nof vectors, matrices, and mathematical\\nproperties of each, useful and extracting\\ninformation about systems of equations. To get the most out\\nof this course, we highly recommend that you are familiar with solving\\na simple equation with one unknown variable and\\nthat you know how to construct simple plots in\\nthe coordinate system. An example of an equation\\nin one variable is say, 2x plus 7 equals 17. From there, you can work\\nout that 2x equals 10, and therefore that x equals 5. An example of a plot is\\nthe function y equals x, which if you plot in the plane, it's a diagonal starting at the origin and going\\nup 45 degrees. If you need to review some of these concepts, don't\\nworry about it. We have some recommendations\\nin the resources section. For example, you can check\\nout Khan Academy's course on introduction to algebra and then come back join\\nus in this course. If you already feel\\ncomfortable with basic algebra, then\\nlet us get started.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"You are now starting the first course in\\nthe series of math for machine learning. Welcome, we're very excited\\nto have you on this journey. In this course, you will learn to\\ntranslate real world scenarios into practical applications\\nusing linear algebra. In the first week you will construct\\nsystems of equations from real life examples and identify the matrix representation\\ncorresponding to these systems. From there, you will determine some\\nimportant concepts such as singularity and linear independence. You will also learn to\\ncalculate the determinant and you'll learn some useful row\\noperations for simplifying matrices. By the end of week two you will be\\ncomfortable solving two by two and three by three systems of equations, and you'll be familiar with concepts\\nsuch as the rank of a matrix. In week three, we will swim into the depth\\nof the simplest representation of a greater values, the vector, and how matrices can transform\\none vector into another. And in week four, you will learn some more\\nadvanced concepts such as eigen values and eigen vectors. You might encounter working with vectors\\nin your life without even realizing it. So follow me along in the next\\nvideo to get started.\",\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " ('Well done on completing\\nWeek 1 of linear algebra. This is a great achievement and you should be\\nproud of yourself. You started this week with a machine-learning application\\nof linear algebra. Then you learned systems of equations and translating\\nthem into matrices. You can now extract\\ninformation from matrices such as their singularity\\nand their determinant. You are now ready\\nto dive deeper and learn to solve any system\\nof linear equations.',\n",
       "  'mathematics for ml::machine learning linear algebra::01 systems of linear equations'),\n",
       " (\"Just like numbers\\ncan be added and subtracted to obtain\\nother numbers, vectors can also be added and subtracted to obtain\\nother vectors. This can be done in a very\\nnatural way. Here it is. If you'd like to\\nadd two vectors, for example the vectors\\nu with coordinates 4, 1 and v with coordinates 1, 3, all you've got to do is add the coordinates to\\nget the vector 5, 4. Now this is a nice\\ngeometric interpretation. The sum vector is precisely the diagonal in a\\nparallelogram formed by the vectors u and v. Something similar happens\\nwith the difference, except with the\\nother diagonal of the parallelogram The difference\\nbetween the vectors 1, 3 and 4, 1 is also taken\\ncomponent-wise to be 3, -2. That is, this vector over here, which doesn't look\\nlike anything special except if you were\\nto translate it, it matches precisely\\nwith the vector obtained by joining\\nthe points 1, 3 and 4, 1. Here is the general\\ncase definition for the sum and\\ndifference of vectors. Suppose you have two\\nvectors, x and y. The sum can be expressed as the sum component by component. This means that the\\nfirst component of the new vector\\nwill be the sum of the first component of x and the first component\\nof y and so on. Notice that with\\nthis definition, x and y must have the same\\nnumber of components. Similarly, the difference\\nbetween two vectors is defined as the component\\ny's difference. The difference between\\ntwo vectors is helpful to tell how far apart two\\nvectors are from each other. For example, how different\\nis the vector 1, 5 from the vector 6, 2? One way to tell is by the L1 distance or the L1\\nnorm of their difference. This is the sum of\\nthe absolute values of the components,\\nwhich is eight. Another way to tell is by the L2 distance or the L2\\nnorm of their difference, which in this case is 5.83. In machine learning, it's very useful to know distances between vectors because many\\ntimes you want to calculate different similarities\\nbetween data points, and these measures\\nare very useful. Another very useful and\\nsimple operation you can do in vectors is multiply\\nthem by a scalar. For example, if the vector is 1, 2 and you'd like to multiply it by the scalar Lambda = 3, then the result is the\\nelement y's product. That means you multiply each of the elements of the vector by 3 to obtain the vector 3, 6. Graphically means you\\nstretch the vector 1, 2 by a factor of 3. What if the scalar is\\nnegative? No problem. What happens here is\\nthat the vector gets again stretched by the factor, but also reflected\\nabout the origin. For example, if the\\nvector is again 1, 2 and the scalar is -2, then the product of\\nthe scalar times the vector is -2, -4,\\nwhich also corresponds to the entry-wise\\nproduct as expected. Let's formalize the definition. Consider once again an\\nn-dimensional vector x and let Lambda be a scalar. Then Lambda x is simply\\nwhat you obtain when you multiply each component\\nof the vector x by the scalar\\nLambda like this.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"Welcome to Week 3.\\nThe main topics for this week are\\nvectors and matrices, and some of their\\nproperties such as the size and direction\\nof a vector. You will also learn many operations you can\\napply in them. See, in a way, vectors and matrices are a lot like numbers. Numbers can be added,\\nmultiplied, divided, and many of these\\noperations can be propagated into\\nvectors and matrices. For example, one can think\\nof the sum of two vectors, the product of two vectors, or the product of two matrices, or the product of a\\nvector and a matrix. Furthermore, just\\nlike you can find the multiplicative inverse\\nof a number, for example, the inverse of 2 is 1/2, you can also find the\\nmultiplicative inverse of a matrix. That is under some\\nconditions that you'll see, but actually you've seen before. You'll see throughout the\\nweek that matrices and vectors are essential\\nin any kind of dataset. Another very important concept\\nyou learned this week is the concept of a\\nlinear transformation. Linear transformations\\nare very special way to visualize matrices\\ngraphically, and one that can make many\\nconcepts much more clear. But before diving\\ninto all of that, let's take a look at how these concepts appear\\nin machine learning. At the start of\\nWeek 1 we looked at linear aggression as an\\nexample of a machine learning modeling situation\\nwhere you treat your dataset as a system\\nof linear equations. As a reminder, the way that\\nworked was that you have a dataset made up of some set of features you called x_1, x_2, all the way up to x_n. Then you had multiple\\nexamples in your dataset. You had multiple rows of x's in your feature\\nmatrix like this, and you had a superscript along each row denoting the\\nnumber of each example. Then you had a vector\\nof targets or y values. The idea with linear aggression is that you come up\\nwith a set of weights, w_1, w_2, and so on up to w_n, one for each x feature, as well as a bias value\\nthat allows you to characterize this dataset as a system of linear equations. Then to simplify\\nthis expression, I can just write W to\\ndenote the vector of w values and X to denote\\nthis matrix of x values, and add b to that and\\nset all that equal to y hat to represent the\\nvector of target values. Just like that, you're back to the simple equation of a line. Of course, if you're a linear\\nalgebra expert already, you may notice that I might be missing a transpose\\non the W or X here, depending on how I've set up\\nthese vectors and matrices. But I'm not going to worry\\nabout that for the moment. Of course, real-world\\ndatasets are not typically the systems of equations that\\nyou can solve analytically, like you were doing in\\nlast week's materials. But your assumption\\nthat this dataset could be approximated as a system of linear equations turns out to be a\\nreasonable one. Then machine learning\\nwill allow you to solve this system in an\\niterative fashion and make predictions of your target y\\nfor any new set of x values. It turns out, however, that\\nyou can only get so far by approximating real-world\\ndatasets with linear models, because in many cases, the relationship\\nbetween features and targets is nonlinear. One of the most powerful\\nmachine learning models for representing nonlinear systems\\nis the neural network. One of the most\\namazing things about neural networks is\\nthat under the hood, they're really just a large\\ncollection of linear models. You might have seen neural\\nnetworks represented like this with these\\nvertically-oriented layers, so-called artificial neurons, that are all connected\\nwith lines like this. The way you can think of what this diagram represents\\nis that you have this layer on the\\nleft representing your inputs to the network,\\nwhich are your features. X_1, x_2, and so on up to x_n,\\nlike you had before. I'm going to write this\\nvertically this time inside these nodes in the\\ninput layer of the network. Then what these lines are indicating is that\\nyou're going to send all those x values to each one of the individual neurons\\nin the next layer. For example, all these x's\\ngo to the first neuron, and that neuron has a vector of w values and a b value\\nassociated with it. You can write what's\\ngoing on up here in this first neuron as your w vector times your matrix of x features plus the b value. You have a linear model\\njust like before. In fact, you have a whole\\nsystem of linear equations. But now it's all contained\\nin this one little neuron. Just like before, I can\\nrepresent the system of linear equations inside\\nthis neuron as W*X+b. Now, something you don't\\nneed to worry about, but that I'll mention\\njust for completeness, is that what's really\\ngoing to happen up here in this neuron\\nis that you're shoving this whole WX+b expression into something called\\nan activation function. That activation\\nfunction generates some output that here\\nI'll just call a, which is another vector. Then each of these neurons in the layer does that\\nexact same thing, but with a different set of weights and biases in each case, and generate their\\nown output, a vector. Just to be clear that each\\nof these neurons is unique, I am going to add subscripts to all the w's and b's and a's, where the subscript 1 means you're in this first\\nneuron at the top, and similarly, on the way down. Of course, the matrix\\nof x values you're passing through each neuron\\nis the same in each case. That doesn't need any subscript. As if we didn't have\\nenough notation already, I'm going to add a\\nlittle superscript 1 in square brackets up here to denote that\\nthese operations are happening in the first\\nlayer of the network. That's because what happens\\nnext is that you pass these a values to the next layer and the process\\nhappens all over again. Now in the second\\nlayer, the inputs are your a values\\nfrom the first layer. The linear model in the\\nsecond layer here looks the same except that instead\\nof x's, you have a's. Just to be clear, these\\na superscript ones are now matrices containing all the a vectors from\\nthe previous layer, this a_1, a_2, and so on to a_n. You're multiplying those\\nby a whole new set of weights and adding a different\\nbias value in each case. Everything else,\\nthe w's and b's and the new a values you generate get a superscript\\nsquare brackets 2 to indicate their part\\nof the second layer. Then this all repeats as you propagate forward through each of the layers of the network, multiplying the outputs of the previous layer\\nby a set of weights, adding a bias term, and applying an\\nactivation function until you get to\\nyour final output. Now this may all sound\\npretty complicated, but I wanted to walk through\\nthis in a bit of detail, so you can see how systems of linear equations are a core\\ncomponent of neural networks. If you want to understand\\nall the stuff in detail, I will again recommend the machine learning specialization\\nby deep learning AI. But here in this course, the\\npoint is not to worry about all the intricate details of how a neural network\\nfunctions. Quite the opposite. The point here is\\nthat there's nothing particularly fancy going on\\ninside this neural network. It's really just a\\nlarge collection of linear models that taken together can work to model\\nhighly nonlinear systems. Instead of writing down a\\nzillion little linear equations to represent the neural network, you will instead represent\\nthe inputs and outputs of each layer as vectors, matrices, and tensors, which\\nspoiler alert, are just like higher\\ndimensional matrices. You'll apply linear algebra\\nto operate on those vectors, matrices, and tensors, and compute your machine\\nlearning results. This week, similar to\\nthe last two weeks, if you have studied\\nlinear algebra before, you might be familiar\\nwith some of the concepts we're going\\nto cover this week. Take a look at the\\nknowledge check in the next reading item, and if you can easily\\nanswer all the questions, then congratulations,\\nyou're ready to move on to the assessments\\nat the end of the week. If you're not sure how to answer any or all\\nof those questions, then also congratulations,\\nyou're in the right place. Join me in the next\\nvideo to get started.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In this video you will learn a very nice and\\ncompact way to express systems of linear\\nequations using matrices and vectors\\ncalled the dot product. The dot product is a very important operation\\nin linear algebra. In fact, you've already\\nlearned the dot product, but here, you will learn it formally. This is how it works. Imagine that you have\\nthe following problem. You buy some fruit,\\nlet's say two apples, four bananas, and one cherry. Each apple costs $3, each banana costs $5, and each cherry costs $2. The question is, how much\\ndoes everything cost? That's an easy problem, but the point is the\\nway to express it. The way to express the number of fruits\\nis using a vector, which is simply a\\ncolumn of numbers, 2, 4, and 1. That's the amount of\\nfruits we bought of each. The prices can also\\nbe expressed as a vector with\\nentries 3, 5, and 2. That's the price of\\neach of the fruits. In order to find the\\nprice of all the fruits, you can find the\\nprice of each fruit individually, namely, two apples multiplied by three, which is the price of each\\napple to get 2*3 = $6. That's the amount of money\\nyou spent on apples. You can do the same thing for\\nbananas and get 4*5 = 20, and on the same\\nthing for cherries, so 1*2 = 2. Six is the amount of money\\nyou spent on apples, 20 on bananas, and\\ntwo on cherries. The total price of fruit\\nis 6+20+2, which is 28. Now, what you just did\\nwas the dot product, because you can\\nabbreviate it like this. That means 2*3+4*5+1*2,\\nand that's 28. We can actually\\nexpress it like this. It's more common to have\\nthe first vector as a row and the second\\nvector as a column, and then forget about the\\nfruits and get the dot product. Dot product of the vectors 2, 4, 1, and 3, 5, 2 is the sum of each\\ncorresponding pair of entries. That's it. The dot product is very often used in\\nlinear algebra, and next you'll see\\nsome of its uses. There's a nice connection\\nbetween dot product and norm. Let's go back to the vector\\nwith coordinates, 4, 3 whose norm was 5. Notice one thing, and it's that 4^2+3^2 is actually\\nthe dot product of the vector and itself. It's 4*4+3*3, which is 25.\\nThis is always the case. The L2 norm is always the square root of\\nthe dot product between the vector and itself. Sometimes you'll see it\\nwritten in this notation with an angled bracket on the left and an angled bracket\\non the right. Let's go back to that operation\\nI showed you before where the column vector was\\ntransformed into a row vector. This is called a transpose, and it essentially transforms\\ncolumns into rows. You denote the operation\\nby a superscript T. The result will convert this column\\nvector into a row vector. You can also apply the\\ntranspose to a row vector, and it works exactly\\nin the same way, except now the row vector has been converted into\\na column vector. You can also transpose a matrix. Let me show you how by\\nadding a second column. This is now a three\\nby two matrix. To transpose the matrix, simply transpose\\neach of the columns. First, you will get\\nthe first column of the original matrix and transpose it to\\nget the first row of the transpose matrix. Then you do the same thing\\nwith the second column. Again, you are turning\\ncolumns into rows. Notice that dimensions\\nof the matrix swap. If you start with a\\nthree by two matrix, its transpose will be\\na two by three matrix. Let's conclude by looking at the formal definition\\nof the dot product. Start with two vectors x and y with the same\\nnumber of components. Then the dot product is\\njust x_1*y_1+x_2*y_2, and you add these products\\nover all n components. Notice that angled brackets are another notation for\\nthe dot product. In some context, it is expected\\nthat the dot product has a row vector on the left and a column vector on the right. In those cases, you\\nmight see the transpose used on one of the vectors to\\nget them arranged properly.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"As you can imagine, the angle between\\ntwo vectors is very important. There are some nice relations between\\nthe angle and the dot product, which I'll show you in this video. To begin,\\ntake a look at these two perpendicular, also called orthogonal vectors,\\nwith entries -1, 3 and 6, 2. Now take the dot product and notice that the dot product is\\n6 x -1 + 2 x 3, which is 0. This always happens, in fact,\\ntwo vectors are orthogonal, if only the dot product is 0. Now, for a recap of what you've seen. The dot product between a vector and\\nitself is precisely the norm squared, or the length of the vector squared. The dot product between two perpendicular\\nor orthogonal vectors is always 0. What about the dot product between\\ntwo random vectors u and v, is there a nice way to write this? And it turns out that there is. So let's look at it from\\na slightly different way. You saw, the dot product on the vector\\nin itself is the norm squared over the product of the norm and itself. The dot product between a vector and\\na longer vector in the same direction is very similar, it's simply the product of\\nthe two norms, norm of u times norm of v. Now, what vectors have\\nan angle in between? It's almost the same, except you have to\\nperpendicular project one vector onto the other one, this is precisely the shape\\nthat one vector leaves on the other one. So if this vector is u prime,\\nthen the dot product between u and v is the product of the magnitude\\nof u prime and the magnitude of v. The interesting thing is that it doesn't\\nmatter if you project u onto v or v onto u,\\nyou still get the same dot product. An easier way to see this is this, if the angle between the vectors is theta,\\nthen the dot product of u and v is the magnitude of u times the magnitude\\nof v times the cosine of the angle. Using what you know about the dot product, you can now tell if the dot product\\nbetween two vectors is positive, negative or 0, for example,\\ntake a look at a vector 6, 2. A vector perpendicular to it, such as -1, 3, has a dot product of\\nzero with the vector 6, 2. A vector to the right of it,\\nsuch as 2, 4, for example, has a dot product of 20,\\nwhich is positive. A vector to the left of it, such as -4, 1, has a dot product of -22,\\nwhich is negative. Now, why is the dot product with 2,\\n4 positive? Well, the reason is because\\nthe projection of this vector on two 6, 2 has positive length, and why is\\nthe dot product with -4, 1 negative? And the reason is because the projection\\nonto the vector 6, 2 is negative, because you have to go in the opposite\\ndirection of the vector 6, 2. So therefore, the sine of the dot\\nproduct of a vector corresponds to being on one side or the other\\none of that perpendicular vector. In other words, this happens in general. So the vector is u, then the vectors that\\nhave dot product of 0 with u are all the perpendicular vectors to u. The vectors that have a positive dot\\nproduct with u are all the vectors in this region, and the vectors that have\\na negative dot product with u are the vectors in this region, over here.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In the previous\\nweeks, you've learned matrices as a arrays of numbers. A vector is a simpler\\narray of number, one that only has one column. It turns out that vectors can be seen as\\narrows in the plane, or in a higher\\ndimensional space. Two very important components of vectors are their magnitude, namely, their size,\\nand their direction. This is what you learn next. A vector is simply\\na tuple of numbers. It could be two numbers,\\nthree numbers, anything. The number of coordinates\\nin the vector is the dimension of the\\nspace in which it leaves. For example, the vector\\nwith coordinates (4, 3) lives in the plane and\\nis precisely the arrow that points at the point with\\nhorizontal coordinate 4, and vertical coordinate 3. You can have vector in\\nspace of higher dimensions. For example, in 3D, the vector which coordinates (4, 3, 1) lives in the space, and is the arrow pointing at the point which coordinates (4, 3, 1) with respect to\\nthe axes x, y, and z. A vector has two very\\nimportant components, the magnitude or size, and the direction, which\\nI will show you next. The magnitude or size of the vector can be\\ndefined in several ways, but the great news\\nis that all of them emulate the distances\\nyou use in real life. The first way to\\nmeasure distance is imagine that you live in a city with lots of buildings and blocks formed by streets, that are either\\nhorizontal or vertical. You need to go from\\nhome to the store, but you can only\\ngo on the streets. Therefore, the distance\\nbetween points, say if there are four blocks horizontally and three blocks vertically between\\nyour house and the store, is the sum 4+3. This is called the\\ntaxicab distance. No matter what route you take, you will always end up at a\\ndistance of seven blocks. The other way you can go\\nto the story is to forget your car and actually\\nhop into a helicopter. The helicopter doesn't need to follow the streets or corners, and it can simply fly directly. The best you can do with a\\nhelicopter is the square root of 3^2+4^2, which\\nis precisely five. This is because of the\\nPythagorean theorem that states that the length\\nof the hypotenuse in a right triangle\\nis the square root of the sum of the\\nsquares of both sides. Now why this distances? Because both give\\nus ways to find the size of the vector\\nwith coordinates (a, b). The first one, the L1-norm, is defined as the\\ntaxicab distance between the origin\\nand the point (a, b), which is precisely the sum of the absolute values of a\\nand b. Why absolute values? Because a and b can be\\npositive or negative, but the distance to walk\\nis always positive. The second measure of the\\nsize of vector is the L2-norm defined by the\\nhelicopter distance between the origin\\nand the point (a, b). As you've seen, this\\nis the square root of the sums of the squares\\nof the coordinates. By default, when you don't\\nspecify which norm to use, we are using the L2-norm. The reason is that it's\\nthe more natural one, because precisely, it is\\nthe length of the arrow. The direction of a vector can also be deduced from\\nits coordinates. For example, for this vector\\nwith coordinates (4, 3), if the angle with the\\nhorizontal axis is theta, then the tangent of\\ntheta is precisely 3/4. This means that theta is the arctan or the\\ninverse tangent of 3/4. That is 0.64 in radians. Or if you prefer, 36.87 degrees. Vectors can have different norms while pointing in\\nthe same direction. For example, notice\\nthat the vector (2, 1.5) points in the same\\ndirection as the vector (4, 3) while having a smaller norm. There are many notations\\nfor referring to vectors, some of which you'll see in\\nthis course and others that you may see in textbooks or\\nresources you find online. Let's show many ways of representing a\\nvector simply called x. Vectors can be written\\nhorizontally as row vectors, or vertically as column vectors. The components of a vector\\nare numbered with subscripts. The second component\\nin x is called x_2. In other resources, you may\\nsee vector names written with a little arrow or in bold font to call out that\\nthey're a vector. But in this course, I'll\\njust stick the plain old x. Vectors can also be written with square brackets instead\\nof parentheses. Square brackets can sometimes\\nbe a helpful reminder that a vector is part of a matrix or is simply\\na small skinny matrix. But there is absolutely\\nno conceptual difference between these notations. You'll see both\\nparentheses and brackets used in this course\\ndepending on context. Let's use that vector notation to generalize the definition of the L1 and L2 norms to any vector with\\nn total components. In general, the L1-norm\\nwill be the sum of the absolute values of\\nall the components. The L2-norm will be\\nthe square root of the sum of all the square\\ncomponents of the vector.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In this video, you\\nwill learn how to multiply a matrix and a vector. This is also something\\nyou've already learned, as it is precisely how a system of linear\\nequations looks. Here's how it works. Recall\\nfrom previous videos that the dot product\\nbetween two vectors is the sum of the products of\\nthe corresponding entries. An equation with a known\\nvariable, such as, for example, 2a+4b+c=28 can be written as a product of the row vector\\nwith entries 2, 4, 1. Column vector with the\\nunknown entries, a, b, and c. Vectors can\\nhave variables on. This represents\\nan equation where the unknowns in\\nthe column vector represent the price of\\neach of the fruits, and the number in the row vector represents the amount of\\neach fruit that you bought. The $28 represents the dot\\nproduct of the two vectors. Now, imagine that\\nyou have a system of three equations\\nwith three unknowns. Each one of these equations can be expressed\\nas a dot product. For example, the first one, a+b+c=10 can be expressed as the dot product of\\nthe vector 1, 1, 1, which is the coefficients\\nof a, b, and c, and the vector with\\nthe variables a, b, and c, and\\nthat's equal to 10. The equation a plus\\ntwo, b plus c equals 15 can be seen as\\na product of 1, 2, 1 and a, b, c, and that's 15. Finally, the equation a+b=c can be seen as a product\\nof vector 1, 1, 2 and the vector a, b, c, and that's equal to 12. Now, it seems clumsy to represent this with three\\ndifferent dot products. Is there a nicer way?\\nThe answer is yes. First thing we do is\\nput them like this. On the left, we\\nhave our system of three equations and\\nthree unknown variables, and on the right, we\\nhave three dot products. Now, notice that the\\ncolumn vector is the same, so we can simply unite the three vectors\\nand get a matrix. Now we get the product of\\na matrix and a vector. The product of a matrix\\nand a vector is nothing more than three dot\\nproducts stacked together. If you have more\\nequations in your system, then you have a bigger matrix. But from now on, we're going\\nto be expressing systems of equations as a product\\nof a matrix times a vector. Note that this is a 3*3 matrix with three rows\\nand three columns, and this vector has\\na length of three. The number of columns\\nin the matrix must equal the length\\nof the vector. If they don't match, you are\\nessentially trying to take the dot product of vectors with different lengths,\\nwhich is undefined. The interesting thing, however, is that the matrix could\\nbe rectangular as long as these two circle\\ndimensions match. For example, you could add a fourth linear equation\\ncreating a 4*3 matrix, and still multiply it by the vector containing the\\nthree components, a, b, and c. Notice that the results of vector\\nwith length four, which will match the number\\nof rows in the matrix. That brings us to the end of the first lesson\\nof this week. After this, you'll have an ungraded practice\\nquiz to help prepare you for the graded\\nquiz at the end of the week. There is also an ungraded lab in which you will learn how to perform the dot product in Python using the Num Pi library. You'll need the skills\\nyou learn there on the programming\\nassignment you'll complete at the\\nend of this week. Enjoy those activities, and I'll see you when you\\ncome back for lesson 2.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"Congratulations on finishing\\nthe third week of linear algebra. This week, you worked heavily\\nwith matrices and vectors, and learn how to perform operations on them. These operations led to a very\\nuseful way to visualize matrices, called the linear transformation. We are excited to see you apply what\\nyou've learned in these three weeks in the rest of your career. With prepared interactive exercises for\\nyou throughout this course, where you'll get further experience\\nworking with these concepts.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In the last video, you learn how to find matrice inverses. Also that some matrices\\nhave no inverse. What is the rule then for a matrice to have an\\ninverse or not? You will be happy to find that the rule is something\\nyou've already learned. See, matrices behave\\na lot like numbers. You just realize that\\nsome matrice have multiplicative inverses\\nin the same way that some numbers have\\nmultiplicative inverses. For example, the inverse\\nof five is 1/5 or 0.2. The inverse of eight\\nis 1/8 or 0.125. However, not all numbers have\\nmultiplicative inverses. For example, what is\\nthe inverse of zero? The inverse of zero is not\\ndefined as there is no number which when multiplied\\nby zero, gives us one. Now, from the last video\\non the previous quizzes, you know that this is also\\nthe case with matrice. Some matrices have inverses\\nlike this two over here, and some don't like\\nthis one over here. What is special about\\nthese three matrices? Can you look at\\nsomething special? Well, as you've seen before, the first two are non-singular and the third one is singular. Could it be that this\\nis the requirement? You'll be happy to see that this is exactly the requirement. Non-singular matrices\\nalways have an inverse, and that's why we also call\\nthem invertible matrice. Singular matrices\\nnever have an inverse, which is why we call\\nthem non-invertible. Something interesting happens. When you look at\\nthe determinant, the determinant is non-zero for invertible matrice\\nin the same way that non-zero numbers\\nhave an inverse, and the determinant is zero\\nfor non-invertible matrices in the same way that zero doesn't have a\\nmultiplicative inverse. I like to remember it that way. Non-zero determinants mean that the matrice has an inverse, and a zero determinant that the matrice does not\\nhave an inverse.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"Now onto a machine\\nlearning application. Recall from Week 1\\nthat neural networks, one of the most successful and powerful machine\\nlearning models out there with\\nnumerous applications, is based largely on matrices\\nand matrix products. Let me show you a\\nconcrete example of a simple neural network which actually works\\nwith the dot product. First, let's start\\nwith the small quiz. Imagine that you have a spam dataset and in\\nthis spam dataset, you've pinpoint\\ntwo words that are quite deterministic for spam, which are the words\\nlottery and win. These seem to appear\\nmore on spam emails, but of course the\\nappearance doesn't guarantee that the\\nemail is spam. You've counted the number of appearances in the emails that are spam or not spam\\nand got this table. Now the goal is the following. You want to build a spam filter, the best possible spam filter. That is called a classifier, which is some little machine that will try to\\nguess if an email is spam or not based on the\\ncontents on this table. This particular classifier is going to work in\\nthe following way. You assign a score to the word lottery and a\\nscore to the word win, then you calculate the\\nscore of a sentence by adding the scores of the\\nwords with repetition. For example, if the score for\\nthe words lottery and win are three and two then\\nthe sentence win, win the lottery gets a total\\nscore of seven points, two for each appearance of win, three for the\\nappearance of lottery, and one for the appearance\\nof the word done. Now the rule to guess if an email is spam or\\nnot is the following. If the number of points\\nof the sentence is bigger than some amount\\ncalled the threshold, then the email is\\nclassified as spam. Otherwise, it's not. Watch out. This doesn't mean\\nthe email spam, simply means that the\\nclassifier thinks it's spam and sends it\\nto the spam box. Now the goal of the quiz is to find the best\\npossible classifier. That means one that fits the results of the table\\nas well as possible. In other words, you want to find the best score for\\nthe word lottery and for the word win and for the threshold in such a\\nway that the results of the classifier are as close as possible to the spam\\ncolumn of the table. I'll give you a\\nhint, it's actually possible to find\\nthree numbers so that the classifier makes\\nthe correct guess for every single email in the\\ntable. Give it a try. Here's the answer. Actually,\\nmany answers work, but out of the\\noptions of the quiz, the only one that\\nworked was this one. The words lottery and\\nwin both have a score of one point and the\\nthreshold is 1.5 points. Notice that when you calculate the scores of the sentences, they end up being the sums of the number of times\\neach word appears. When you check the\\nnumber is larger than equal to the\\nthreshold of 1.5, the column of the answers is exactly the same column that records if the\\nemail spam or not. This is the perfect spam filter for that particular dataset. That means the classifier did a pretty good job in the\\ndataset that it was given. Now this is called natural\\nlanguage processing because the input was language, it was words, and it used these words to\\nmake a prediction. This classifier can also be\\nseen graphically as follows. Let's plot the dataset\\nin a plane in which the horizontal\\naxis is the number of times the word\\nlottery appears, and the vertical\\naxis is the number of time the word win appears. The dataset looks like these. Notice there's quite\\npeculiar as a line can separate spam from non spam. Also this line happens to have the equation given by the\\nscores and the threshold, which is one times win\\nplus one times lottery, is equal to 1.5 The line also\\ngives rise to two regions, the positive and the\\nnegative regions. The positive region\\nis the one for which the score in the sentence is\\nlarger than the threshold, and the negative region\\nis that one for which the score of the sentence is\\nsmaller than the threshold. This is precisely a\\nlinear classifier, and it's actually the\\nsimplest neural network. It's a neural network\\nwith one layer. Also note that the\\nmodel does make sense. The more appearances of\\nthe words lottery and win, the higher the score\\nin the sentence and the more likely\\nthe email is spam. One layer neural\\nnetwork can be seen as a matrix product followed by a threshold check. Here it is. On the left, there's the dataset and in the\\nmiddle there is the model, those two numbers, 1,1 are the scores of the\\nword lottery and win. Now let's look at one of the\\nrows, say the second one. It's a sentence with\\ntwo appearances of the word lottery and one\\nappearance of the word win. Now in order to find the\\nscore of that sentence, one simply takes\\nthe dot product of the sentence row and the\\nmodel column to get 2+1 = 3. Now after the dot product, you can apply the check\\nwith the threshold. This one, it turns\\na yes if the result is more than 1.5\\nand no if it isn't. In this case, three\\nis bigger than 1.5, so the prediction is spam. That means the classifier\\nthinks the email is spam and that is correct. Let's do another one,\\nsay the fifth row. This one only has\\none appearance of the word win and no appearance\\nof the word lottery. The dot product is\\nequal to one which after the check results\\nin a prediction of no because the\\nclassifier doesn't think the email is spam and\\nit's correct again. Now notice that if you\\nhad a lot more words, you would simply have a much\\nwider matrix on the left and a much longer model vector on the right, but\\nthis still works. Now an easier way to do all these predictions\\nat the same time is to actually take the product of the\\nmatrix and the vector. If we take the product of\\nthe matrix and this vector, we get the vector of scores and all we\\nhave to do is apply the check to all the elements in this vector to\\nget our predictions. As I mentioned before,\\nif you had many words, you would just have a\\nmuch wider matrix and a much longer model vector but this would work\\nexactly the same. But for now, let's go back\\nto the two column case. Here's another way to\\nlook at this classifier. The equation to check for\\nspam is that the score of the sentence is bigger than\\nthe threshold, which is 1.5. But that's the same\\nthing as saying that the score -1.5 is\\nlarger than zero. In this case, this -1.5\\nis called the bias. The way to include this into\\nthe matrix multiplication is to add an entire column\\nwith the number 1 in the data table and a role with the bias with a\\n-1.5 in the model. Now instead of checking if the results is larger than\\nthe threshold of 1.5, we only have to check if the modified score is\\npositive or negative. That gives us the\\nexact same classifier. Sometimes you'll see\\nclassifiers with the bias and sometimes\\nwith the threshold. For more complicated\\nneural networks, the bias tends to\\nbe more common. But let's not worry\\nso much about the bias and continue\\nwith the threshold, and instead let's look\\nat a simpler problem. This is called the end operator. The dataset is\\nactually very similar. It's just four rows of the previous dataset with\\nthe same labels as before, except now it's not a\\nspam or not spam problem. Now the end column simply\\nsays yes if both the x and the y columns containing\\none and says no otherwise. That's just the very\\nwell known end operator for two elements. Now why do I bring\\nthe end operator up? Because if you consider\\nthis as a small dataset, then you can model it\\nwith a neural network. In fact the exact same one you used for\\nthe spam detector. When you multiply the matrix\\nby this model vector, you get these results over here. Which when you check with the\\nsame threshold as before, you get predictions that look exactly like the end dataset. The end dataset can be\\nmodeled as a perceptron, as a one layer neural network. Graphically, here's\\nthe classifier nodes. It seems exact same\\nline as before, which separates the points, except that now there's only\\na subset of the points. The equation of the\\nline is the same one , 1*x+1*y-1.5 = 0. Here's a graphical way to represent the previous\\nand perceptron. The inputs are x and y, and the bias is -1.5. In this diagram,\\nthe number inside the node gets multiplied\\nby the weight in the edge, and then they get added\\nin the next node, then the activation\\nfunction is applied. The activation is\\nprecisely, the check. It's the one that returns\\na one or a yes if what it comes is non negative and a zero or no if what it\\ncomes is negative. Using variables, this is the perceptron that\\nappears in the lab. The input is x coming\\nfrom the dataset. The weights of the model are\\nthe Ws and the bias is b. Inside the node, there appears a dot product that is\\nadded to the bias. The output of the\\nnode then gets passed the activation function and that generates the output\\nof the perceptron. This is the end of the\\nlectures for Week 3, but there are still some exciting activities\\nwaiting for you. You have a graded quiz that covers all the topics in Week 3, then you have a\\nprogramming assignment in which you'll use\\nyour knowledge of matrix multiplication\\nto implement pieces of a neural\\nnetwork yourself. You should already\\nbe able to see the ways that the linear\\nalgebra concepts you're learning power some of the most widely used machine\\nlearning algorithms. To prepare you for\\nthat assignment, you'll find a couple of\\nungraded labs that teach you important skills in\\nPython. Good luck.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In the previous videos you have seen\\nmatrices as a race of numbers that represent systems of linear equations. But there is another very powerful and\\nvery useful representation of matrix and it is as linear transformations. A linear transformation is a way to send\\neach point in the plane into another point in the plane in a very structured way. Here, I show you how. In this video you'll see an illustration\\nof how matrix is a linear transformation in two dimensions, but I invite you\\nto imagine it in more dimensions, such as a three dimensional space for\\nexample. Say you have this two by two\\nmatrix with entries 311 and 2. So here's the linear\\ntransformation it corresponds to. First consider two planes\\nwith access labeled a and b. The transformation will send every point\\non the plane in the left to a point in the plane in the right\\nin the following way. Any point has two coordinates. Those two coordinates\\nform a column vector. To get the vector in the right,\\nwe multiply the first vector by the matrix and whatever you\\nget is the point in the right. So this will be easier with some examples. First, let's look at\\nthe point in the origin. The 0 0 that becomes the vector 0 0, which we multiplied by the matrix,\\nwe get the vector 0 0. So 00 goes to 00. This actually always happens\\nwith linear transformation. The origin gets sent to the origin. Now let's look at the point 1 0. So the matrix times the vector\\n1 0 gives us a vector 3 1. So 1 0 goes to 3 1. Now let's look at 0 1. The matrix times 0 1 is equal to 1 2. So 0 1 goes to 1 2. And finally let's look at 1 1. Matrix times the vector 1 1 is\\nthe vector 4 3, so 1 1 goes to 4 3. And actually this defines\\nthe entire transformation. So let's look at this little square\\nform that these four points. It goes to this parallelogram. The square on the left is called a basis. And so\\nis the parallelogram on their right. And they're very,\\nvery important concepts in linear algebra. And you'll know why they're\\ncalled basis in a little bit and a very special property that basis have\\nis that they cover the entire plane. So actually, since this square\\nactually tessellates the whole plane and the parallelogram tessellates\\nthe whole plane as well. Then the linear transformation is simply\\ndefined as a change of coordinates. So for example, if we want to\\nfind where the point minus 2, 3 goes well, the point minus 2,\\n3 on the left is obtained by starting at the origin walking two blocks\\nto the left and three blocks up. So to find where that point goes in the\\nlinear transformation we simply started the origin and\\nwalk two blocks to the left and three blocks up in these new coordinates. And we get the point minus 3, 4. And so the matrix times vector minus 2,\\n3 is the vector minus 3, 4. In the language of buying apples and\\nbananas. I like to see transformations\\nin the following way. Let's say that you go to the store on\\nthe first day you buy three apples and one banana. And on the second day you buy\\none apple and two bananas. So if the price of apples is a and\\nthe price of bananas in b. Then the resulting prices for days one and\\ntwo are the entries of the vector corresponding to the product of\\nthese matrix with entries 3 1 1 2. And the vector a b. The linear transformation takes you from\\na point in the left where the access are the price of an apple and the price\\nof a banana to a point in the right, where the access are the price\\nyou paid on the first day and the price you paid on the second day. So the apples are $1 and\\nthe banana are also $1. Then the first day you paid $4 and the second day you paid three does a\\nlinear transformation of the matrix sends the point with coordinates 1 1 to\\nthe point with coordinates 4 3. The change of coordinates then\\nis the price of apples and bananas to the price of the first day and\\nthe second day.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"A matrix can have a very special matrix\\nassociated with it called the inverse. When I think of the inverse of a matrix,\\nI think of the inverse of a number, namely the number that when multiplied to,\\nit gives us one. For example, the inverse of the number\\ntwo is the number one-half and the inverse of minus\\nfive is minus one-fifth. The inverse matrix is\\nprecisely that matrix for which the product of the matrices\\nis the identity matrix. In a linear transformation, the inverse matrix is the one that\\nundoes the job of the original matrix, namely the one that returns the plane\\nto where it was at the beginning. And here's how an inverse matrix works. Imagine that you have a linear\\ntransformation corresponding to the usual matrix with entries 3, 1, 1 and 2, and that turns the square\\ninto this parallelogram over here. Now there exist some transformation that\\nwould turn this parallelogram back into the original square. And don't worry about the entries of\\nthe matrix, we'll find them later. Just know that there is one and that would\\nmean that the composition of the two transformations is the one\\ncorresponding to the identity matrix, the one that does nothing to the plane. It leaves it by itself. So if the inverse matrix is the one with\\nentries a, b, c and d, that means that when we multiply the original matrix\\nby the inverse, we get the identity. Now just like with numbers,\\nwe're going to call the inverse, the original matrix elevated\\nto the power of -1. In the same way that you say two\\nto the minus one is one-half, then this matrix to the -1\\nis the inverse matrix. And this happens to have these entries and\\nyou can verify that if you were to multiply the original matrix by this one,\\nyou get the identity matrix. Now how do we find the entries\\nin this inverse matrix? The answer is by solving\\na system of linear equations. So notice that if we take\\nthe entries in this matrix on the left to be equal to\\nthis counterpart on the right, then we really have\\nthe following four dot products. Each one of these dot products\\ngives us some linear equation. Now you have a system of four linear\\nequations with four unknowns, a, b, c and d. We can use our usual methods of\\nelimination to solve it to get a = 2/5, b = -1/5, c = -1/5 and D = 3/5. So now here's a quiz. Find the inverse of the following matrix. If you find the task is impossible,\\nfeel free to click on I couldn't find it. So by solving the corresponding system of\\nlinear equations, we get the following. We get that the answers are a = 1/4, b = -1/4, c = -1/8, and d = 5/8. Now here's another quiz,\\nfind the inverse of the following matrix. And if you find that\\nthe task is impossible, feel free to click on\\nI'm reaching a dead end. And it turns out that\\nthe inverse doesn't exist. We need to solve the following system\\nof linear equations a + c = 1, 2b + 2d = 0, 2a + 2c = 0 and b + d = 1. Now this is clearly a contradictions since\\nthe first equation says a + c = 1 and the third equation says 2a + 2c = 0, but\\nif a + c = 1, then 2a + 2c has to be 2. It cannot be 0. So therefore this matrix\\ndoesn't have an inverse.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"When I think of numbers and\\nmultiplication, I think of the number one\\nas a very special number. See, one is special because when\\nyou multiply any number by one, you get the same number you started with. The identity matrix satisfies this\\nexact same role among the matrices. The identity matrix is the matrix that\\nwhen multiplied by any other matrix, it gives the same matrix, and its corresponding linear\\ntransformation is very simple. It is the one that\\nleaves the plane intact. Here's how it looks. The identity matrix has\\na very simple look. It has ones in the diagonal and\\nzeros everywhere else. And why does it work? Because when you multiply it by any\\nvector, say, of entries a, b, c, d and e, you can verify that the resulting\\nvector is also the one with entries a, b, c, d and e. I encourage you to verify\\nthis with pen and paper. For example, the first entry\\ngets calculated as follows. The dot product between the first row and\\nthe vector, which you can check that it's a. And the identity matrix in a linear\\ntransformation is very simple, as it sends each point\\nprecisely to itself. As you can see,\\nthis matrix sends the vector 0, 0 to 0,0, the vector 1, 0 to 1, 0, etc. And that's why it's called\\nthe identity matrix.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"In the previous video,\\nyou learned how to turn a matrix into a linear\\ntransformation, but going the other way\\naround is just as easy. Here I'll show you\\nhow to start with a linear transformation and then find the\\ncorresponding matrix. Let's say we have some\\nincognito matrix, and we know it sends this fundamental square or basis into this one over here, and the goal is to find\\nthe entries of the matrix. Let's look at where it\\nsent each of the points. The point 0, 0 gets sent to 0, 0, which always happens. The point 1, 0 gets sent to 3, -1, which is the bottom corner. The point 0, 1\\ngets sent to 2, 3, and the point 1, 1\\ngets sent to 5, 2. It turns out we don't need\\nthat much information. We only need these\\ntwo points over here. I'm going to use arrows\\nto denote this point, since vectors tend to\\nbe denoted by an arrow growing from the origin\\nall the way to the point. The vector with coordinates 1, 0 gets sent to the vector\\nwith coordinates 3, -1, and the one\\nwith coordinates 0, 1 gets sent to the one\\nwith coordinates 2, 3. Now I invite you to take a piece of paper and do the math and convince yourself\\nthat that's how matrix multiplication works. But the matrix that\\nsends the vector 1, 0 and 0, 1 to 3, -1 and 2, 3 is precisely the one\\nthat has columns 3, -1 and 2, 3. That is how you turn a\\nlinear transformation into a corresponding matrix. You only look at where the\\ntwo fundamental vectors 1, 0 and 0, 1 go, and those are your\\ncolumns of the matrix. Next you'll have a chance to use an interactive tool to explore the relationship\\nbetween matrices and linear transformations. This is probably my\\nfavorite one in the course. You'll be able to\\nchoose different values for a two by two matrix, and watch the way it\\nsqueezes and rotates at two-dimensional\\nspace when viewed as a linear transformation. As always, there are some\\nsuggested activities for you below the tool. Enjoy.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"Previously, you've\\nlearned how to multiply matrices and vectors. In this video,\\nyou'll learn how to multiply matrices and matrices. The product formula\\nis very intuitive, but what I really\\nenjoy is how it looks like for linear\\ntransformations. In short, matrix\\nmultiplication corresponds to combining two linear\\ntransformations into a third one. Follow me and I'll show you how. Imagine that you have\\na matrices with 3, 1, 1, 2, the one\\nyou've already seen. Let's quickly take a look at the linear\\ntransformation here. The vector 1, 0 goes to 3, 1, which gets plotted here. The vector 0, 1 goes\\nto the vector 1, 2, which gets plotted here. Then the fundamental\\nbasis on the left gets sent to this\\nbasis on the right. Let's do another one, but let's start with the\\nbasis on the right. Let's see what linear\\ntransformation corresponds to the\\nmatrix 2, -1, 0, 2. Let's see how it acts on\\nthe two basis vectors, which now are the vector 3, 1. 3, 1 goes to 2, 5. Therefore, this vector over here goes to point 2, 5 on the right. The other vector is 1, 2. This matrix times 1, 2\\nbecomes the vector 0, 4, so we plot 0, 4. Then this parallelogram\\non the left turns into this\\nparallelogram on the right. That is a way to see that\\nlinear transformation. Now let's put them together. We have the first\\nlinear transformation corresponding to\\nthe matrix 3, 1, 1, 2 and the second\\none corresponding to the matrix 2, -1, 0, 2. Now if we forget\\nabout the middle one, then there's a linear\\ntransformation between the first and the third, and that has to correspond\\nto some matrix. The question is, what matrix corresponds to that\\nlinear transformation? Let's simplify a bit and only\\nlook at the basis vectors. By looking only at the\\nleft and the right, we'd see that the vector 1, 0 on the left gets sent\\nto 5, 2 on the right. Therefore, as we saw before, the first column of\\nthe matrix is 5, 2. The other basis\\nvector, vector 0, 1, gets sent to the vector 0, 4. Therefore, the second column\\nof the matrix is 0, 4. The combination of the two\\nlinear transformation gives us the linear transformation\\ncorresponding to the matrix 5, 0, 2, 4. Now, is there any\\nway we can obtain the matrix from the first to the third from the\\nother two matrices? The answer is yes, we can. That's what matrix\\nmultiplication is. This operation over\\nhere is matrix on the left times matrix on the right equals\\nthe third matrix. Now notice something that\\nthe matrix got flipped. This corresponded to the\\nfirst linear transformation, the one on the left, and this corresponded to the second one. That's something very\\nimportant to keep in mind. The reason is because the\\nlinear transformations act on the vector on the left, so you multiply\\nmatrix times vector. Therefore, you first\\nmultiply the matrix 3, 1, 1, 2 and then multiply\\nthe matrix 2, -1, 0, 2 so they go in\\nthe opposite direction. Now, is there a fast way to see matrix multiplication\\nwithout having to draw linear transformations? The answer is yes, and you've pretty much seen it already because it's a\\nlot of dot products. Basically look at the matrix on the left as two rows and at\\nthe matrix on the right as two columns and take every possible combination of dot products between\\nrow and columns. For example, the first\\nrow and the first column go in the top left corner, and the first row of the first and the second\\ncolumn on the right go in the top right corner. Now let's calculate\\nthese dot products like we already know. The first one is five, the second one is zero, the third one is two, and the fourth one is four. The matrix 5, 0, 2, 4 is the product of these\\nother two matrices. In summary, you can look at a matrix product this way as\\nmultiplying two matrices, but you can also\\nsee it as combining two linear transformations into a third linear transformation. Every matrix in this example\\nis a two-by-two matrix, but multiplication\\nis also defined when your matrices\\naren't square like this. Let me show you an example. This time I'll multiply\\na two-by-three matrix and a three-by-four matrix. The result will be a\\ntwo-by-four matrix. Before I multiply\\nthese matrices, see if you can predict\\nwhat value would be in the bottom left\\ncell of the result. Let's see if your\\nprediction was correct. Remember, to multiply a matrix, you take rows from the first matrix and\\ncolumns from the second. The first dot product is\\nthe dot product of 3, 1, 4 and 3, 1, -2.\\nThe result is two. The next dot product is 3, 1, 4 and 0, 5, 1 giving a result of nine. Completing this row\\nof the matrix gives the dot product of 21 and -6. Now let's check that bottom\\nleft cell of the matrix. Following the pattern, I'll\\ntake the dot product of the second row of\\nthe first matrix and the first column of\\nthe second matrix. That's 2*3+-1*1+2*-2, which\\ngives a final answer of one. Continuing with\\nthe same pattern, you can solve for the remaining\\nvalues of the matrix. These examples show you that\\nit's possible to multiply matrices even if they're\\nboth rectangular. Here are the key takeaways\\nfrom the example. First, the number of\\ncolumns of the first matrix must match the number of\\nrows of the second matrix. In other words, these numbers circled in here need to match. Second, the result matrix takes the number of rows\\nfrom the first matrix. In other words, these numbers circled in blue need to match. Finally, the result takes the number of columns\\nfrom the second matrix, so these numbers circled\\nin purple need to match. Otherwise, though,\\nthis works just like the two-by-two\\nexamples you saw before. You are simply taking\\nthe dot product of rows from the\\nfirst matrix and columns from the\\nsecond matrix to fill in each cell of your\\nresulting matrix.\",\n",
       "  'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'),\n",
       " (\"Now this method is actually\\nmore powerful than you think. In the previous examples,\\nyou saw it in action for a non-singular system that\\nhas a unique solution. But what happens with a singular\\nsystem namely one that does not have a unique solution. Let us solve the singular system that also\\ncame up before the system of equations a + b = 10 and 2a + 2b = 20. Recall that the system was redundant since\\nthe second equation is equivalent to the first one. So let's try the same steps as before\\nto try to eliminate a from the second equation and see where that takes us. The first step is to divide both\\nequations by the coefficient of a getting the two equations a + b = 10 and\\na + b =10. Notice that they're the same. The next step is to remove a from\\nthe second equation by subtracting the equation a + b = 10 from\\nthe equation a + b = 10. Now what do you get here? Well, you get 0 = 0 because you're\\nsubtracting the entire string from itself. This equation 0 = 0 is trivially true and unfortunately gives you zero\\ninformation about what b can be. What happened? Well, in your efforts of removing a from\\nthe second equation or to leave b alone, you also remove b. And there is nothing you can do to get b. There's no manipulation you can do\\nto remove only one of the variables because anything you do to remove\\none will also remove the other one because the system is singular. And again, this is because the second\\nequation adds no value to the system. So in other words, the solved system\\nis this one where a + b = 10 and there's no other equation to\\ngive us more information. So can you still get the solution\\nto look like a equals something and b equals something else? Well you still can pick any number x and\\nlet a equals be x. How much is b then? Well if a + b = 10, then b must be 10- x. This is a solution to the system. It's not a unique solution\\nbecause x can be any value. So the solve system has one degree of\\nfreedom and if you vary this degree of freedom x, you can get many\\ndifferent solutions to the system. The solutions for\\nexample form a line as you said before. Now, what happens if you use the same\\nmethod with a singular system that is contradictory? Recall that the contradictory\\nsystem that you saw earlier was this one a + b = 10 and\\n2a + 2b = 24. And let's use the same steps to solve it. First we want to eliminate\\na from this equation. So we divide both equations\\nby the coefficient of a. We subtract equation 1 from equation 2 and we get that 0 n the left\\nis equal to 2 on the right. 0 is never equal to 2 so\\nthat's a contradiction. Therefore there are no\\nsolutions to the system. So now you're ready for a quiz. The quiz is the following. Can you solve the following system of\\nequations 5a + b = 11 and 10a + 2b = 22. So if you look closely into\\nthe two equations in the system, you will find that equation\\n2is equation 1 times two. Or equivalent if you take equation 2 and\\ndivide it by 2, you obtain equation 1. Therefore the system is singular and\\nit has infinitely many solutions.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Before we get into the whole\\nrow reduction process, it is important to note that the same manipulations\\nthat you use to solve systems of\\nlinear equations can be used in matrices. These are called row\\noperations in a matrix and a very important\\nproperty that they have is that they preserve the\\nsingularity of a matrix. In other words, if you apply\\nthem to a singular matrix, you get a singular\\nmatrix and if you apply them to a\\nnon-singular matrix, you get a non-singular matrix. In this video, you will learn more about\\nthese row operations. Consider the matrix\\nwith entries 5, 1, 4, and 3. First, check this\\nmatrix is singular or non-singular by calculating\\nthe determinant. The determinant is 5 times 3 minus 1 times 4, which is 11. Therefore, the matrix\\nis non-singular. Now the first-row operation that you learn is\\nswitching rows. If you switch the\\npositions of the two rows, you get the matrix\\nwith entries 4, 3, 5, and 1. I promise you that's in\\nthe original matrix, non-singular, then this\\none is also non-singular. How do I know this? Well, let's calculate the determinant. The determinant is 4\\ntimes 1 minus 3 times 5, which is minus 11, which is different than\\nzero. It's non-singular. In fact, the determinant\\nof the matrix obtained after switching\\nrows like this, it's always a negative of\\nthe original determinant. The reason for this\\nis that the diagonals change place so the\\none you're adding, you're now subtracting, and the one-year or subtracting\\nnow you're adding. Now instead of\\ngetting 15 minus 4, you get 4 minus 15, hence the minus 11. You can imagine that if\\nthe original determinant was a zero, then the resulting one\\nwould also be a zero, which means that if you apply\\nthis to a singular matrix, you've got a singular matrix. Unlike wise, if the original\\ndeterminant was not zero, then the resulting\\nwas also not zero. Row switching\\npreserves singularity or non-singularity of a matrix. The next operation is multiplying a row by\\na non-zero scalar. Let's use the same matrix\\nwith determinant 11. Now, let's modify the first row. We're going to leave\\nthis second row by itself and the first row, we're going to multiply\\nit by a number, say 10, to get 50,10. Now that's going to\\nbe the new first row. What's the determinant of\\nthe new modified matrix 50, 10, 4, 3? It's this and notice, that it is 10 times 11, 10 times the determinant\\nof the original matrix. Because in each\\nof the diagonals, you have taken\\nexactly one element, the one on the top row, and multiplied it by 10 so the whole thing gets\\nmultiplied by 10. Now notice that the scalar\\nhas to be non-zero. In the scalar that\\n10 is non-zero, then this operation turns a\\nnon-zero determinant into non-zero determinant\\nand a zero determinant into a zero determinant. Thus, this operation also preserves singularity\\nand non-singularity. The final operation is to take a row and add it to another row, for example, let's\\ntake the sum of the first and the second row, and that is 9, 4, so 9, 4 is going to be the top one\\nof the rows in the matrix, and the bottom one is\\ngoing to be the same. This new determinant is 9\\ntimes 3 minus 4 times 4, which is actually 11. Believe it or not,\\nwhen you do this, you get the same determinant\\nas the beginning. Proving this is slightly\\nharder and you can find it in a written tandem in the\\nresources of the class. But the most important\\npart is that since the determinant stays the\\nsame after this operation, then this operation\\nalso preserves singularity and non-singularity\\njust like the other ones.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"You are now ready to learn\\nsomething that has been lurking yet present in the previous lessons. In one of the first quizzes,\\nyou took a system of linear equations and found its solution. In general, this is hard to do by\\nsimply looking at the equations. However, there exists\\na simple way to do this. In this video, you will learn a method or\\nan algorithm that helps you find a solution to a system\\nof linear equations and that is also able to tell you if this\\nsystem is singular or non singular. First, let's go back to how you\\nsolve the first system of equations. The one with equations a + b = 10 and\\na + 2b =12. Recall that the scenario was that an apple\\nand a banana cost 10 and an apple and 2 bananas cost 12. So you figured out that since the second\\nday you bought an extra banana and paid 2 more,\\nthat that extra banana must cost 2. From this, and\\nthe fact that they both cost 10, you concluded that the apple must cost 8. Thus, you went from the original system of\\nequations to a solved system, one that is still a system of equations, except\\nthat the equations are a = 8 and b =2. The solved system is much simpler because\\neach equation actually tells us the value of each of the variables. The goal is to take every system and\\nturn it into a solved system. That is, if the system is non singular and\\nhas exactly one solution. Now, in order to get from\\nthe system to the solved system, you followed some process. This process required manipulating\\nthe equations without noticing what you actually did\\nin your head was to follow a specific process that involves\\nmanipulation, such as swapping equations, adding them,\\nmultiplying them by constants. In this video you'll see what\\nthis means more in detail. The first step in order to go from\\na system to a solved system is the following. Notice that in the system on the left,\\nthe equations both have a and b on them. You'd like to get to one in which\\nthe a and the b are isolated and they only appear in one equation each. The first step will be to look\\nat the second equation and try to eliminate the variable a from it. But before you do that, it's important to\\nknow how one can manipulate equations. That is, take some linear equations and produce some other ones that are still\\ntrue based on the original ones. One way to manipulate equations is\\nto multiply them by a constant. For example, if an apple and\\na banana cost $10, how much would 7 apples and\\n7 bananas cost? Well, the answer is $70,\\nas this is 7 times 10. Equation a + b = 10 can be\\nmultiplied by 7 on both sides to get the equation 7a + 7b = 70. And these equations carry\\nthe exact same information. Thus, if the first one is true,\\nso is the second one. Another way to manipulate\\nequations is to add two equations. For example, if an apple,\\na banana cost $10, and 2 apples and 3 bananas cost $26,\\nthen how much are 3 apples and 4 bananas? Well, if you add these two equations, you get that 3 apples plus\\n4 bananas is equal to 32. If the two first equations are true,\\nthen so is their sum. So now I'll show you how to solve\\na harder system of equations in order for you to see how these\\nmanipulations are used. So, here's the system of\\nequations that you learn to solve. The equations are 5a + b = 17,\\nand 4a -3b = 6. And recall that your first goal in order\\nto get to a solved system is to try to eliminate the variable a from the second\\nequation in order to leave b by itself. The first step is not always necessary,\\nbut it will make your life easier. The step is to divide each equation\\nby the coefficient of a in order for both of them to have a coefficient\\nof one beside the a. So, if we do this,\\nthe new equations are a + 0.2b = 3.4, and a- 0.75b =1.5. And as you saw before,\\nthese are equivalent to the first two, because all we did was multiply the entire\\nequation left and right by a constant. Now, in order to remove\\na from the second equation, one way is to subtract the first\\nequation from the second one. Like this, we take the second equation, we subtract the first one, and\\nwe can do this component wise. A minus a is 0a,\\n-0.75 b minus 0.2b is -0.95b, and 1.5 minus 3.4 is -1.9, the resulting equation is -0.95b= -1.9. Now, we can divide it by -0.95\\nat both sides to get that b =2. So you have succeeded. You have found the value of b. In other words, you removed a from the first equation\\nto get b=2 in your solve system. Now that you know that b =2, you can\\nsimply plug that into the first equation. The first equation says a + 0.2b is 3.4. So a + 0.2(2) = 3.4, 0.2 times 2 is 0.4. So we have a + 0.4 = 3.4, and\\nsubtracting 0.4 from both sides, you get that a = 3 and\\nthat is the value of the second variable. So in a nutshell, that is how you solve a\\nsystem of two equations and two variables. Now here's a small caveat,\\nimagine that you are solving this new system with\\nequations 5a + b = 17 and 3b = 6. And you'd like to eliminate\\na from this equation. The first step is to divide\\nboth by the coefficient of a. For the first equation,\\nthis goes exactly as before. However, for the second equation,\\nthe coefficient of a is 0 and you can't divide by 0. So, are we doomed? Well, actually,\\nthe doom turns into good luck. Check it out, the equation says 3b = 6. It already has a eliminated from it and actually implies that b=2 if\\nyou divide by 3 on both sides. So this is the value of b already, and the second equation is\\nalready solved in the system. In order to solve the first one,\\nyou proceed to do the same thing. Replace on the first equation that b =2, get that a =3, and\\nbring it back as the solution. So you're now ready for a quiz. Solve the following system of equations,\\n2a+ 5b =46 and 8a+b = 32, and the solution to\\nthe system is a =3 and b =8. I invite you to verify that those\\nvalues actually work for the equations.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now that you learn how to solve systems of two equations\\nwith two variables, let me show you how\\nto solve systems of three equations with\\nthree variables. It's actually very similar. The goal here is first\\nto leave a by itself. We're going to make sure that the only equation that contains\\nan a is the first one, and that there's no a on the\\nsecond and third equations. The way to do this is to\\nnormalize the first column. Namely, divide each row by\\nthe coefficient of a to make sure that every equation\\nhas a coefficient of one for the variable a. Now, use the first equation to remove the variable a\\nfrom all the other ones. Subtract the first equation\\nfrom the second and then from the third\\nto get the following. Now, in the resulting system, you have successfully isolated a and what remains\\nis a system of two equations with two\\nunknowns that are b and c. You know exactly\\nhow to solve these ones. When you solve it, you get the values for b and c.\\nLet me show you how. Now, let's forget about\\nthe first equation and solve the second and third,\\nand you know how to do this. First, divide these two\\nrows by the coefficient of b to get this where the leading coefficient\\nof b is now one. Now, use this equation, the second one, to remove\\nb from the third one. Subtract the second equation\\nfrom the third one to get this -11/6c = -11/2. In this way, you have isolated\\nb in the bottom two rows. From here, you can divide\\nby -11/6 to get that c = 3. Now let's go back\\nto the beginning. Now we know that c = 3. Then we can go\\npropagate upstairs, so replace c = 3 in the\\nsecond equation to get b = 2. Now that you know b\\nand c replace b = 2 and c = 3 in the first\\nequation to get a = 4. The final solution is a = 4, b = 2, and c = 3. After this, you'll get to use a new interactive\\ntool to explore systems of equations\\nin three dimensions. You'll be able to\\nsee each equation as a plane floating in\\nthree dimensional space, and the solution to the system at the intersection\\nof these planes. It's a fun tool to play\\nwith and hopefully deepens your understanding of the concepts you've\\nbeen studying. As always, some\\nsuggested activities are below the tool\\nitself. Enjoy.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now that you know how to\\nfind the solutions to linear equations by\\nmanipulating them, you pretty much know what\\nmatrix row reduction is. Matrix row reduction, also called the Gaussian elimination, consists of applying the\\nexact same manipulations except to the rows of a matrix. In order to turn\\nthat matrix into a much more simplified form, from which you can\\nextract lots of useful information.\\nLet me show you how. We call them when\\nyou want to solve a system of linear equations, say the one with equations, 5a plus b equals 17 and\\n4a minus 3b equals 6. You first went through an\\nintermediate step of removing the variable a from\\nthe bottom equation in order to calculate\\nthe value of b. Then you went through a step of replacing the value of b in the first equation in order\\nto get the value of a. The solved system has equation\\na equals 3 and b equals 2. Now, what happens if you follow the same procedure but in\\nthe matrix of coefficients? Here's the matrix corresponding to the original system\\nand recall that we can forget about the\\nconstant 17 and six and only pick up\\nthe coefficients 5, 1, 4, and minus 3 and here is the matrix corresponding to\\nthe intermediate system. Now you'll see this\\nmore in detail later, but for now bear with me. From the original matrix, you can get the matrix in the intermediate system by applying some row manipulations. An important feature\\nof this matrix is that it has ones in the main diagonal and zeros\\nunderneath the diagonal. This form of matrix is called row echelon form and finally, some more manipulation will\\nget you to the matrix with ones in the diagonal\\nand zeros everywhere. Why is it the matrix corresponding\\nto the system above? Because you can see the\\nsystem of equations a equals 3 and b equals 2 as a\\nsystem of equations, 1a plus 0b equals 3 and\\n0a plus 1b equals 2, from which this matrix\\nwith entries 1, 0, 0, and 1 stems. The first matrix, the\\none in the middle, is called a row echelon\\nform and this one over here is called the\\nreduced row echelon form. This one will be introduced\\nmore later, but for now, let's focus on the row\\nechelon form one as it gives us a lot of useful\\ninformation of the matrix. Now what happens if you\\nhave a singular system of equations such as the one\\nyou've seen in the beginning. As you've seen before,\\nthis system can be manipulated and turned into\\nthis system over here, where the first\\nequation is the same, but the second one is\\nan obvious 0 equals 0. Thus the original matrix can be manipulated and turned into\\nthe matrix with entries 1, 1, 0, and 0. This is also a row echelon form. Here's another example from\\nthe quiz on the last video. In this system, you\\nhave the equations, 5a plus b equals 11 and\\n10a plus 2b equals 22. Notice that subtracting twice the first equation\\nfrom the second one, you get the trivial equation, 0 equals 0, indicating that\\nthe system is singular. In the matrix we've\\ngone from this one over here with entries 5, 1, 10, and 2 to this one\\nwith entries 1, 0.20 and 0, and that one is the\\nrow echelon form. Finally, let's look at this\\nvery singular system of equation 0 equals\\n0 and 0 equals 0. The system can not be\\nmanipulated any further, so the row echelon form of this matrix is actually itself. The general way that a matrix in row echelon form looks\\nlike is the following. On the main diagonal, we have a bunch of ones followed by perhaps a bunch of zeros. You could potentially\\nhave all ones, but you could also\\nhave all zeros. Below the diagonal,\\neverything is a zero, to the right of the ones any number is allowed and finally to the\\nright of the zeros, everything must be zero. Following these rules in the\\ncase of 2 by 2 matrices, only three things may happen. You have 2, 1s in the diagonal. You have 1, 1 in the diagonal or you have 0, 1s\\nin the diagonal. These are the ones you've\\nseen earlier in the video.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now that you've seen\\nrow echelon form for two-by-two matrices. Let me show you what it\\nis for bigger matrices. Recall this system of equations and recall that the\\nprocess you took to solve it was this\\nintermediate step over here, where the first equation\\nhas variables a, b, and c, the second one only\\nhas variable b and c, and the third one\\nonly has variable c. The exact same\\nrow operations can be done in the matrix\\ncorresponding to the system in order to get\\nthe matrix on the right, which now has ones in the diagonal and zeros\\nunderneath the diagonal. This is the row echelon\\nform of the matrix. Now, this is the way a\\nrow echelon form matrix looks like in general, here are two different examples. In here, the stars\\nrepresent numbers that could be zero or\\nnon-zero, it doesn't matter. The matrix may or may not\\nhave rows full of zeros. However, if it does, they need to go at the bottom. Furthermore, notice\\nthat every row that is non-zero has a leftmost\\nnon-zero entry. These are called the pivots. Every row has a particular pivot and there's a rule about\\npivots which said that every pivot has\\nto be strictly to the right of the pivots\\nof the row above. In other words, if you stand at a pivot and you look at\\nall the pivots above, they all need to be\\nstrictly to its left. Now the row echelon\\nform, as you saw before, is very useful to tell\\nyou the rank of a matrix. It's actually the\\nnumber of pivots. The matrix on the\\nleft has Rank 5 and the matrix on the\\nright has Rank 3. Now here's a very\\nimportant note in terms of notation that we're\\ngoing to use in this class. On the left, you see a\\nrow echelon form matrix. Now you could do some cosmetics and divide the\\nfirst row by three, the second one by one, or they mean the same, and\\nthe third one by minus four to get a matrix\\nthat looks like this. Obviously the stars are\\nnow different values, but the important thing is that the pivots are now one and\\nthey're in the same location because dividing by\\na number one turn a zero into a non-zero or\\na non-zero into a zero. In most textbooks, the form\\nis the one on the left. In general, pivots different\\nthan one are allowed. However, for this class, we're going to use\\nthe form in the right. We're going to take that\\nextra step of dividing by the leading entry in the pivot in order to get\\nthe pivots to be one. This makes no mathematical\\ndifference in terms of rank. It's the same rank and it's more consistent\\nwith the way we are solving our system of equations by dividing by\\nthe leading coefficient. But this is definitely\\nsomething to keep in mind. Here are some other examples of row echelon forms of matrices. The matrix over here\\nthat you've seen before can be reduced to this one by simply subtracting the first row from the\\nsecond row and third row. Notice that the one on the\\nright is in row echelon form. Now what if the\\nmatrix is singular? Let's try to find the\\nrow echelon form of this singular matrix over here,\\nwhich you've seen before. First subtract the\\nfirst row from the second and third ones to\\nget this matrix over here. Now take the second row, multiply it by two, and subtract it\\nfrom the third one. You get this matrix over here. That is the row echelon\\nform of that matrix. Now let's find the\\nrow echelon form for another singular matrix\\nthat you've seen before. For this matrix, what you can\\ndo is take the first row, multiply it by two, and subtract it from the\\nsecond row to get this matrix. Now, you can again take the first row and multiply\\nit by three and subtract it from this third row\\nto get this one over here. Now check this out. Just like for\\ntwo-by-two matrices, the rank of the\\nmatrix is the number of pivot ones in the\\nrow echelon form. Here we have the\\nrow echelon forms that we've just calculated. If you look at the pivot ones, there's three here, because\\nthe matrix has Rank 3, two here, because the matrix has Rank 2 and 1 here and zero here. That's exactly the ranks of\\nthe corresponding matrices.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now that you've learned\\nthe row echelon form, let me show you the\\nreduced row echelon form, which is just one more step. Let's say that you're solving this system 5a plus b equals 17, and 4a minus 3b equals 6. Recall that what you do is go to an intermediate\\nstep of removing the variable a from\\nthe bottom equation in order to calculate\\nthe value of b. Then you want to step up\\nreplacing the value of b in the first equation to\\nget the value of a. Soft system then has equation\\na equals 3 and b equals 2. Now following the\\nsame procedure, but with the corresponding\\nmatrix of coefficients, forgetting about the\\nconstants 17 and 6, you go through the\\nintermediate row echelon form with entries 1,0.2 and 1 using\\nmanipulations of the rows. Finally, some more manipulation we'll\\nget it to the metrics would ones in the diagonals\\nand zeros everywhere else. Why is this the matrix\\ncorresponding to the system above? Because you can see the\\nsystem of equations a equals 3 and b equals 2 as a\\nsystem of equations, 1a plus 0b equals 3 and\\n0 equals 1b equals 2, from which this matrix with\\nentries 1001 comes out. Recall that the\\nintermediate matrix is called the row echelon form. The final one is called the\\nreduced row echelon form, is the one corresponding\\nto solve the system. The way to get from the\\nrow echelon form from matrix to the reduced\\nrow echelon form is simply to use each one\\nin the diagonal to disappear all the\\nnon-zero entries above. For example, here you\\nlike to get rid of that pesky 0.2 at the top right. In this case, you can\\nleave the bottom row untouched and from\\nthe first row, you subtract 0.2\\ntimes the second row, so you get 0, 0.2\\nsubtracted from the first row and get the\\nrow 1,0 that's your new row, one of the matrix and that's the reduced row echelon form. The way I've reduced\\nrow echelon form matrix looks in\\ngeneral is this one. Here are two examples. The first rule is that it has\\nto be in row echelon form. Furthermore, each\\npivot has to be a one, and any number above the\\npivot has to be a zero, so that's the main difference. That the numbers above the\\npivots have to be zero. A nice property\\nthat it has is the same as the row\\nechelon form property, which is that the rank of the matrix is actually\\na number of pivots. This one on the\\nleft has rank 5 and the one on the right has rank 3. Here is the general\\nmethod to go from a row echelon form matrix to\\na reduced row echelon form. Let's say you have one\\nover here where these are the pivots and they're\\nallowed to be not ones, and remember that\\nfor this course, we're using a row echelon form\\nwhere the pivots are one. But if you were\\nto not have ones, that's no problem\\nyou can just divide each row by the\\nleading coefficient. The first one by three,\\nthe second one by two, and the third one by minus\\n4 to get this matrix on the right where\\nthe pivots are ones. Now all you have to do to get to the reduced row\\nechelon form is to use each one to clear\\nout any number above it. For example, if above the one, you have a five, all you do is multiply by five, the row with the one, and subtract it from\\nthe previous one. Here's a small example.\\nLet's say that this is the row echelon\\nform of the matrix, and we're going\\nto turn that into a reduced row echelon form. The first thing is to get rid of that two in the first row, and for that we can subtract 2 times the second row\\nfrom the first one to get rid of that and that\\nchanged some numbers around, but at least it turn\\nthat into a zero. Now, let's get rid\\nof the minus 5, so take the third row multiplied by 5 and add\\nit to the first row, so that gets rid\\nof that minus 5. Now to get rid of that four, take the third row and subtract it from the first one after\\nmultiplying it by four. Multiply by four, subtract\\nit from the first one, and here you get the reduced row echelon\\nform of the matrix.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now, just as it happened\\nwith 2*2 matrices, the rank is a measure of how\\nnon singular a 3*3 matrix is and its geometric meaning is very similar to that\\nfor 2*2 matrices. Follow along with me\\nand I'll show you how the rank works\\nfor larger matrices. In order to define the\\nrank of a 3*3 matrix, we'll look at the systems\\nof three equations and three unknowns that you've\\nbeen looking at so far. Let's focus on what equations bring new information\\nto the table. By not bringing any\\ninformation to the table, I mean an equation\\nthat is already a linear combination of the other equations\\nin the system. In system 1, you\\ncan check out that all the equations are\\nlinearly dependent. There's no way to obtain\\none out of the other two. Therefore, the system\\nhas three equations and all of them are new\\npieces of information, so it has three pieces\\nof information. The number of independent\\nequations is the rank, so the system has rank\\n3 and by convention, we say that the\\nmatrix has rank 3. Now, let's take a\\nlook at system 2. This can be done\\nin different ways, but one way to look at\\nit is to realize that the second equation is the average of the\\nfirst and the third, so you can think of\\nthe first and the third to be the new\\npieces of information. The second one doesn't bring\\nanything new to the table. If you do this in\\na different order, you can get different things. But you will always\\nget that there's three equations and two\\npieces of information. because the system has rank 2, and thus, the matrix is\\ndefined to have rank 2. System 3 is simpler since\\nthe first equation is new. But the second one is twice the first one,\\nso it depends on it, and the third one is also\\nthree times the first one, so it depends on the\\nfirst one as well. We have three equations and\\none piece of information. The system, therefore, has rank 1 and so does the matrix. As usual, system 4\\nis the simplest one. No equation brings\\nanything new to the table because no equation tells you anything about A, B, and C. You have three equations, zero piece of information, and the matrix has rank 0. There is a question. It seems that calculating the\\nrank is not that easy, so is there a simpler way\\nto calculate the rank? The answer is yes, and it has to do with the row\\nechelon form of a matrix.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"In the previous videos, you learn one particular form of a matrix called\\nrow echelon form. This gives you lots\\nof information about the matrix and it can be obtained with simple\\nrow operations. In this video, you will\\nsee this more in detail. Recall from previous videos\\nthat the row echelon forms of these three matrices are\\nthe ones in the right. However, previously, we used the systems of\\nequations to find them. Now you can simply use the row\\noperations you've learned. Follow me along to\\nsee how to do this. In order to calculate\\nthe row echelon form for this matrix with entries 5, 1, 4 and minus 3, here's\\nwhat you would do. The idea is to get rid of\\nthat four in the bottom left. First divide each row by the leftmost\\nnon-zero coefficient to get the matrix\\nwith entries 1, 0.2, 1 and minus 0.75. Now, in order to remove\\nthat bottom-left one, keep the first row the same, but subtract the first row\\nfrom the second row to obtain the row with entries\\nzero and minus 0.95. We have succeeded at\\ngetting a zero in the bottom left corner,\\nwhich is what we wanted. Now as a final step, divide the second row by the leftmost\\nnon-zero coefficient in order to get a one on\\nthe bottom right corner. Now the matrix is in\\nrow echelon form. What if you do this\\nfor a singular matrix? Well, if you try it for\\nthis matrix with entries 5, 1, 10, and 2, let's\\nsee what happens. The first step is\\nto divide each row by the leftmost coefficients, so you get the matrix\\nwith entries 1, 0.2, 1, and 0.2. Now in order to disappear\\nthat bottom left one, what you can do is\\ntake the bottom row, subtract the top row, and what you get is 0, 0. Now that's the new bottom row. Now let's see what happens\\nwhen you try to divide the second row by the leftmost\\nnon-zero coefficient. Well, that's impossible because you'd be dividing 0 by\\n0, which is undefined. That's no problem.\\nWhat you do is you let this one be the row\\nechelon form instead. Finally, for the very singular\\nmatrix entries 0, 0, 0, there's not a lot you can\\ndo because you cannot divide each row by the leftmost coefficients,\\nthose are zero. Instead, you just say this\\nis the row echelon form. In summary, here\\nare the matrices and here are the\\nrow echelon forms. Here is a very interesting\\nconnection with rank. Look at the first one. It has two ones in the diagonal, and it happen to have Rank 2. The next one has 1,\\n1 in the diagonal, and it happen to have Rank 1. The third one has zero ones in the diagonal and it\\nhappen to have Rank 0. That's actually the connection. The rank of a matrix\\nis the number of ones in the diagonal\\nof the row echelon form. That's an easy way to\\ncalculate the rank. Notice furthermore, that the\\nfirst one is non-singular, the second one is singular and\\nthe third one is singular. A matrix is NON-singular\\nif and only if the row echelon form has\\nonly ones and no zeros.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Throughout the last few videos, there\\nhas been a concept that you've seen yet still have not been\\nformally introduced to. It is the notion of the rank\\nof a matrix which in some way measures how much\\ninformation that matrix or its corresponding system\\nlinear equations is carrying. Follow along with me and\\nsee how to define and calculate the rank. One great application of ranking machine\\nlearning is an image compression. Take a look at this image,\\nit's very crisp, but it also uses a lot of storage because every pixel intensity\\nhas to be stored as a number. Could you store this image or perhaps a slightly blurrier version\\nof it using significantly less space. The answer is yes. And this is the main topic of this\\nvideo which is the rank of a matrix. It turns out that pixelated images are\\nmatrices and the rank of matrix is related to the amount of space that is needed\\nto store that corresponding image. This particular image has ranked 200,\\nwhich is quite high. There's a very powerful technique\\non singular value decomposition or SVD in short, which can reduce the rank of a matrix\\nwhile changing it as little as possible. You can see as video applied\\nhere to reduce a heavy image of ranked 200 into images of rank 1,\\n2, 5, 15 and 50. Notice how the images of rank 15 and\\n50 are very similar to the original and we take a lot less space to store. So recall that in systems of sentences, there was a notion of how much\\ninformation the system carried. Let's look at three systems of sentences. System 1, with sentences,\\nthe dog is black and the cat is orange. System 2 with sentences,\\nthe dog is black and the dog is black. And system 3 with sentences the dog and\\nthe dog notice that system 1 has two sentences and\\nit carries two pieces of information. System 2 also has two sentences,\\nbut they're the same. So this system carries only one piece of\\ninformation and system 3, well, it has two sentences, but it carries no information\\nregarding the color of the animals. So it carries zero pieces of information, recall that your goal is\\nto determine the color. So the amount of information a system of\\nsentences carries is defined as the rank of the system. The system 1 has ranked 2, system 2 has\\nranked 1 and system 3 has ranked 0. Next you'll see how this\\nnotion applies to matrices and this corresponding systems\\nof linear equations. Now, let's go back to the three systems\\nof equations from the previous videos. As you've already seen,\\nthe first system has two equations and each equation brings\\nsomething new to the table. Some new piece of information. That's why you're able to narrow\\ndown the solutions to one point. The first equation narrows down to a line\\nand the second one narrows them down to a point that the system has\\ntwo pieces of information. This is how the rank of\\nthe system is defined. So the rank is 2,\\nthe second system has two equations, but recall that the second equation\\nwas the same as the first one. Therefore the system really only\\ncarries one piece of information which is the first equation. This is why you're able to narrow\\ndown the set of solutions to a line, but that's as far as you can get. Thus, the rank of the system\\nis defined as one, and finally the third system\\nhas two equations. But they carry no information as any\\nnumber a and b satisfy these equations. Does the system carries zero\\npieces of information and its rank is defined to be 0. And now on to define the rank of matrix, since its system of information has\\na corresponding matrix and the rank of the matrix is defined as the rank of\\nthe corresponding system of equations. Thus the matrix corresponding to\\nthe first system has ranked 2. The one corresponding to the second\\nsystem has ranked 1, and the one corresponding to\\nthe third system has ranked 0. Now there's a special relationship\\nbetween the rank of a matrix and its solution space. Recall that the solution space for\\neach of these matrices is the set of solutions to the system of equations\\nwhen the constants are zero. So for the first one recall that\\nthe solutions are only a=0 and b=0. So that's a point and the dimensional solution space is 0\\nbecause the dimension of a point is 0. For the second one, the set of solutions\\nwas some line and a line has dimension 1. So the dimension of\\nthe solution space was 1. And for the third one, well every a and b works here because any point a and\\nb is a solution to that system. Therefore the solution space is\\na plane and it has dimension 2. So what happens here in all three\\ncases is that the rank is equal to 2. The number of rows in the matrix minus\\nthe dimension of the solution space. This is always the case for 2 by 2 matrix. And in general, as you'll see later,\\nthis rank and solution space always and then dimensional solution always add\\nto the number of rows in the matrix. Notice also the first\\nmatrix is non-singular and the other two are singular. So a matrix is non-singular if and\\nonly if it has full rank, namely if the rank is equal\\nto the number of rows. This is the same as saying that a system\\nof equations is non-singular if it carries as many pieces of\\ninformation as equations it has, meaning that you carry the maximum\\namount of information possible. Thus that this equation brings a new\\npiece of information to the table and there's no redundancy between equations. And now you're ready for a quiz for\\nthis quiz, please determine the rank of the same two matrices you've seen\\nrecently and the solutions are in. So since the first one has a solution\\nspace of dimension 0, the rank is 2, and the second one has a solution space\\nof dimension 1, so the rank is 1. Notice that the first\\nmatrix is non-singular and the second one is singular.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"Now you're ready to learn\\na very famous and classic algorithm called Gaussian elimination. You will see that this is just the\\nelimination method you studied before, but restructured to solve a\\nsystem of equations and formalized so that it can be followed by\\nhand or implemented in code. Recall that when you learn\\nabout the singularity of a matrix, you ignore the constant values on\\nthe right hand side of your equations. Those equations have\\nconstants 1, minus 2, and minus 1, but you treated them as\\nthough they were just 0. To actually solve a system of equations,\\nyou will now need to pay attention to them. Here's how it's done. To begin, make a matrix\\nfrom the coefficients in your system of equations,\\njust as you always have. Now add another column\\nto the right side of your matrix which holds the constant values 1, minus 2, and minus 1. This is\\ncalled the augmented matrix. The vertical line is used\\nto separate the constants, so you remember they're\\nnot part of the variables. And now if you proceed with the\\nelimination method as normal, you can use the augmented matrix\\nto solve your system of equations. Recall that to complete the\\nelimination method you'll repeatedly find an element called a pivot\\non the diagonal of the matrix. To start, you'll select the\\ntop left cell as your pivot. Your first task will be to use row\\noperations to set your pivot to 1. Next, you'll use row operations to set\\nall the values below your pivot to 0. You then repeat the process\\nrow by row using row operations to simplify the matrix down to\\nthe reduced row echelon form. Whatever row operations you perform\\non the matrix will also be applied to the column of constants you\\nincluded to form the augmented matrix. As you see they'll eventually help\\nus solve the system of equations. So this is the high level overview, now\\nlet's see how this process actually works. To start, you'll need to turn the\\npivot from the first row into a 1. Since the pivot is currently at\\n2, multiply the row by one half. After the row\\noperation, the pivot is now a 1, and every other value in\\nthe row is also divided by 2. Replace r1 with the\\nupdated row you just calculated. Notice that even the\\nconstant on the right hand side was also changed and is now one half. Remember that this matrix represents\\nthe system of equations you started with, so let me also update the system of\\nequations to match what's in the matrix. Remember that next you\\nwant to set all the values below the pivot to 0 using row operations. I'll start with this 2 in row 2. This is the row operation I'll use to\\nupdate row 2 and hopefully you can see why. The pivot is 1 and the\\nvalue you want to cancel is 2. If you subtract 2\\ntimes row 1 from this row 2 you will be left with a 0 below\\nthe pivot, just like you wanted. Completing this row operation will\\nleave you with a new row 0, 3, 3, minus 3, which I'll use to\\nupdate row 2 in a moment. Next you'll need to\\ncancel out these 4 in row 3. Again, since the pivot is a 1, it's straightforward to choose\\nthe row operation to do this. You can just subtract 4 times row 1\\nfrom row 3 to cancel out the pivot. Completing this row operation results\\nin this new set of values for row 3. Great! Now the first column is done. I'll update the system of equations\\nto reflect the changes in the matrix. Notice that the first column\\nis a 1 followed by two 0s. So, you're ready to move on to the\\nsecond column and the second pivot. Moving along the diagonal,\\nyour new pivot is this 3. Just as before, you need\\nto set the pivot to 1, and then set the values\\nbelow your pivot to 0. Since the new pivot is a 3, multiply\\nthe row by 1 third to set the pivot to 1. This works out as follows. And once again, update\\nthe system of equations. Now you'll need to turn the second\\nelement of the third row into 0. Once again, it's easy to come\\nup with a new row operation to do this since your\\npivot is already a 1. Subtracting 3 times row 2 from row 3 will\\ncancel out the 3 and leave you with a 0. Completing this row operation will\\ngive you the new row, 0, 0, minus 5, 0. Once again, I'll update\\nthe system of equations to match the new values in the matrix. You're almost done! Make the minus 5 along the\\ndiagonal your final pivot. As before, you will need to set the pivot\\nto 1, so divide the third row by minus 5. The final values for row 3 will\\ntherefore be 0, 0, 1, 0. And once again, update\\nthe system of equations. Note that the matrix is in\\nrow echelon form. The diagonal is all ones and below\\nthe diagonal there's only zeros. The row operations performing\\nthe matrix have also changed the values of the constants\\nand this is where you'll use the information in that column\\nto actually solve the system of equations through a\\nprocess called back substitution. Here's what back substitution looks like. You will start from the bottom\\nrow and work your way to the top. You'll use the pivot from each row to\\ncancel the values in the cells above it. This process actually looks very similar\\nto creating the pivots in the first place. So start with the pivot in the last row\\nand start by cancelling out the 1 above it. The pivot is a 1 and you\\nneed to cancel out a one, so the row operation will\\nbe row two minus row three. After this row\\noperation, the new values for row two will therefore be\\n0, 1, 0, minus one. Next, cancel out this one half in row one. The row operation you'll need this time\\nis row one minus one half times row three. Completing this row operation gives new\\nvalues for row 1 of 1 minus 1 half, 0 and 1 half. Now you get the following matrix\\nwith this updated system of equations. Finally repeat the process\\nand use the pivot in the second row to create\\nzeros in the rows above. In this case you will add 1\\nhalf of row 2 to row 1 as follows. And you're done, you get\\nthis matrix over here. Notice that you've got a\\nmatrix that has all 1s in the diagonal and 0s\\non all other positions. I will update the system of equations\\none more time to represent the matrix, and you see that you now have a\\nsolution to the system where a equals 0, b equals minus one, and c equals 0. Note that the square part of the\\naugmented matrix has only 1s in the diagonal. Such a matrix is\\ncalled the identity matrix, and by simplifying the matrix to\\nthis form using Gaussian elimination, you have solved the\\noriginal system of equations. Now let's talk about the singular case. Will Gaussian elimination\\nwork if the matrix is singular? Well you already know that if the\\nmatrix is singular then in reduced row echelon form you will\\nhave a row that is all zeros. There's no need to worry, once you\\nget here the algorithm just stops. The whole point of Gaussian elimination is to find solutions to a\\nsystem of equations. If you find a row of zeros however, you know your matrix is\\nsingular and there is no solution. That said, you can still\\ndetermine if your matrix is contradictory and has no solutions or if it\\nhas infinitely many solutions. To do that you just need to\\nlook at the column of constants. If the constant value in the\\nrow of zeros is also zero, the row just says zero a plus\\nzero b plus zero c equals zero. No matter what values you choose for a, b\\nand c, the left side will always equal 0, and this equation will be true. So this system has\\ninfinitely many solutions. What if the system of\\nequations changed slightly so that the third equation equals 10? Well, after row reduction you\\nget the following matrix with the constant value in the\\nthird row changing to a 4. Now the last row states that\\n0a plus 0b plus 0c equals 4. No matter what values of\\na, b and c you choose, the left side of this equation will\\nequal 0 but the right side equals 4. This means that the system\\nhas no possible solutions. To recap, if you find a full\\nrow of zeros in row-eixial form and the constant in that row is zero, then the system has infinitely many\\nsolutions. If that constant is not zero, however, the system has no solutions. And here's a recap of the whole\\nprocess of Gaussian elimination. You first create the\\naugmented matrix by adding the constants to a new\\ncolumn on the right. Next, get the matrix in the\\nreduced row-echelon form. And finally, complete back\\nsubstitution to find the values on the right. And finally, complete back substitution\\nto find the values of your variables. If you ever encounter a row of\\nzeros, stop as the system is singular. You'll have more opportunities to\\nstudy this in the coming assignment. Okay, that brings us to the end\\nof the lecture for this week. After this, you'll have a graded\\nquiz and a programming assignment. The quiz covers all the\\ntopics you learned this week. The quiz covers all the\\ntopics you learned this week. The programming assignment is\\ndesigned to guide you through actually implementing the\\nGaussian elimination algorithm. Think of this assignment as a capstone to\\neverything you've learned about systems of equations and how to solve them\\nthrough the first half of the course. It's also a great opportunity to hone\\nyour skills with the NumPy library, which you'll be using in the coming weeks. This is definitely the most\\nmath focused assignment in the course and it helps\\nprepare you for the machine learning focused\\napplications of these concepts and skills awaiting you\\nin weeks three and four. Good luck!\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"You have arrived at\\nthe end of Week 2, and you have a lot\\nto be proud of. This week, you build from your Week 1\\nunderstanding of solving a two-by-two and\\nthree-by-three systems of linear equations to zooming into the properties of matrices, you can now comfortably solve systems of equations\\nby translating into matrices and conducting\\nrow operations to find a solution in a\\nsimple yet fast way. Equivalently, you learn\\nhow to turn matrices into the row echelon forms and\\ntheir further simplification, the row reduced echelon form. From here, you learn how to extract all the possible\\ninformation of a matrix, including the singularity\\nor non singularity, the determinant, the rank, the solutions to its corresponding\\nsystem of equations, and its graphical\\nrepresentation. We've covered quite a\\nfew concept this week. Follow along with me for an exciting journey into Week 3, where you'll learn about\\nvectors, dot-product, matrix multiplication, linear transformations,\\nand much more.\",\n",
       "  'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'),\n",
       " (\"A fundamental concept of linear\\nalgebra is the concept of a basis. You've actually seen basis many\\ntimes through this course in several different ways, but this week you\\nlearn how to identify them and use them to your advantage. So what do we mean by a basis? In the previous week, you learn that\\na matrix can be seen as a linear transformation from the plane\\nto the plane, which sends this fundamental square into this\\nparallelogram and both were called basis. Now, why are they called basis? Well, in reality, all that matters here\\nis not the four points of the square of the parallelogram, but\\nthe two vectors that define it. Namely those two vectors\\ncoming from the origin. Those are the ones that are referred\\nfrom now on as a basis. The main property of a basis is that\\nevery point in the space can be expressed as a linear combination\\nof elements in the basis. What do I mean by that? Well, let's say we have\\nthese two vectors and I'm telling you that those\\ntwo are a basis, why? Well, pick any point in space,\\nsay this one. Can we get to that point only walking in\\nthe two directions defined by the basis? Well, of course we do. We can do it this way or\\nthis way or this other way. There are many ways to do this. Actually, there's infinitely many since\\nyou don't have to take unit steps, you can walk in tiny\\nfractions of steps and you can even walk backwards in\\nthat direction if you want. So this is one of the possible basis for\\nthe plane. Can you help me think of some more? Well, there are lots and lots,\\nfor example, look at this basis. I could get to any point, for example,\\nthis point using those two directions. Here's one way, I walk in this way,\\nand then in this one. Remember that I can walk\\nbackwards if I want to. So those two vectors also form a basis. Do these two vectors form basis? Yes, they do, I can walk like this and\\nthen like this to get to the point. So as a matter of fact,\\npretty much any two vectors form a basis. Now it's almost hard to think of what's\\nnot a basis, what would be a non-basis. So here's an example,\\nlet's take the two vectors over here. These two vectors,\\nI can arrive for example, to this point over here, but\\nI could not arrive to this point over here by walking in those two directions\\nbecause I only have one direction. So I really can only cover that line,\\nI cannot cover the entire plane. So, anything that comprises two vectors\\nthat go in the same direction and they could be opposites or\\nthey could go in the same direction, but as long as they belong to the same line,\\nthe two vectors do not form a basis. So that is what a basis is and\\nwhat a non-basis is\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you have understood\\nthe idea behind PCA, let's formalize\\nthe steps one last time using all the different\\nformulas you've studied. In this example, there\\nwill be five variables, but it works the\\nsame for any number. You start with a data\\nset of points with n observations of\\nfive variables, x_1, x_2, x_3, x_4, and x_5. Your goal will be to reduce your data from 5-2 dimensions. First, construct a\\nmatrix of your data. This matrix will\\nhave five columns, one for each\\nvariable or feature, and n rows, one for each observation. It will be called X. This is the same matrix you used earlier to get the\\ncovariance matrix, which at that point\\nwas called A. We are using X to\\ncall this matrix because now all the\\nvariables are called x_i. Also, this is more\\nstandard notation. Next, center your data. To do this, calculate the column averages and\\nsubtract them from each column, giving you the\\nmatrix X minus mu. Next, calculate the\\ncovariance matrix. This is just a simple\\nmatrix multiplication using the matrix X minus mu\\nyou just calculated. In this case, you end up with a five-by-five matrix of covariances between\\neach pair of variables. Next, find the eigenvalues and eigenvectors for the\\ncovariance matrix. Once you have them, sort them from smallest to\\nlargest by their eigenvalues. Now you'll create a matrix\\nto project your data. Since your goal is to reduce your dataset to\\njust two variables, you will only keep\\nthe first and second eigenvalue, eigenvector pairs. Create a matrix V, which has two columns where each is one of the\\neigenvectors you choose, scaled by its norm. Finally, project the data onto\\nthe vectors you choose by multiplying the center data\\nby your projection matrix. The result is a\\nnew dataset, XPCA, which will have just two columns of data representing\\nthe projection of your original dataset onto the two principal\\ncomponents you choose. That's it. This is the step-by-step\\noperations you need to perform PCA for dimensionality\\nreduction of your data.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Let's look a bit more closely why\\neigenvalues and eigenvectors are so special. I'm going to multiply the matrix 2,\\n1, 0, 3, by the two vectors that are already shown\\nyou are eigenvectors and one that isn't. The first vector 1, 0 is an eigenvector. After the transformation it becomes 2, 0. Notice that the vector is still\\npointing in the same direction. If I wanted,\\nI could write the result out like this. 2 times the original vector 1, 0. The second vector 1,\\n1 is also an eigenvector. After the transformation it becomes 3, 3. Again, it's pointing in\\nthe same direction and so I can rewrite the result as\\n3 times the original vector. The final vector -1,\\n2 is not an eigenvector. After the transformation\\nit becomes the vector 0, 6. Notice that it's no longer pointing\\nin the same direction and there is no constant that I can multiply the original\\nvector by to get the final vector 0, 6. Let me formalize what these\\ntwo equations are saying. I'll call our matrix here A and\\nI'll call the first eigenvector here v1. Then this equation says that for\\nthis special vector v1 A times v1 is the same as multiplying\\nv1 by the scalar lambda 1. I'm using more formal notation but\\nin this case, you already know v1 is the vector 1,\\n0 and lambda 1 is the scalar 2. The second equation is nearly identical. It says that A multiplied by v2 is equal\\nto the scalar lambda 2 multiplied by v2. You already know that the vector v2 is 1,\\n1 and the scalar is 3. v1 and\\nv2 are the matrix A's eigenvectors and lambda 1 and\\nlambda 2 are the matrix a's eigenvalues. Notice that eigenvectors and\\neigenvalues come in pairs. So, v1 and lambda 1 form a pair, and\\nv2 and lambda 2 form a second pair. You may still be wondering why\\neigenvectors are so special. Let's look more closely at\\nwhat this equation is saying. On the left side of these equations\\nyou have a matrix multiplication, and on the right side you\\nhave a scalar multiplication. And one big difference here is that matrix\\nmultiplication is just a lot more work. Even for this small two by two example, calculating the left side of\\nthe equation requires 8 multiplications, while the right side only\\ntakes 2 multiplications. For matrices with hundreds or thousands of columns,\\nthe difference becomes astronomical. What this equation is saying then is that,\\nat least along a matrix eigenvectors, you can turn a large\\ncomputation into a smaller one. And if you apply what\\nyou know about bases, you can actually use\\nthis shortcut everywhere. Let me show you how. This is the example I showed you earlier. The red vector is not an eigenvector. So to find out where it was sent, you needed to just complete\\nthe matrix multiplication as normal. Notice however that the two\\neigenvectors 1, 0 and 1, 1 are linearly independent and\\nspan the plane, so they form a basis. This is actually the eigenbasis\\nof the matrix 2, 1, 0, 3. I will use this to rewrite the equation. Since the vectors 1, 0 and 1,\\n1 form a basis, then you know that you can write the vector -1, 2 as a linear\\ncombination of those basis vectors. In this case it's -3 times 1,\\n0 + 2 times 1,1. So you can simply replace, -1,\\n2 by- 3 times 1, 0 + 2 times 1,1. This is the same vector- 1,\\n2 you had before, but rewritten as a linear combination\\nof the vectors in your basis. Now I'll rearrange the equation slightly. All I did was move the matrix\\nmultiplication inside the brackets and move the -3 and the 2 forward. And at this point, you are ready\\nto use your eigenvector shortcut. Look up here, you have already calculated\\nthese matrix multiplications and you can just substitute the scalar\\nmultiplications like this. Now your equation,\\nis just scalar multiplications. You have -3 times 2 times the vector 1,\\n0, + 2 times 3 times the vector 1, 1. You can quickly multiply\\nthrough these scalars, and finally solve this\\nequation to get the vector 0, 6, which if you remember,\\nis the vector you solved for before. Only this time, even though the red\\nvector is an eigenvector you were able to solve this linear transformation without\\ndoing any matrix multiplication. You just used scalar\\nmultiplication the entire way. Okay, I need to admit I\\nlied a little bit here. Remember earlier, where I just gave you\\nthe red vectors coordinates with respect to the eigenbasis -3, 2. Perhaps you could have eyeballed that\\nanswer in this simple case, but in general, there's actually substantial work\\ninvolved in getting those coordinates. I haven't taught you why this is the case,\\nbut to do this, for every point in the plane you'd\\nneed to calculate the inverse of your eigenbasis and then multiply\\nthat inverse by the red vector. Or in other words kind of a lot of\\nwork and even a matrix multiplication. So at least in this example a more\\naccurate way of describing eigenvectors is not that they remove all the work, but they let you decide when\\nyou want to do the work. In some machine learning contexts it may\\nbe worth to calculate these coordinates ahead of time, so that when it's time to\\napply a transformation you can do it much more quickly. You'll see other powerful applications\\nof eigenvectors in this course as well. For now here are the main takeaways\\nabout eigenvalues and eigenvectors. They are characterized by\\nthe equation Av equals lambda v for each pair of eigenvectors and eigenvalues. As you just saw, this means that along\\nthat vector a matrix multiplication just becomes a scalar multiplication. Visually you can think of eigenvectors\\nas telling you the direction in which a linear transformation\\nis just a stretch, and the eigenvalues tell you\\nhow much it is stretched. You can create a basis from\\nthe eigenvectors called the eigenbasis. You will often see the eigenbasis\\nrepresented as a matrix with one eigenvector in each column. From the perspective of this eigenbasis\\nthe linear transformation A is just a collection of stretches. And as you'll see,\\neigenvectors can save work and help characterize a linear\\ntransformation in powerful ways.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you know what a\\nbasis of the plane is, the following question arises. Can something not be\\na basis of the plane but still be a basis\\nfor some other space? What exactly\\nconstitutes a basis? For this let me introduce\\nthe concept of a span. The span of a set of vectors is simply the\\nset of points that can be reached by walking in the direction of these\\nvectors in any combination. For example, you've seen that the span of these two\\nvectors is the plane, as you can get to any point in the plane by walking in\\nthese two directions. Likewise the span of these two\\nvectors is also the plane. It may be a while to\\nget to many points, but one can using only\\nthese two directions. These two vectors however\\ndon't span the plane. Because as you saw before, not every point can be reached by walking in these\\ntwo directions, they're the same direction. What side do they span? Well any point in this\\nline can be reached by walking in the directions\\nof the vectors, so the span of these two\\nvectors is that line. What about these two that are at an angle of 180 degree apart? Well the span is again the\\nline that contains them, because you can get\\nto every point on that line by walking on\\nthese two directions. What about this one vector? What's the span of\\nthat one vector? Well the span is the line that contains it\\nand goes to the origin, because that's the set of points that you can reach by\\nwalking in that direction. Here's a question.\\nDo these two vectors form a basis of this line? What do you think? The\\nanswer is actually no. The reason is that\\na basis needs to be a minimal spanning set. Here there are too many vectors. See any one of those two\\nvectors span the line, so the two are one too many. A basis is a minimal\\nspanning set, so each one of them separately\\nis a basis of that line. But the two of them\\nare not a basis. They're a spanning set, but they're not a basis. Now is this a basis\\nof something? Yep, that is a\\nbasis of this line. As a matter of fact, any vector that starts at the\\norigin and goes in that same direction is also\\na basis for that line. In a summary, a basis is\\na minimal spanning set, so the vector on the left\\nforms a basis of this line. But these two vectors don't, because they are too many. This happens also for higher dimensions.\\nLet's look at this set. This set forms a basis of the plane because\\nit spans the plane, because we can get to any point by walking in\\nthose two directions. But if we remove any of them, they don't span\\nthe plane anymore, now they just span one line. However this set of vectors over here do not form a\\nbasis of the plane. See, they span the plane because any point\\nin the plane can be reached by walking\\nin some combination of these three directions. But they're too\\nbig to be a basis. Any subset of two of these\\nthree vectors is a basis, but the third one is redundant, so that's not a base. Now notice something\\ninteresting, and is that the\\nlength of the basis, is the space of the\\ndimension of the plane. Now notice something\\ninteresting, which is that the\\nlength of the basis of a space is the\\ndimension of that space. On the left the line has\\na basis of length 1, and it has a dimension of 1, because line has dimension 1. On the right the plane\\nhas a basis of length 2, formed by two vectors, and the plane has\\na dimension of 2. This implies that any basis of any space actually has the same number of elements\\nas any other base. I encourage you to take a look at for example a three\\ndimensional space, and imagine how would\\na basis look there. Now that you have a good\\nintuition of what basis are, let's see the formal definition. To do that we'll need to\\nintroduce the concept of linearly independent and\\nlinearly dependent vectors. A group of vectors is said to be linearly independent if none of the vectors in the group\\ncan be obtained as a linear combination\\nof the others. If you consider just one\\nvector in the plane, it is always going to be\\nlinearly independent. Now let's add another vector. Notice that there is\\nno way you can get the new vector as a linear\\ncombination of the first one, simply because they point\\nin different directions. These two vectors are still\\nlinearly independent. What if you added this\\nother vector instead? In this case the new\\nvector in red is pointing in the same\\ndirection as the green one, but it's simply twice as\\nlong as the original vector. Since one vector can be obtained as a linear combination\\nof the others, this set of vectors is\\ncalled linearly dependent. Notice also that even though we added a new vector to our set, the span of these\\nvectors did not change, it remained a straight line. Now let's look at a\\ndifferent example. This time we'll keep the\\norange and green vectors that were linearly independent, and we'll add a\\nthird one in red. While none of these two vectors is a multiple of any other one, they are no longer independent. That is because you can obtain\\nthe third vector by adding one times the green vector to three times the\\norange vector. I give away that\\nthis is the case, is that once again we added\\na new vector to our set, but the span of the\\nvectors did not change, they still only span the plane. It turns out that any other\\nvector you add can be written as a linear combination\\nof the first two. This result will\\nhold in general. If you have more vectors and dimension of the space\\nyou're trying to span, you will always have a\\nlinearly dependent group. This means having three or\\nmore vectors in the plane, or four or more vectors\\nin 3D space, and so on. Let's see how we can check if a set is linearly\\ndependent or not. Let's use the example\\nfrom the last slide. Let's call the orange vector, v_1, with coordinates -1,1. V_2 will be the teal vector, which has coordinates 2,1. Finally, v_3 will be the red vector which\\ncoordinates -5,3. To see that the set\\nof vector is linearly dependent you need\\nto find constants Alpha and Beta that\\nsatisfy that Alpha times v_1 plus Beta times\\nv_2 equals v_3. In other words, you\\nare looking for the coefficients of\\nthe linear combination that gives you v_3. Alpha and Beta are\\nactually numbers. This gives place to a set of two equations\\nwith two unknowns, minus Alpha plus\\n2 Beta equals -5, and Alpha plus Beta equals 3. In other words we have a\\nsystem of equations which you've already learned to\\nsolve earlier in this course. Let's review one\\napproach you could take. Let's call them equations\\n1 and 2 respectively. If you sum equations 1 and 2, you get that 3 Beta equals -2. Which means that\\nBeta equals -2/3. Using this result in equation 2, you get that Alpha\\nminus 2/3 equals 3. Alpha is 11/3. Since you could find a solution for the system of equations, then v_3 is a linear\\ncombination of v_1 and v_2, and this set is\\nlinearly dependent. If you were to find that\\nthe system has no solution, then that means the set\\nis linearly independent. Now let's do a quiz. I'm going to give you three\\nvectors and you tell me if these vectors are\\nlinearly independent or not. Actually they are not, because the first\\nvector times 1 plus the second vector times -1\\nequals that third vector. In fact they are\\nlinearly dependent. However if you were to remove\\nany of these three vectors, then you get a linearly\\nindependent set. But since each linear\\nindependent set only has two vectors and they live in\\nthree-dimensional space, because they have\\nthree coordinates, none of this is a basis for\\nthe three-dimensional space. Geometrically would mean\\nthat if you were to plot these three vectors in\\nthree-dimensional space, they will all lie\\ninside the same plane. Now that you've seen some examples of\\nlinear independence, let's come back to the formal\\ndefinition of a basis. A basis is a set of vectors that satisfies two conditions. The set must span\\na vector space, and the vectors in that set\\nmust be linearly independent. Coming back to the\\nexamples you saw earlier, these first two span a line\\nor a one-dimensional space, and a plane or a\\ntwo-dimensional space, and they are linearly\\nindependent, so they form a basis. The last two are\\nlinearly dependent, and they do not form a basis. As we see in these two examples, we need to be careful to\\nremember that not all sets of n vectors will form a basis\\nfor an n-dimensional space. Hopefully you're\\nstarting to feel more confident about the\\nconcept of span. Next, you'll get a chance to use a new interactive tool that will help you\\nvisualize the span of vectors in\\nthree-dimensional space. As always, there are some\\nsuggested activities provided, but you're more than\\nwelcome to just explore the tool on\\nyour own as well.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you know\\nwhat a basis is, you should know that some basis are more\\nuseful than others. In particular,\\nthere's one basis to rule them all called\\nthe eigenbasis. The eigenbasis is\\ntremendously useful, in particular for machine\\nlearning applications such as principal component analysis,\\nas we mentioned before. Here's how eigenbasis work. Let's look at the\\nlinear transformation corresponding to the\\nmatrix with entries 2, 1, 0, and three. We're going to see how it acts with respect to the\\nfundamental basis. This basis has two vectors, the vector 1, 0, which when you\\nmultiply by the matrix, you see that it\\ngets the vector 2, 0 and the vector 0, 1, which when you put\\nit in the product, you get the vector 1, 3. This square on the left goes to this parallelogram on the right and the rest of\\nthe plane follows. This is also called a change of coordinates or a change of basis because that's\\nexactly what you're doing. You're going from the\\nsquare coordinates in the left to the parallelogram\\ncoordinates in the right. But that's something\\nyou've already seen. Now let's get to something\\nmore interesting. The choice of the square\\nwas very arbitrary. Let's actually pick\\na different basis and see what happens. Let's pick again the vector 1, 0 that goes to the vector 2, 0. As a second element\\nof that basis, let's pick this vector 1, 1 just to see what happens. That goes to the vector 3, 3. This parallelogram goes\\nto this parallelogram. What is special here? Notice that the sides of the two parallelograms\\nare parallel to the corresponding\\none in the other basis. That is very special. Notice that the rest of\\nthe plane also follows. Because these sites\\nare parallel, then what we're doing\\nto the plane is we're stretching by\\ntwo in this direction, the horizontal direction, and stretching by three in\\nthis diagonal direction. This is a very special basis. It only consists of\\ntwo stretchings. That is what's called\\nan eigenbasis. It's a very special way of looking at a linear\\ntransformation with respect to a basis that sends a parallelogram to\\nanother parallelogram with sides parallel\\nto the original one. Why is that useful? Let's say that you want to find the image of the point 3, 2. You can multiply it by\\nthe matrix and get 8, 6, which is over here. But you can also\\nexpress that point as a combination of elements in the basis. What\\ndoes that mean? That we find a path\\nto get to that point using the two directions\\nthat we have. Now the linear transformations\\ncorresponds to stretching the\\nhorizontal vector by two and the vertical\\nvector by three. This really simplifies the\\nlinear transformation. It just consists of\\ntwo stretchings. The two vectors in the\\nbasis are gonna be called the eigenvectors\\nand the stretching factor, the two and the three, are going to be\\ncalled eigenvalues. Eigenvalues and eigenvectors are\\ntremendously important in linear algebra because they really simplify\\nour calculations.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Congratulations on making it to the last assignment\\nof this course. In this video, I want to talk about one final\\nconcept that will be helpful on that assignment,\\ndiscrete dynamical systems. On the assignment,\\nyou will learn to use this approach to model how\\npeople navigate the Internet. As you'll see, it's a fairly straightforward\\napplication of eigenvectors. Suppose today is sunny, you want to know what\\nare the chances that tomorrow will be sunny,\\ncloudy, or rainy? Suppose that if today is sunny, you have a 0.8 probability\\nof tomorrow being sunny, 0.15 of it being cloudy, and 0.05 of it being rainy. What about if today is cloudy? Then the probabilities\\nmight change to 0.45 of tomorrow being sunny, 0.35 of it remaining cloudy, and 0.2 of it rainy. Finally, if today is rainy, then the chances of tomorrow\\nbeing sunny is 0.3, cloudy is 0.4, and rainy is 0.3. You might have noticed that\\nthese values are positive, although they could\\nalso be zero, and the columns add to one. if square matrix has\\nthese properties, we say it's a Markov matrix. As you'll see in a moment, Markov matrices are\\nimportant because they allow you to infer\\nthe probability of how your system will evolve. Imagine that today is cloudy. You can represent that\\nwith the vector 0, 1, 0 because, with 100% certainty, you know that it is\\ncloudy right now. This is called the state vector because it represents\\nthe state of the system. Since this is the first you're considering, I'll call it X_0. To know the probabilities of tomorrow being sunny,\\ncloudy, or rainy, you simply take the dot\\nproduct between the matrix and the vector to get\\nthe new state vector, 0.45, 0.35, and 0.2. Notice that this is just the\\nsecond column of the matrix, the one corresponding\\nto a cloudy day, and this is the prediction of\\nthe weather one day ahead. Now, say you want the prediction\\nfor the following day, so you take the dot product\\nbetween the matrix and the new state vector\\nX_1 to get X_2, which is the probability that the day after tomorrow will\\nbe sunny, cloudy, or rainy. Knowing that today is cloudy, X_2 has values 0.5575, 0.27, and 0.01525. You can keep going\\nwith this procedure to find the prediction\\nfor three days ahead. This one has a probability\\nof 0.6293 of being sunny, 0.2421 of being cloudy, and 0.1286 of being rainy. Now, repeat this\\nprocess again to find the predictions for\\nfour days ahead, for five days ahead. Note that the change\\nbetween states is getting smaller and smaller. For six days ahead, you now have the\\nfirst two decimals constant between X_5 and X_6. We can keep going\\nand going and going. Notice what is happening. The difference between\\nconsecutive state vectors is now on the fourth decimal. Let's do one more iteration. What happens between\\nX_10 and X_11? Well, it turns out\\nthat they're the same up to these four decimals. As you can see, this\\nprocess tends to stabilize. Essentially, you are saying that the matrix dot product\\nof the vector X_10, which is 0.6665, 0.2223, and 0.1112 is one times essentially\\nthe same vector. This means that X_10 is\\nessentially an eigenvector. Now, there's a small difference\\nbetween X_10 and X_11, but imagine going\\npretty far along. As a matter of fact,\\nimagine taking the limit as n goes\\nto infinity of X_n, then we definitely\\nhave an eigenvector, and one is the eigenvalue\\nassociated with it. You might have noticed that once you reach this vector state, then you can never move\\nanywhere else since it's an eigenvector\\nwith eigenvalue one. This matrix is usually called\\nthe transition matrix. Remember that for\\nthis to be valid, the transition matrix needs\\nto be a Markov matrix with each column made up of non-negative values\\nthat add up to one. This matrix is usually called\\nthe transition matrix. Remember that for\\nthis to be valid, the transition matrix needs\\nto be a Markov matrix with each column made up of non-negative values\\nthat add up to one. This vector is called the\\nequilibrium vector and it gives you the long-run probabilities\\nthat on a given day, it will be sunny,\\ncloudy, or rainy. The cool thing about\\nit is that no matter what your initial state was, in the limit at infinity, you will get to this\\nequilibrium state. As you've seen, it is also an eigenvector of the\\ntransition matrix. Now, you should have a\\nbetter understanding of discrete dynamical\\nsystems and the role that eigenvalues and eigenvectors play in\\nunderstanding their behavior. You are now ready to give it a go with the\\nprogramming assignment. This brings us to the end\\nof the lecture for Week 4. After this, you have a\\nfew final activities. There is a graded quiz that covers all the topics in Week 4. You'll also find this week's\\nprogramming assignment, which is a fascinating\\nlook into how eigenvalues and eigenvectors\\nare used in practice. You'll see them used to model traffic between web\\npages and implement PCA to reduce the dimensionality of a data set of cat images. I think these are\\nsome of the most fascinating topics\\nin this course, and a great final challenge that will require you to apply\\neverything you've learned. To help you prepare\\nfor the assignment, you will also find a\\nshort, ungraded lab. Best of luck. I'll see\\nyou once you're done.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"The final major topic of this course will\\nbe principal component analysis, or PCA. The goal of PCA is to reduce the\\ndimensions or number of columns of a data set while preserving as much\\ninformation as possible. Put simply, PCA takes a large table or\\ndata set and converts it into a smaller one. Your original data set has many rows or\\nand many columns or features that store useful\\ninformation about each observation. PCA will reduce the number\\nof features in your table while maintaining the same\\nnumber of observations. Put another way, the data set will have\\nthe same number of rows and fewer columns. It will be just as tall,\\nbut it will get skinnier. Here's an example dataset collected by\\nan online store about its customers. This table has four observations and\\nfive features. Customer age, account age days since\\nlast login, total purchases and total money spent there\\nare two primary reasons you may want to reduce\\nthe dimensions of a data set. The first is just that\\nit's literally too big and you want to work with something smaller. In this example,\\nyou only have five features, but in some machine learning contexts you could easily\\nhave hundreds or thousands of features. It is useful to be able to shrink\\ndatasets to a more manageable size. The second is to help with visualization,\\nespecially in exploratory analysis. Many common charts, like a scatter plot or\\nbar charts, really only help you visualize your data when looking at one or\\ntwo features at a time. Reducing the number of dimensions or\\ncolumns you need to consider makes it easier to quickly visualize your data and\\nlook for patterns. So you want to reduce\\nyour table's dimensions. And the question is, how should you do it? Well, here's one easy approach. Just start deleting columns. You could easily delete the last two\\nthat contain the total purchases and total amount spent by each customer. This data set is looking\\neasier to use already. Unfortunately, however, you've also\\ndeleted a lot of useful information. Your data set has fewer dimensions, but any insight you could have gained\\nfrom those two columns is lost. PCA is designed to address this problem. It will allow you to still reduce\\nthe dimensions of your data, but it also preserves much of the information\\nyou might lose if you simply started deleting columns. The idea behind dimensionality reduction\\nis to move your data points into a vector space with fewer dimensions. This is called a projection. You already have studied all those\\nconcepts underlying projections, so let me show you how they\\nwork with an example. Suppose you have this table of data\\nwhere you have four observations of variables x and y, which on\\nthe plane looks something like this. Now imagine you want to move or project your data onto this\\nline which has equation y = x. All the points move perpendicularly\\ntowards the line, except the 1, 1 which was already on the line. But where do these points end up? I'll start with the easiest example,\\nthe point 1, 1 which didn't move at all. Before I would have given this\\npoint location as 1, 1 but those are actually two\\ndimensional coordinates. I can actually now give this\\npoint's location as one coordinate its distance along the line\\nfrom origin, that is, this distance here. From some basic trigonometry you can\\ntell that the length of this segment is the square root of 2. With just this number you could find\\nthe location of the point along the line. It might not seem super clear to you why\\nyou would want to do this, but notice that square root of 2 can be rewritten as\\n1 + 1 divided by the square root of 2. Here the coordinates of the original\\npoint are starting to show up. Let's see why this happens. Let's start by trying to get that 1 + 1. This will come from the dot product of the\\ncoordinates of the first point with some other vector shown here in orange. To choose that orange vector note\\nthat the line y equals x is actually the span of the vector with coordinates 1,\\n1. So let's use that vector 1, 1. Now if you take the dot product of that\\nfirst row in the table and the orange vector, you're essentially saying take\\n1 times the point x coordinate and one times the point y cordinate to find\\nthe new location projected along the line. However, summing the two variables gives\\nyou a longer vector than expected. This vector's length is 2. But you know the final length of this\\nnew vector is supposed to be the square root of 2. In other words,\\nit overshot by a factor of root 2. So go ahead and divide by that. Now you get exactly the point\\nyou were looking for. Note that 1 over square root of 2 is\\nactually 1 over the norm of the vector 1, 1. And this is the main idea of a projection. Multiplying by the vector projects\\nthe points along that vector and dividing by the vector's norm ensures\\nthat there is no stretching introduced. Another way of thinking about this is that\\nyou're just changing your vector to have a new norm of 1. Now let's see what happens for\\nthe second row. When you multiply the second\\nrow by the vector 1, 1, you end up getting the sum of 1.2 + 1.6,\\nwhich again you went too far. If you scale it back by square root of 2,\\nhowever, you get exactly the point\\nyou were looking for. Same thing happens for row 3. You will multiply with vector 1, 1. Just as before, you overshoot. And again dividing by root 2 will\\nget you to the desired point. Finally, the same process holds for\\nthe fourth observation. So this vector gives the final coordinates\\nof each point after projecting each point onto the x = y line. So the four points with two\\nvariables each simplify to the vector 1.4142, 1.9799, -0.2121 and\\n-1.344 as you may have noticed, you now need only one column\\nvector instead of a two column matrix in order to represent your\\npoints locations along these line. In general, if you wannas project any\\nmatrix A onto the direction given by vector v, you first need to\\nmultiply matrix A by vector v. However, as you just saw, you need to\\nscale vector v so that it has norm 1. So divide v by its own l 2 norm and this is the projected matrix\\nwhich we are calling Ap. It is important to keep\\ntrack of dimensions. If A has r rows and c columns,\\nthen vector v must have length c. You can also think of\\nthis as a c by 1 matrix. So the projection has r rows and 1 column, you can project onto\\nmultiple vectors at once. Projecting onto two vectors is the same\\nthing as projecting onto the plane those vectors span. In this case,\\nsimply create a matrix of size c x 2, where each column is each vector v1 and\\nv2, each divided by their norm. Let's call this matrix v. The result will be a projection\\nwith r rows and 2 columns, which means you have the same number of\\ndata points, but now only two variables. In the end, the projection can just be represented\\nby the simple equation Ap = AV. Once you build your V matrix,\\nall you need to do is multiply. Projections are a very useful way of\\nreducing the amount of information you need to store in your data set. The question now becomes how do you\\npick the vectors to project onto? And that's what we'll\\nbe looking into next.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"You've been learning\\na lot about variance and covariance because\\nyour ultimate goal will be to build\\na special matrix called the covariance matrix. It is a complex way of storing all the relationships between pairs of variables\\nin your dataset. To introduce this concept,\\nlet's look at three datasets. All 3x variables measured on the horizontal axis have more\\nor less the same spread. I won't worry about calculating the variance here\\nprecisely so I'll just say that each one\\nhas an x variance of 3. Viewed along the vertical axis these datasets also have more\\nor less the same spread, though it's a bit smaller than the spread in the x direction. Here I'll say they have\\na y variance of 1 again, don't worry if these values\\nare exactly correct. It is clear, however, that the covariances\\nare different. The first example has\\na downward slope, so I'll say it has\\na covariance of -2, the second one seems\\nto have a flat slope, so I'll say the covariance\\nis 0 and the last one has a positive trend so I'll say\\nit has a covariance of +2. Using these measures,\\nI will now build a covariance matrix\\nfor each dataset. In the diagonal I will put the x and y variances\\nand in the off diagonal, I will put the covariances. This is called the covariance\\nmatrix and it's simply store the covariances and variances for each\\npair of variables. Let's formalize the\\nprocess a little bit more. You first calculate the\\nvariance of each variable and the covariance of each\\ncombination of variables. Here that's just\\nthe variance of x, the variance of Y, and the covariance of\\nthe two variables. Next you build a square matrix, which I'll refer to with a C with a row and column for each\\nvariable in your dataset. Here we have just two, so\\nthey are named x and Y. At every position\\nin your matrix, you place the covariances\\nof the variables of that row and column so\\nin these two positions, we place the\\ncovariance of x and Y. Notice that the\\ncovariance of x, Y, is the same as the\\ncovariance of Y, x and you can verify this by using the formula\\nyou learned previously. Along the main diagonal, you place the variance of\\neach variable like this. Remember how similar\\nthe covariance and variance formulas were, it turns out that the covariance of a\\nvariable with itself is actually just the variance so you could rewrite your\\ncovariance matrix like this. This truly is just a matrix of covariances\\nbetween variables. Normally you'll see it\\nwritten with variance along the diagonal but\\nthe important point here is that we're performing the exact same calculations\\nat every cell in this matrix. Finding the covariance of the variables in\\nthat row and column. It is very common to see the covariance matrix expressed in matrix notation which gives a straightforward\\nand efficient way of computing the covariance matrix from your data or observations. For this, you first\\nneed to store all your data\\npoints in a matrix. Each row will be an observation of the variables x and Y, and each column contains all the observations\\nfor a single variable. Let me call this\\nmatrix A you will also need to define a matrix that has the same shape as A, and each column takes on the\\nmean value for the variable. This matrix is called Mu, using these two matrices, you can write the\\ncovariance matrix as 1/n-1*A minus Mu transpose\\ntimes A minus Mu. Now let's begin by replacing A and Mu by their expression. Here, I'm just replacing the symbols with the\\nmatrices they represent. Next, you need to complete\\nthe calculation A minus Mu in both terms subtracting element wise gives this new expression. Next, you need to transpose the first matrix which\\ngives this new expression. Note that the first\\nmatrix is size 2 which is the number\\nof variables by n, which is the number\\nof observations. While the second\\nmatrix is size n by 2 this means that the product between the 2 matrices\\nwill be size 2 by 2, which is precisely the size\\nof the covariance matrix. This is a good sign\\nthat we're on track. Now, begin doing the\\nmatrix multiplication. For the first element of\\nthe resulting matrix, you need to multiply the\\nfirst row of the first matrix by the first column\\nof the second matrix, which is simply x_1\\nminus Mu_x* x_1 minus Mu _x+x_2 minus\\nMu_x8x_2 minus Mu_x, so on for all the n terms. Which actually\\nreduces to the sum of n terms of the square of the difference between\\neach observation of variable x and the\\nmean of that variable. If we incorporate the n-1, then this is the expression\\nfor the variance of x. Now let's complete the second\\nelement of the first row. Here you will multiply the first row of the matrix by the second column of\\nthe second matrix, which gives you x_1 minus Mu_x*y_1-y+x_2 minus\\nMu_x*y_2 minus Mu_y, etc. This expression\\ncan be simplified to this sum right\\nhere and again, if you include the 1/n-1 term you get exactly\\nthe expression for the covariance between x and Y. Notice that everything\\nis really symmetrical in this matrix multiplication\\nso if you were to look into the product of the second row of the first matrix and the first column\\nof the second matrix, then you would again\\nget covariance of y, x. Lastly, the product\\nbetween the second row of the first matrix and the second column of\\nthe second matrix gives you the variance of Y. This means you recover the\\noriginal expression of the covariance matrix from much smaller simpler\\nmatrix operations. That admittedly got\\nsomewhat complicated, but let's see what\\nthose 2 equations actually look like\\nwith a real dataset. The dataset has 8 observations plotted on an x, y\\ncoordinate grid. Given the distribution\\nof the data, you would expect the x and\\ny variances to be about the same and the negative covariance because of the downward trend. First we write your\\ndata on a table of two columns, one\\nfor each feature. You'll need to calculate\\nMu_x and Mu_y, the average of each column. In this case, the average\\nvalues of these columns are 8 and 6 now create the\\nmatrix A minus Mu, in which all the values have the column\\naverage subtracted. Now transpose that matrix and set up the matrix\\nmultiplication. This table has 8\\nobservations so add 1/n-1, or one seventh, to the\\nfront of this product. Now all you need to do is multiply the result will\\nbe this 2 by 2 matrix, which is C the covariance matrix and as predicted the x\\nvariance and the y variance, which lie on the diagonal, are fairly similar and the\\ncovariance is negative. The examples you\\nsaw were all for datasets of two variables, but the same process works\\nfor any size dataset. Here the same steps\\nare shown with a new column z added,\\nhere's what you do. Arrange the data with a different\\nfeature in each column. Calculate the column\\naverages, Mu, subtract the average\\nfrom their columns, generating the\\nmatrix A minus Mu. Multiply A minus Mu transposed\\nby A minus Mu and divide by n-1 given the covariance\\nmatrix C. Again, this has been a technical\\nlook at the process underlying this very important\\nmatrix and you'll have more opportunities to study\\nthe statistical concepts in more detail in the third\\ncourse of this specialization. I think it's beautiful though, that a complicated process that generates the\\ncovarian matrix can be so succinctly expressed\\nusing matrix multiplication. Now you're also ready\\nto see the final, incredible step that powers PCA.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"PCA relies on a few\\nconcepts on statistics, which I'll be introducing\\nin this video. If you study statistics\\nin the past, this will be a helpful review. If not, you'll learn more about these ideas in the third\\ncourse of the specialization. With that, let's start with the first statistical\\nconcept, the mean. Consider this dataset, each point in the plot\\nis an observation composed of two\\nvariables, x and y. Each point is at a\\nlocation x_i, y_i. The mean of your data is\\nsimply the average value of all the observations\\nwhich will be somewhere around in this point. Here's how you find\\nit mathematically. For the x variable, sum all n values of\\nx and divide by n, the mean of y works\\nin the same way, you just average the\\nvalues of each feature. This middle point therefore has the coordinates mean\\nof x and mean of y. Next, you'll understand\\nthe concept of variance, which describes how\\nspread out your data is. If you wanted to describe how the dots appear in the chart, you might say that the dots\\nare more spread out or have a larger spread if we\\nlook at them along the horizontal axis\\nand are more compact, or have a smaller\\nspread if we look at them along the vertical axis. In statistics, the\\nspread is measured or described by the\\nvariance of a dataset. A dataset with no spread of values has a variance of zero, and a dataset with\\na large spread will have a large variance. To see this more clearly, I'm going to take our two\\ndimensional chart and move every point to the\\nhorizontal axis like this. Again without yet worrying about how variance\\nis calculated, we can see that this\\nvariance along the horizontal x-axis is\\nrelatively large. If you repeat the process\\nalong the y-axis, the values are spread out\\nacross a smaller range. The y variance is\\nrelatively small, but still bigger than zero, since there is some\\nvariation in the values. Here's the equation\\nfor calculating the variance of a\\nlist of values. To help you understand\\nthe different parts of this equation, consider a very simple dataset with one column representing the variable x and five\\nrows numbered 1-5. Each row is an observation x_i. First, you'll need to calculate\\nthe mean of x by simply adding up the values of\\nx_i and dividing by 5. This gives us a total\\nof 45 and a mean of 9. Next, find the difference\\nbetween every value and column x_i and the mean\\nyou just calculated, or in other words, just\\nsubtract 9 from each row. Now square each of\\nthese differences, placing the results\\nin a new column. The summation tells us\\nto sum this 5^2 values, giving a total of\\n64 and finally, divide this total by\\nn-1 since n here is 5, we divide by 4, giving\\nyou a variance of 16. When this equation is\\nwritten in formal notation, some of these terms are\\ntypically shortened. Variance is often\\nshortened to var, and the Greek letter Mu is\\nused to represent the mean. Another way to think\\nof variance is as the average squared\\ndistance from the mean. The fact that it's the average\\nsquare difference is a little odd but the most\\nimportant takeaway here is that as your data\\nbecomes more spread out and average is further from the mean , variance will increase. Returning to the\\ndataset from earlier, you'd now refer to that\\naverage point as Mu_x, Mu_y. You could calculate\\nthe variance of x and y using the formula\\nyou just reviewed. X variance is larger\\nthan the y variance, and from that formula, it's\\nclear why that's the case. Built along the x axis, these points are\\nfurther away from Mu_x, so the average square\\ndistance will also be larger. Meanwhile, along the y-axis, the average square distance\\nfrom Mu_y Y is smaller. Variance helps us quantify\\nhow spread out your data is but now consider\\na situation where variance alone\\nwouldn't be helpful. These two datasets have\\nthree observations each. They would have\\nidentical y variance and x variance and yet, it's obvious that there's\\na significant difference in the patterns of\\nthese datasets. The solution is now a\\nmeasure called covariance. Covariance helps measure\\nhow two features of a dataset varies with\\nrespect to one another. Notice that in the left dataset, the pattern within the\\ndata is down and to the right as x values\\nincrease, Y values decrease. In the right dataset, we have the opposite pattern\\nas x values get larger, y values get larger too. Covariance quantifies\\nthis relationship, resulting in the\\nleft dataset having a negative covariance and the right dataset having\\na positive covariance. Now that you have a high\\nlevel understanding of what it's trying to measure, let's see how covariance\\nis actually calculated. The equation for covariance\\nlooks like this. It can be a little\\ncomplicated at first, so I'll break it\\ndown into pieces. As you can see however, it looks pretty similar to the variance equation and if you expand out the squared term at the end, they're\\nnearly identical. The only difference\\nis that inside the circle items which now\\ndepend on the values of both the x and y\\nvariables as well as the average of these\\nvalues Mu_x and Mu_y. Let's look at these\\nthree example datasets to understand how\\nthis equation works. For the first dataset,\\nwe expect to calculate a negative covariance as the\\ndata is trending downwards. In the second case,\\nthe trend line fitting the observation\\nseems flat, so we'd expect the covariance of zero or a very small value. In the third case, x and y seem to trend upwards together, which should result in\\na positive covariance. To understand the impact\\nof the circled items, you can draw the mean point, Mu_x, Mu_y on top\\nof each dataset. Subtracting Mu_x from\\neach x and Mu_y from each y essentially recenters\\nthe data around this point. And you can imagine it splitting the plane into four quadrants. Each point in the\\ndataset lies in one of these quadrants\\nand results in either a positive or\\nnegative contribution to the overall covariance. In the first quadrant, both\\nx and y are greater than their means so the product in the circled term is positive. In the second quadrant, x is less than its mean, but y is still greater. The circled term is\\nnow the product of a negative and a\\npositive number, and the results are negative. In the third quadrant, both x and y are less than their mean. The circled term is now the product of two\\nnegative numbers, so the result is\\npositive and for dots in the final quadrant x is\\ngreater than its mean, but y is smaller. The circle term is a product of a positive\\nand negative number, and the result is negative. Recall that the first part of the covariance equation is just asking to take a summon divided by the number of\\nvalues minus one. In other words,\\nwe're more or less averaging all these products so in a somewhat\\nsimplistic way you can think of covariants\\nas just asking, on average, are\\nthere more dots in the positive quadrants\\nor in the negative ones. For the first date is that more are in the negative quadrants, so the covariance\\nwill be negative. In the second, the\\ndots are about equally spread between positive\\nand negative quadrants, which results in a\\ncovariance close to zero. In the third, most of the dots are in the positive quadrants, leading to a\\npositive covariance. Whether you feel like you fully understand the\\ncovariance equation, what's most important\\nat this point is to understand intuitively\\nwhat it measures. For now, you can think of\\ncovariance as measuring the direction of the relationship\\nbetween two variables. A negative covariance\\nindicates a negative trend, a small covariance\\nindicates a flat trend or no relationship and a\\npositive covariance indicates a positive trend. So long as you're comfortable\\nwith these concepts, you're ready to move on.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Okay, so now you might believe that this\\nprocedure works, but I haven't explained why a rigorous proof would be\\na little challenging to show here. However, I can give you some of\\nthe intuition behind this process. Let's go back to the case of\\na dataset with just two features, which I'll still call X and Y, which you\\ncan think of as both a table of data and points graphed in space. So once you have your data,\\nyou can get the covariance matrix C. It contains information about how spread\\nout the data is from the perspective of each pair of variables or maybe better\\nyet, how spread out is the data. This is easier to see if you\\nthink of C as a change of basis, how would it transform space? Well, the fundamental vector\\n10 would be moved to 94, and the fundamental vector\\n01 would be moved to 43. You can also check that the vector\\nminus 10 moves to minus 9 minus 4, and the vector 0 minus 1\\nmoves to minus 4 minus 3. Can you already see what\\nthe transformation looks like? If you can, that's okay. Let's see what happens with multiple\\ndirections all around the circle of radius 1. Let's start by adding this point and\\nseeing its transformation, and keep going all around the circle. If you join all the dots\\nin the transformed plane, you will see that they\\nare mapping out an ellipse. So that the circle of radius 1\\nis transformed into the ellipse, where you can see that all the points\\nare stretched into different directions. Note that we consider the circle a radius\\n1 because we are only interested in the direction of the stretches. If you had chosen points of another,\\nbigger circle, you would still get the same ellipse,\\nonly bigger. Looking at the transformed points, which would you say is the direction\\nof the biggest stretch? I would say that is this\\nred line right here, which aligns with the major\\naxis of the ellipse. If you cut the ellipse in any other\\ndirection, you will get a shorter line. Here's where Eigenvalues and\\nEigenvectors come in. Remember that this covariance\\nmatrix has two Eigenvectors 21 with Eigenvalue 11 and\\nminus 12 with eigenvalue 1. Together, these two vectors form an Eigen\\nbasis, and from the perspective of these Eigen bases, the transformation\\nc just stretches the plane. Now it looks like the direction\\nof greatest stretches along the greatest Eigenvector,\\nwhich is true. Let me help you understand why that\\nmakes sense based on what you've learned about Eigen bases. The whole point of getting\\nthe Eigenvalues and Eigenvectors was to reframe a linear\\ntransformation as two stretches. Any point along the vector to one will\\nbe stretched by a factor of eleven, its Eigenvalue. Any point along the vector minus 12\\nwill get stretched by a factor of 1, which is its Eigenvalue. And any other vector in the plane would be\\nstretched by a factor somewhere between 1 and 11. Let me show you a couple of examples\\nto demonstrate that this is the case. Ill calculate the norm\\nof the vector before and after the transformation using the norm\\nequation, and find the ratio between them. Ill start with vector 01 in teal, which\\nyou already saw, is sent to the vector 43. In this case, the vector starts with a\\nnorm of 1 and ends up with a norm of 5, so it gets stretched out by a factor of 5. Now consider the vector 01 in orange, which you already saw was\\nsent to the vector 94. This vector also starts with a norm of 1,\\nand if you use the norm equation you can\\ncalculate a final norm of about 9.85. So again, that's how much\\nthis vector was stretched by. This time the stretch was closer to 11,\\nbut is still less than the maximum stretch\\nwhich will occur along the eigenvector 21. Again, this isn't a rigorous proof, but\\nhopefully this helps build the intuition behind choosing the Eigenvectors\\nwith the biggest Eigenvalues. Your covariance matrix C characterizes\\nthe spread of your data. The matrix C Eigenvectors tells you\\nthe direction in which the matrix can be viewed as just straight stretching. The largest Eigenvalue tells you where\\nthat stretching is greatest, and any other direction will\\nbe stretched out less. So choosing the Eigenvectors with\\nthe biggest Eigenvalues will give you the directions with the biggest\\nstretch or the most variance, and that's what you've been looking for\\nthis whole time.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"In the example from last video, you had a three by\\nthree matrix which had three different eigenvalues and three different eigenvectors, one for each of the eigenvalues. Could it be that all\\nthree by three matrices always have three eigenvectors? Well, it turns out that\\nthat's not always the case. Let's look into some examples. Consider this three\\nby three matrix A with the values shown here. Now find the\\ncharacteristic polynomial as the determinant(A-Lambda I). Using the formula\\nfor the determinant for three by three matrices, we get (2-Lambda)^2*(4-Lamda). All the remaining terms are zero due to all the zeros in\\nthe original matrix. Now to find the eigenvalues, you need to find the\\nzeros of this polynomial, which in this case\\nis super simple. This gives the\\neigenvalues 4, 2, and 2. Notice that the number\\n2 is repeated twice. Now watch what\\nhappens when we find the eigenvectors\\nassociated with them. Let's start with\\nthe eigenvalue 4. The eigenvector needs\\nto satisfy that A times the vector x_1, x_2, x_3 equals the vector\\n4*x_1, 4*x_2, 4*x_3. The product between the matrix and the vector can be expanded as the vector with entries\\n2x_1, -x_1+4x_2-0.5x_3, and 2x_3 and we want this to be\\nequal to the vector 4x_1, 4x_2, 4x_3, which leads to this\\nsystem of equations. This one can be\\nwritten as -2x_1 = 0, - x_1-0.5x_3 = 0 and\\nnegative 2x_3 = 0. Notice that all I\\ndid here was moving everything to the left\\nof the equal sign. Now we can simplify a\\nlittle bit and get that the first one is\\nequivalent to x_1 = 0. The third one is\\nequivalent to x_3 = 0. Now, since x_2 doesn't appear in any of these\\nequations at all, then the second\\nequation is 0, 4, 3 and x_2 can be any number in order to\\nsatisfy these equations. To keep it simple, we can\\nset x_2 to be equal to one, and you get the eigenvector 0, 1, 0 associated to\\nthe eigenvalue 4. Now let's repeat for\\nthe eigenvalue 2. In this case, A dot product\\nwith the vector x_1, x_2, x_3 should be equal to the vector 2x_1, 2x_2, 2x_3. The dot product can\\nbe expanded with the same expression as before. We again get a\\nsystem of equations. 2x_1 = 2x_1, -x_1+4x_2-0.5x_3 is 2x_2, and 2x_3 = 2x_3. When we manipulate\\nthis by taking everything to the left\\nof the equal sign, we get that the first\\nequation is quite trivial, it's 0 = 0. The second one is\\n-1+2x_2-0.5x_3 = 0. The third one is, again trivial, it's 0 = 0. You can rewrite the\\nsecond equation as x_1 = 2*x_2-0.5*x_3. This equation has\\ninfinitely many solutions depending on what values\\nof x_2 and x_3 you choose. So let's say that you\\nchoose x_2 = 1 and x_3 = 0. That means x_1 has to be two for it to be an eigenvector, giving the vector 2, 1, 0. But you can also choose x_2\\nto be one and x_3 to be two, which gives the\\neigenvector 1, 1, 2. These two vectors point\\nin different directions. So there are actually two\\ndifferent eigenvectors. Remember that you\\nonly care about the direction of the\\neigenvector since any scale version\\nof it will still be an eigenvector to\\nthe same eigenvalue. It is worth mentioning that you could have found\\ndifferent pairs of eigenvectors depending on\\nthe values of x_2 and x_3. But the important thing is\\nthat you can always find two distinct directions\\nfor the eigenvalue 2. To sum up, this matrix A has the following pairs of\\neigenvalues and eigenvectors. The first eigenvalue Lambda_1, which is four, has an\\neigenvector 0, 1, 0. The second eigenvalue\\nLambda_2 which is two, has as an eigenvector\\nthe vector 2, 1, 0. Finally, the third\\neigenvalue Lambda_3 also equals two and has as\\nan eigenvector 1, 1, 2. In this case,\\nyou're able to find three distinct eigenvectors even though you had a\\nrepeated eigenvalue. Let's look into one more example where this\\ndoesn't happen. Let's change just one value in the matrix and see what happens. The characteristic polynomial\\nstays the same as before. Once again, we're going\\nto have the values 4, 2 and again two as eigenvalues. Let's repeat the process\\nfrom before to find the eigenvectors and start\\nwith the eigenvalue 4. The only equation that\\nchanges here is the last one, where now you have 4*x_2+2*x_3. That gives us this system of equations where the first\\ntwo remain the same, being -2*x_1 = 0\\nand -1-0.5x_3 = 0. Again, let's name\\nthem equations 1, 2, and 3 respectively. From equation 1, you get\\nthat x_1 has to be zero. Combining equations 3 and 1, you get that x_3 has to be zero. This leaves, again, x_2\\nto take on any value. Just as with the\\nprevious matrix, you can choose the vector 0, 1, 0 as the eigenvector. It's the same one for\\nthe previous matrix. For the eigenvalue 2, the same thing happens. When you solve this equation, you get that 2x_1 = 2x_1. That -x_1+4x_2-0.5x_3 is 2x_2, and that 4x_1+2x_3 is 2x_3. Which gives rise\\nto the following system of equations that can be simplified as 0 = 0, as -x_1+2x_2-0.5x_3 = 0. Finally, 4*x_1 = 0. This imposes the restriction\\nthat x_1 can only be zero. Now, from equations\\n2 and 3 you get that x_3 must equal 4*x_2. Now you only have one\\ndegree of freedom, since x_1 is always zero. And once you fix x_2, the value of x_3\\ngets fixed as well. For example, you can consider\\nx_1 = 0 and x_2 = 1, which makes x_3 = 4 given\\nthe eigenvector 0, 1, 4. What would happen\\nif, for example, you choose x_2 to be 1/2? Then x_3 must be two given\\nthe eigenvector 0, 1/2, 2. However, these two vectors\\nlie on the same line. One is simply a scaling\\nfrom the other, which means that they are\\nactually the same eigenvector. You can test it, you can't find any vector which\\nsatisfies equations 1, 2 and 3 that spans\\na different line. This means that the number 2\\nis two times an eigenvalue, but you can only find one\\neigenvector associated with it. Then the eigenvectors are\\nof the form 0, k, 4k, but there's only one\\ndirection which can be paired with eigenvalue 2, even though the number 2\\nappears twice as an eigenvalue. So a recap. For the matrix entries 2,\\n0, 0, -1, 4, -0.5, 4, 0, 2, you have an eigenvalue\\nof four associated to the eigenvector 0, 1, 0. Then eigenvalue of\\ntwo associated to the eigenvector 0, 1, 4. Again, an eigenvalue\\nof two which doesn't have an associated eigenvector. That means that you can't\\ncreate an eigen basis for the three dimensional\\nspace because you're lacking one vector\\nto span the whole space. As a summary, if you have a two by two matrix with eigenvalues Lambda_1\\nand Lambda_2. If the two eigenvalues\\nare different, then you always get two\\ndistinct two eigenvectors. However, if the\\neigenvalues are the same, then you can have either\\none or two eigenvectors. If you have a three\\nby three matrix, so eigenvalues Lambda_1, Lambda_2, and Lambda_3, then you get some more\\noptions to explore. If all the three\\neigenvalues are different, then you can always find\\nthree distinct eigenvectors. If you have one eigenvalue\\nrepeated twice, with the other being different, then you can have either\\ntwo or three eigenvectors. These are the two examples\\nyou just explored. However, if you have the same eigenvalue repeated three times, you can have any number of eigenvectors between\\none and three. In this week's\\nprogramming assignments, you will get to explore many\\nmore interesting examples of matrices with\\ndifferent properties and number of eigenvectors.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you know what\\nan icon basis is, you may be wondering\\nhow to find it. The process is actually\\nnot too difficult. It entails solving an equation with a determinant. Here it is. First, take a look at the\\nmatrix with entries 2,1, 0,1,3 and take a look at how it acts on these points\\naround the square. This should make the entire\\ntransformation clear. As you've seen before, the two horizontal\\nvectors get stretched by two and the diagonals\\nget stretched by three. Now for the other\\npoints this happens. Now compare this\\nto another matrix, one that simply stretches the entire plane by a factor\\nof three in any direction. This matrix has entries 3,0,0 and 3 and it's really three\\ntimes the identity matrix. Notice one thing these\\ntwo transformations are not the same transformation, but they do coincide\\nin many points. In other words, they act the exact same way for\\ninfinitely many points. All the points in this line. To be more specific,\\nin this diagonal, the two transformations\\ndo the exact same thing. They match in\\ninfinitely many points. Now that is strange, the transformation should only match at one point,\\nthe point 0,0. When they match at\\ninfinitely many points, something non singular\\nis happening. What's happening? Well, let's\\nlook at the difference. If these two transformations match at infinitely many points, that means the difference is zero at infinitely many points. If you apply the difference of these two matrix to any vector\\nin any of these diagonals, you get the vector 0,0. In other words, this\\nmatrix times vectors x, y for infinity metric\\nvectors is 0,0. Now that is the trait of a\\nsingular transformation. Recall that non\\nsingular transformation has a unique solution to the equation matrix times vector equal 0,0 and that's\\nthe vector 0,0. If you have an infinitely\\nmain solutions to this, means your matrix is singular, and you can verify that this\\nis indeed a singular matrix, as a determinant is zero. Now let's do something similar, but for another\\ntransformation on the right. Our transformation does\\nexactly as it did before. Now let's compare it to\\nthe transformation that stretches the plane by\\ntwo in every direction. These two are not the same, but they match in\\nthis entire line. In other words, matrix on the left times xy is equal to matrix of\\nthe right times xy. For any vector xy on this line, that's infinitely many points. We can do the same procedure, take the difference\\nand that matrix times a vector equal 0,0 for\\ninfinitely many vectors. That means that the\\nmatrix 0,1, 0,1, or the difference\\nbetween our matrix and two times the identity, is a singular matrix. You can check indeed that it is singular matrix because\\nits determinant is zero. What is special about\\nthe eigenvalues? What happened for the Eigenvalue\\n2 and the Eigenvalue 3? Let's think about it in general. If Lambda was an eigenvalue, then the transformation given by a matrix and the transformation\\ngiven by scaling the plane entirely by\\na factor of Lambda are equal for infinitely\\nmany vectors xy. That means their\\ndifference times a vector is equal to 0,0 for\\ninfinitely many vectors, an equation when you\\ncreate many solutions. Therefore, this\\nmatrix, 2-Lambda, 1,0,3 minus Lambda has to be a singular matrix so\\nour determinant is zero, it's determinant is given by this equation when we expand it, and that's called the\\ncharacteristic polynomial. Basically to find the eigenvalues\\nLambda all we need to do is look at the characteristic polynomial\\nand find the roots. The place where the\\ncharacteristic polynomial is zero are the eigenvalues. In this case, they're\\ngoing to be two and three. Now that you have\\nthe eigenvalues, let's try to find\\nthe eigenvector. Recall that the eigenvector\\nis the vector that satisfies the equation matrix times vector equals eigenvalue times vector. If we expand this, we get these equations over here and the solution\\nfor these are x=1, y=0, or any multiple vector. Here is one of the\\neigenvalue vector, the one corresponding\\nto the Eigenvalue 2. We do the same thing with three, solve for these\\nequations and get 1,1. The eigenvector 1,1 is\\ncorresponding to the Eigenvalue 3. Now you're ready for a quiz. Find the eigenvalues and\\neigenvectors of this matrix. The solution is that for\\nthe eigenvalues it's 11,1, and for the eigenvectors is 2,1 corresponding to\\nthe eigenvalue 11, and minus 1,2 corresponding\\nto the eigenvalue of one. Why? Well, if you look at the\\ncharacteristic polynomial, it is the matrix with\\nentries 9-Lambda, 4,4, and 3-Lambda. That expands as\\nLambda^2-2 Lambda + 11, which factors as\\nLambda-11*Lambda-1. Therefore, the eigenvalues are Lambda equals 11\\nand Lambda equals one. I'll leave it as an\\nexercise for you to solve the equations\\nfor the eigenvectors and verify that they are\\ngoing to be 2,1 and minus 1,2 or some multiple of them because all that\\nmatters is the direction. The process of finding\\nin vectors for a three by three matrix\\nis very similar. Consider A equals\\n2,1,-1, 1,0,-3, -1,-3,0. The characteristic polynomial is the determinant of A minus Lambda times is the\\nidentity matrix of whatever size you need. In this case, Lambda\\nminus I will be a three by three matrix\\nthat looks like this. The characteristic\\npolynomial will be the determinant of the difference\\nof these two matrices. Using the diagonal method to calculate the determinant\\nof a three by three matrix, you can construct the pieces of the characteristic\\npolynomial, then combine terms to get the polynomial -Lambda^3+2\\nLambda^2+11 Lambda-12=0. Factoring this polynomial, which I won't show\\nin detail here, will give you these\\nthree factors. Now it's easy to find the\\nzeros of this equation, which are when Lambda\\nis -3,1, or 4. That gives you the three\\neigenvalues of this matrix. Now that you found the three\\neigenvalues from matrix A, let's find the eigenvectors associated to each eigenvalue. Let's begin with our\\nlast eigenvalue, four. Remember that to\\nfind the eigenvalue you need to solve the equation Av=Lambda v. Then that means you need to\\nsolve this equation, where x_1, x_2, and x_3 are the three components\\nof the eigenvector. Multiplying the form\\nby the vector gives the new vector 4x_1, 4x_2, 4x_3. Completing this dot product on the left gives you\\nthis new vector. Now all you need to do is\\nset these three vectors equal to one another and\\nsolve for x_1, x_2, and x_3. I'll rewrite the system\\nof equations up here. Now subtract the terms from the right hand side,\\nleaving just zero. This gives the final system of equations you'll\\nneed to solve, which are labeled\\nRows 1,2, and 3. You can use this equation to solve for x_1, x_2, and x_3. Adding Rows 2 and 3\\nyou get -7x_1-7_x3=0, which simplifies 2x_2\\nbeing equal to -x_3. Adding three times\\nRow_1 to Row_3, you get -7x_1-7x_3=0, which simplifies 2x_1\\nalso being equal to -x_3. This is actually as far as you can get solving the system, and the result is that there are infinitely many solutions. Any vector that satisfies\\nx_1= k, x_2= k, and x_3=-k will be a solution for any value\\nof k. For example, 1,1-1 works, but 2,2-2 would work as well\\nand this makes sense. There are actually always going to be an infinite number of potential eigenvectors that\\nlie along the same line. In this case though, let's\\njust keep it simple and say the eigenvector is\\n1,1-1 and that's it. You found the first eigenvector. If you want to find the\\nother two eigenvectors, you can just repeat the process with the other two eigenvalues. You already found that\\nthe Eigenvalue 4 is associated with the\\neigenvector 1,1-1. If you follow the\\nsame process for eigenvectors one and -3, you could find their\\neigenvector 0,1,1 and 2-1,1. If you notice the two\\nexamples you worked on were for two by two and\\nthree by three matrices, all of which are\\nsquare matrices. Could you compute\\nthe eigenvalues and eigenvectors for any\\nmatrix of any shape? Remember to get the eigenvalues you need to solve\\nfor a determinant. But as you learned\\nin previous lessons, the determinant is only\\ndefined for square matrices. For any square matrix, you can find eigenvalues\\nand eigenvectors. However, the matrix is not squared like the one right here, then it doesn't have any\\neigenvectors or eigenvalues.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Let's trace the steps of PCA and finally put all\\nthe pieces together. Remember at the start, you had this dataset which you centered. The goal was to\\nproject your data onto the line that preserved\\nthe most information. You learn that this\\nbest line is the one which preserves the most\\nvariance in your data. The big question then is, how do you find this best line? This is where\\neverything you've been learning in this\\nlesson comes together. So far, you've learned\\nabout projections and how a simple matrix\\nmultiplication allows you to project data into a\\nlower dimensional space. You've also learned\\nabout eigenvalues and eigenvectors and\\nhow they capture the directions in which a\\nlinear transformation only stretches space but doesn't\\nrotate it or share it. In the previous video, you got familiar with the\\ncovariance matrix and how it compactly represents\\nrelationships between the variables\\nin your dataset. PCA works by combining these three ideas in\\na clever way that helps reduce the number\\nof dimensions in your data. Let's\\nsee how it works. I'll start again with\\nthe same dataset which have centered\\naround its mean point. Let's begin by finding\\nthe covariance matrix C for your data points. Suppose that the x variance is nine and the y\\nvariance is three. The covariance is four, which makes sense as there's a positive trend\\nline to this data. Now here's the big leap\\nof this entire process. You find the eigenvalues and eigenvectors of the\\ncovariance matrix. This is how you'll find the line onto which you should\\nproject the data. Remember, the eigenvectors and\\neigenvalues come in pairs, with the eigenvector\\ngiven a direction and the eigenvalues\\ngiving a magnitude. The first eigenvector is 2, 1, and it has an eigenvalue of 11. The second eigenvector is -1, 2, and it has an\\neigenvalue of one. You may have noticed\\nthese two vectors are at a 90 degree angle\\nto one another, which is also called orthogonal. This is not a coincidence, but it is true for\\nthe eigenvectors of every matrix that is symmetric\\naround its diagonal. If you transpose this matrix, these two forces would just\\nchange spots. C is symmetric. In fact, every covariance\\nmatrix is symmetric, so the eigenvectors you calculate will always\\nbe orthogonal. Now you have two eigen vectors, or as they're called in\\nPCA principal components, and you want to project your\\ndata along one of them. You can probably tell from\\nlooking that there is much more variance along the red vector than\\nalong the green one. But how could you determine\\nthis mathematically? It turns out that\\nthe eigen vector with the largest\\neigen value will always be the one that will give the greatest variance when\\nyou project your data. In this case, the vector 21\\nhas an eigen value of 11, which is much larger than 1, so 2,1 is the winner. That is the line you'll\\nproject your data along. You can, meanwhile, discard the second eigen vector with\\nits smaller eigen value. You can now draw the line\\nthat the vector 2,1 spans, and all that is left to do is project your data\\nonto this line. Now that the points are\\nprojected along this vector, you no longer need to graph\\nthem in two dimensions. All that matters is their\\nposition along the vector. I'll graph the projected\\ndata like this. Just like that, you have\\nreduced the dimensionality of your data and preserved as\\nmuch variance as possible, which were the two goals of PCA. Visually, you can\\nsee your data went from two dimensional\\nto one dimensional. Rather than storing an\\nx and a y variable, you can think of your\\ndata having been reduced to a single\\nnew value, z, which tells you where on the eigen vector each\\nobservation was projected to. Your data has fewer dimensions and the maximum\\npossible variance. This process works for\\nmuch larger datasets too. Suppose you have a nine\\ndimensional dataset, so a table with nine\\ncolumns or features, as many rows or\\nobservations as you like. I'll just say there\\nare n observations. Here's how you'd perform\\nPCA on this data. From your data, you can get the covariance matrix just like you saw in the\\nearlier Electra. Since you have nine variables, this will result in a nine\\nby nine covariance matrix. Next, you would need to find the eigenvalues and\\neigenvectors of this matrix and sort them according to the eigen value from\\nbiggest to smallest. Imagine you want to reduce your data set to\\njust two variables. Then you'll simply keep the\\ntwo biggest eigen values and their associated eigen\\nvectors and discard the rest. Now you have the two\\nvectors you wish to project your data onto, v_1 and v_2. To project your data, create a new matrix where each column is one of the two eigen vectors\\nscaled by its own norm. Finally, multiply the matrices to project your data\\nonto these two vectors, giving you your final dataset which has only two features.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you've seen\\nthe concept of a projection, let's see how it would be used by PCA\\nto reduce the dimensions of a data set. In this graph, each dot represents\\na different observation composed of two features graphed\\nas their x and y position. Reducing the dimensions of this data means\\nmoving from two-dimensional data graphed as points in the plane, to one-dimensional\\ndata, a graph as points along a line. As you can see, the set is not centered\\naround 0,0, so let's start by doing that. Now let's see what happens if\\nyou project onto the x-axis. Let's save this projection for later. Now, let's project onto the y-axis. You can already see that the points in\\nthis projection are less spread because the points are closer to each other. You can project them to any line,\\nfor example, this one, which can be represented by\\nthe equation y plus x equals 0, which is equivalent to projecting onto the\\n1 minus 1 vector since it spans the line. Or you could also consider this\\nother line that solves the equation 2x minus y equals 0,\\nwhich is projecting onto the 1,2 vector, or even in this direction over here. Notice that this line fits\\nthe data pretty well, and the resulting projected points\\nare still relatively spread out. As you can see, after these projections,\\nthe data points may be more or less spread out, and that's going\\nto end up being very important. The reason is that more spread out\\ndata points preserve more information from the original data set. In other words, preserving more spread\\nmeans preserving more information. Now, I'll sort the projections from the\\none that has the points most spread out to least. The projection at the top has the points\\nmost spread out, and therefore, it preserves the most information from\\nthe original data set, and the projections at the bottom have the least spread and\\ntherefore preserve the least information. So the goal of PCA will be to find\\nthe projection that preserves the maximum possible spread in your data, even as you reduce\\nthe dimensionality of your data set. And again, here are the main benefits\\nof dimensionality reduction in general and in PCA. Dimensionality reduction makes data sets\\nthat are easier to manage simply because they're smaller. PCA in particular allows you to\\nreduce dimensions while minimizing information loss. Thanks to this reduction in dimensions,\\nit is possible to analyze visualize data in ways which might not have\\npreviously been easy or possible. Now, this was a really quick introduction\\nto the concepts underlying PCA. In the following videos,\\nyou'll learn how it actually works.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"And next, I'm going to show you\\na pretty interesting rule for determinants of inverses,\\none that you may all ready be suspecting. And the way you can figure\\nthis out is with a quiz. So please find the determinants\\nof these two matrices. And the determinants are 0.2 and 0.125. Now recall that this matrix over\\nhere is actually the inverse of the matrix we've been using for\\nmostly examples. This matrix has determinant 5, and\\nas you've recently calculated, this matrix has determinant 0.2,\\nwhich is exactly one fifth or the multiplication members of 5. Now, the second matrix is\\nthe inverse of this one over here, which has determinant 8. And you've calculated this one\\nto have determinant of 0.125, which is the multiplication members of 8,\\nprecisely. And for the last one,\\nif you recall from a few videos ago, this matrix over here\\ndoesn't have an inverse. And it's actually a singular\\nmatrix of determinant 0, and it has no inverse and\\nprecisely 0 has no inverse. So a nice coincidence is spot over here, which is that it seems that\\nthe determinant of an inverse matrix is the inverse of the determinant\\nof the matrix. And that is precisely the case\\nwhen the matrix is convertible, then the determinant of the inverse is\\none over the determinant of the matrix. And why is this? Well, let's check. We know by the product formula that\\nthe determinant of a product is equal to the determinant of\\nA times the determinant of B. Now let B be actually A inverse. So the determinant of A times A inverse\\nis equal to the determinant of A times the determinant of A inverse. Now AA inverse is\\nprecisely identity matrix. So the determinant of the identity\\nmatrix is determined of A times determinant of A inverse. Now the determinant of\\nthe identity matrix is always 1. And therefore that means that\\nthe determinant of A inverse is precisely 1 over the determinant of A. Now why is the determinant\\nof the identity matrix 1? Well, let's actually calculate it. Notice that for a two by two matrix is\\n1 times 1 minus 0 times 0, which is 1. And I'll leave you the exercise of taking\\na big matrix and identity matrix and actually calculating the determinant and\\nverifying for yourself that it's always going to be 1.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Welcome to week 4. This week you'll continue learning some\\nof the most important concepts of linear algebra through the lens\\nof linear transformations. First, you'll see what some concepts,\\nsuch as determinants and singularity, mean in terms of\\nlinear transformations, and you'll be delighted to see\\nhow naturally they appear. Then you'll learn one of the most powerful\\napplications of linear algebra in the real world, eigenvalues and eigenvectors. They are used extensively in many fields,\\nincluding machine learning, in particular in a very important\\ndimensionality reduction algorithm called principal component analysis, or PCA. Here's a quick explanation of\\nprincipal component analysis, the topic we'll be covering\\nat the end of this week. Imagine that these points\\nform your data set, and you want to simplify your data set a bit. For example,\\neven though the points are in space, it seems that all of them lie so\\nclose to this line over here. Perhaps we can look at the data set from the point of view of the line. PCA would be able to find this line, and\\nwhat it would do is to simplify the data set by imagining this line to be the space\\nand projecting all the points to the line. Now you have a simpler data set that\\npretty much captures all the information of the original data set, right? See, what happened here is that PCA helped\\nyou go from a two dimensional data set to a one dimensional data set that carried\\nalmost the same amount of information. That's why it's called\\na dimensionality reduction algorithm. So, in summary, principal component\\nanalysis is a technique that is used in data science applications to reduce the\\ndimensions of a dataset while losing as little data as possible. You can imagine a data set\\nwith many columns of data, which can sometimes be cumbersome\\nto use or difficult to visualize. The goal of PCA is to intelligently\\nreduce the number of columns, providing the benefits of a more\\ncompact data set without losing all the useful information\\nthe original data set contained. Here's a quick summary of the path we'll\\ntake to understand these concepts. For lesson 1, you will start with a linear\\ntransformation represented by a matrix, just as you learned in week 3, and the goal is to be able to\\ncharacterize this transformation. The first thing you'll learn is if\\nthe transformation is singular or not. For example, this first transformation\\nof the plane is non-singular, but this second one, from the plane to a line\\nis singular as you might have guessed. These concepts are closely related to the\\nsingularity of a matrix which you studied earlier in the course. Next, we'll return to\\nthe concept of the determinant, this time with a geometric\\ninterpretation of its value. Linear transformations can stretch or\\nshrink space, and you'll see how the determinant\\nhelps quantify how much stretching or shrinking that particular\\ntransformation produces. Finally, you will learn some\\nproperties of the determinant. For example, if you multiply two matrices\\nor find the inverse of a matrix, there are some very intuitive properties\\nof the determinant that arise. These are very useful when you want\\nto analyze the inverse of your transformations, or if you have a cascade of different\\ntransformations one after another. So to sum up, in the first lesson, you will learn to characterize your linear\\ntransformation using the determinant. Now let's take a look into lesson 2,\\nwhich will conclude with an exploration of principal component analysis,\\nwhich we just described. First thing we will need to introduce\\nis the definition of a basis. A basis is a group of\\nvectors that define a space. Moving along the vectors that make\\nup a basis allows you to move to any point in a space. For example, any of the combinations of\\nvectors shown on the right is a basis for the plane. Next, we'll look at the concept of span. This concept is super useful because it\\ntells us which space can we access or generate with linear combinations\\nof groups of vectors. For example,\\na single vector will always span a line. Two vectors which are not pointing in\\nthe same direction and also not pointing in completely opposite directions of\\n180 degrees, will always span a plane. The last ingredient you will need to learn\\nfor PCA is the concept of eigenvectors and eigenvalues. You will learn about these\\nconcepts in more detail, but for now you can kind of think of eigenvectors\\nas the direction that a matrix points to. Applying a linear transformation\\nto a point along its eigenvector will just move the point to a different\\nlocation on that same vector. In other words, for\\npoints on these special vectors, multiplying by a matrix is the same\\nthing as just multiplying by a constant. Eigenvectors help us quickly characterize\\nthe linear transformations associated with a given matrix, and are used in\\nmany machine learning applications. One of those is PCA, which will be\\nthe final topic we cover in this week. Okay, that's a quick summary of\\nwhat you can look forward to. Let's dive in.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"In Week 1 of this course, you learn what singular and\\nnon-singular matrices are. Since matrices correspond\\nto linear transformations, then linear transformations can also be singular\\nand non-singular. You'll be pleasantly surprised to see that there\\nis a very nice way to identify linear\\ntransformations that are singular\\nand non-singular. Even the rank of a linear\\ntransformation can be identified easily.\\nLet me show you how. You already saw previously how the matrix with entries 3, 1, 1, and 2 corresponds to the linear transformation that sends the basis on the\\nleft to the basis on the right and equivalently sends this blue grid to this\\norange grid over here. This is also called the change\\nof basis since it sends the blue square\\nbasis on the left into the orange parallelogram\\nbasis on the right. But one special thing is this. The orange grid covers every\\nsingle point on the plane. It turns out that this\\nis precisely the trait of a non-singular\\ntransformation. If the resulting points after multiplying it by a matrix\\ncover the entire plane, then the transformation is\\nnon-singular, and vice versa. The points that are\\ncovered in the right are called the image\\nof the transformation. To see this, let's\\nlook an example of a singular matrix\\nsuch as this one, the matrix of entries 1,1, 2,2. Notice that if you multiply the matrix by the vector 0,0, you get 0,0 as usual. If you multiply it by the\\nvector 1,0, you get 1,2. Now notice what happens if I multiply it by the vector 0,1. I get again 1,2. If I multiply it\\nby 1,1, I get 2,4. Which means that\\nthe square basis on the left doesn't go\\nto a parallelogram. Or it does, but it's a\\ndegenerate parallelogram. It's actually a line segment. There's a problem\\nbecause the grid on the left doesn't get sent\\nto the entire plane. It gets sent to this line because when we had\\na parallelogram, no matter how thin, it was able to cover the entire\\nplane on the right. But if a parallelogram is flat and it's only\\none line segment, then that can only cover the\\norange line on the right. It cannot cover\\nthe entire plane. In summary, when a matrix is singular, this thing happens. You're not covering the\\nentire plane on the right, you're only covering a\\nsmall portion of it. In this case, you're\\ncovering a line. Now let's look at a\\nvery singular matrix with entry 0,0, 0,0. This matrix sends\\nany vector with any coordinates into the vector 0,0, which is the origin. The entire plane gets sent to that orange\\npoint in the origin. That matrix is definitely very singular because the image\\nis not even a plane, is not even a line,\\nit's actually a point. Here's a summary. The first matrix sends the whole plane to\\nthe whole plane, so it is non-singular. The second one sends\\nthe whole plane to simply a line,\\nso it's singular. The third one sends the\\nwhole plane to a point, so it's even more singular. Notice that the dimension of the image of the\\nfirst one is two, and that's precisely the\\nrank of this matrix. The dimension of the image here is one because it's a line, and that's the rank\\nof this matrix. The dimension here of\\nthe point is zero, which is the rank of\\nthe third matrix. That's another way\\nto calculate rank; the dimension of the image of\\nthe linear transformation.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Now that you know\\nwhat a determinant means in a linear\\ntransformation, you'll be able to see\\nthat the determinant of a product of matrix follows\\na really nice rule. Check it out. Here's a\\nproduct of matrix that I encourage you to check for yourself and make\\nsure that it works. Now what are the\\ndeterminant of the matrix? The first one has the determinant\\nfive, which is 3*2-1*. The second one has the determinant three,\\nwhich is 1*1-1*(-2). The third one has\\nthe determinant 15, which is 1*3-44*(-3). Notice that 5*3 is 15. Could that always be the\\ncase that the determinant of the product of matrices is equal to the product\\nof the determinants? We'll be happy to see\\nthat this is always true. The determinant of AB is equal to the determinant of A\\ntimes the determinant of B. Now if you try to work\\nit out with matrices, it can be a little messy. I like to think of it in\\na much more simple way, which is with linear\\ntransformations. The first matrix, 3,1,1,2, we've seen it already, and is the matrix that sends this fundamental basis\\nto this basis over here. The five is the area\\nof the parallelogram. That means that\\nthis transformation actually blows up\\nthe areas by five. The second matrix has a\\ndeterminant of three. If you work it out, is the one that sends this\\nfundamental basis here into this parallelogram\\nhere that has area three. The three correspond\\nto the determinant. That means that\\nthis transformation blows up the areas by three. Now that happens\\nwith everything. This parallelogram here goes to this parallelogram over\\nhere with area 15. Because what the\\ntransformation does is, no matter what area you have, it blows it up by a factor\\nof the determinant. If you consider the combination of these two linear\\ntransformations, first you're blowing\\nup the areas by five, and then you're blowing\\nup the areas by three. You're blowing them\\nup by 5*3 which is 15 and that is what this\\ntransformation does. It blows up the area of\\nthe fundamental basis of area 1 into a basis of area 15. Therefore, the\\ndeterminant is 15, which is a product of\\nthe two determinants. Here's a quiz to\\ntest your intuition. What would you say about\\nthe product of a singular and a non singular\\nmatrix in any order? Would it be singular, would it be non singular, or could it be either one? Well, let's say that A is non\\nsingular and B is singular. We know that the\\ndeterminant of AB is the determinant of A\\ntimes the determinant of B but determinant of B is\\nzero because B is singular. The determinant of\\nAB has to be zero, because it is the\\ndeterminant of A*0. Therefore, AB is singular. If one matrix is singular, then that matrix multiplied with any other matrix in the\\nworld is also singular. Now, it's quite logical\\nwhen we think of numbers. See if you have any number, doesn't matter which number. Let's say five. When multiplied\\nby zero, it becomes zero. If you have any matrix, when multiplied by\\na singular matrix, it becomes a singular matrix because whatever\\nits determinant is, when multiplied by\\nzero, it becomes zero. Geometrically this make sense. If you multiply two matrix, for example this one over here, which is non singular, with this one over here, which is singular, well, the second matrix being\\nsingular means it sends everything into\\na part of a line. That means that when\\nyou combine them, then it sends a fundamental\\nbasis into some segment. It has determinant zero because\\nthis segment has area 0. In other words, whatever\\narea you have left, it gets blown up by zero.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"In the first week\\nof this course, you'll learn what a\\ndeterminant is and how it's related to the singularity or non singularity of a matrix. You'll be pleased to see that in the linear transformation, the determinant is very nicely explained as an area,\\nor as a volume. When this area or volume is 0, then the matrix is singular. Allow me to show you how. Take a look at the matrix\\nagain with the entries 3, 1, 1 and 2. It's determinant is 3 times 2 minus 1 times 1, which is 5. Now recall that this matrix\\nsends the blue square on the left into the orange\\nparallelogram on the right. Now, take a look at the area\\nof the square, it is 1. The area of the parallelogram, I encourage you to\\ncalculate it by adding and subtracting\\nareas of triangles. It turns out that\\nthese area is 5, which is precisely\\nthe determinant. That is always the case, the determinant of the\\nmatrix is the area of the image of the\\nfundamental basis formed by the unit\\nsquare on the left. What happens with a\\nsingular transformation? Well, recall that in this\\ntransformation over here, with determinant 1 times 2\\nminus 2 times 1 equals 0, the fundamental square gets sent to a very skinny\\nparallelogram. It's so skinny\\nthat it's actually a line segment and the\\nline segment has area 0, which is precisely the\\ndeterminant of the matrix. Finally, for the very singular\\ntransformation with entries 0,0,0,0 and obvious\\ndeterminant of 0, the transformation sends\\na fundamental unit square to the point 0,0. This point has an area of 0 corresponding to the\\ndeterminant of 0. In summary, we got a non singular matrix\\nwith determinant 5, just precisely the area of the image of the\\nfundamental square, then another singular\\nmatrix with determinant 0, which is the area of the image\\nof the fundamental square, which is a line segment and another singular matrix\\nagain with determinant 0, since the area of the image\\nof the fundamental square, this time is a point, is also 0. Now, here's something that\\nyou may be wondering, what about negative\\ndeterminants? Notice that the matrix on\\nthe right that's formed by permuting the two columns\\non the matrix on the left. For that reason, it's got the negative determinant\\nof negative 5. Here's the small technicality. A parallelogram can have a\\nnegative area depending on what order we take the two\\nbasis vectors as follows. Notice that the matrix\\nwith entries 1, 3, 2 and 1 sends the\\nvector with coordinates 1,0 to the vector\\nof coordinates 1,2, and the vector of\\ncoordinates 0,1, to the vector of\\ncoordinates 3,1. It's the same as\\nthe other matrix, except in the opposite order. It turns out that by\\nthat small technicality, we say that the area of the\\nparallelogram is negative, if we take the vectors in\\ncounterclockwise order and positive if we take\\nthem in clockwise order. Therefore, the area on the\\nsquare on the left is 1, but the area on the\\nparallelogram on the right, is going to be minus 5. But notice that a\\ndeterminant being positive or negative\\ndoesn't actually affect the singularity\\nof the matrix and so all the matters for the\\nmatrix to be non singular, is that the determinant\\nis different than 0.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors'),\n",
       " (\"Congratulations, you've finished the fourth week and with it, you've finished the full\\nlinear algebra course. Now you know how to solve\\nsystems of linear equations, how to turn them into\\nmatrices and vectors, how to consider\\nthese matrices as linear transformations,\\nand a lot more. I'm so excited that you've taken the step in your machine\\nlearning journey. I'm even more excited to see you in the calculus for\\nmachine learning course.\",\n",
       "  'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"../Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# prompt_1 = \"\"\"Review Text: \"%s\"\n",
    "\n",
    "# Task: Your task is to convert this text into Basic Note Type (front/back) Anki flashcards. Prioritize information regarding the imaging features of diseases, unique imaging findings, and methods of differentiating similar disease entities. Ensure that each flashcard is clearly written, and adheres to the specified formatting and reference criteria.\n",
    "\n",
    "# Formatting Criteria:\n",
    "\n",
    "# - Construct a table with three columns: \"Front\", “Back”, \"Number\".\n",
    "# - Each row of the \"Front\" column should contain a single question testing the imaging features of disease, unique imaging findings, and methods of differentiating similar disease entities.\n",
    "# - The “Back” column should contain the succinct answer to the question in the corresponding row of the “Front” column.\n",
    "# - The \"Number\" column will serve to number each row, facilitating feedback.\n",
    "\n",
    "# Reference Criteria for each \"Statement\":\n",
    "\n",
    "# - Each flashcard should test a single concept.\n",
    "# - Each flashcard MUST be able to stand alone. Include the subject of the flashcard somewhere in the text.\n",
    "# - Keep ONLY simple, direct questions in the \"Front\" column.\n",
    "# - Clear concise language but if required give plenty of context.\n",
    "# - Output csv format like the example below.\n",
    "# - Output at least %d rows of question/answers.\n",
    "\n",
    "# Example:\n",
    "\n",
    "# Front;Back;Number\n",
    "# \"How is necrotic tissue identified in acute pancreatitis on a CT scan?\";\"Lack of contrast enhancement.\";1\n",
    "# \"Why should people create their own examples?\";\"Because Jake is too tired to think of good examples.\";2\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# text, tag = all_text[0]\n",
    "\n",
    "# # Split the text based on space and newline\n",
    "# split_text = re.split(r\"[\\s\\n]+\", text)\n",
    "\n",
    "# n_flashcards = int(len(split_text) / 65)\n",
    "# n_flashcards = max(3, n_flashcards)\n",
    "\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a teacher who is writing anki cards for his students.\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         # \"content\": f\"Please write an anki card on the following text:\\n{text}\",\n",
    "#         \"content\": prompt_1 % (text, 20),\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# print(model.device)\n",
    "\n",
    "# terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=1024,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# response = outputs[0][input_ids.shape[-1] :]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write Questions\n",
    "\n",
    "Structure: \n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Model\n",
    "    3. Parse Lecture Material \n",
    "    4. Generate Questions\n",
    "    5. Save to DataFrame\n",
    "    6. Save to File\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os, pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# variables\n",
    "\n",
    "prompt_0 = \"\"\"Review Text: \"%s\"\n",
    "Task: You are tasked with summarizing the core theoretical information from a course lecture. Please identify and extract the key concepts, definitions, principles, and rules presented in the lecture. Focus on the general theory that can be applied universally, and exclude any specific examples, case studies, or situational applications unless they directly illustrate a fundamental concept. The summary should provide a clear and concise overview of the theoretical framework discussed in the lecture, ensuring that it is applicable in a wide range of contexts.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_1 = \"\"\"Review Text: \"%s\"\n",
    "\n",
    "Task: Your task is to convert this text into Basic Note Type (front/back) Anki flashcards. Prioritize information regarding the imaging features of diseases, unique imaging findings, and methods of differentiating similar disease entities. Ensure that each flashcard is clearly written, and adheres to the specified formatting and reference criteria.\n",
    "\n",
    "Formatting Criteria:\n",
    "\n",
    "- Construct a table with three columns: \"Front\", “Back”, \"Number\".\n",
    "- Each row of the \"Front\" column should contain a single question testing the imaging features of disease, unique imaging findings, and methods of differentiating similar disease entities.\n",
    "- The “Back” column should contain the succinct answer to the question in the corresponding row of the “Front” column.\n",
    "- The \"Number\" column will serve to number each row, facilitating feedback.\n",
    "\n",
    "Reference Criteria for each \"Statement\":\n",
    "\n",
    "- Each flashcard should test a single concept.\n",
    "- Each flashcard MUST be able to stand alone. Include the subject of the flashcard somewhere in the text.\n",
    "- Keep ONLY simple, direct questions in the \"Front\" column.\n",
    "- Clear concise language but if required give plenty of context. \n",
    "- Output csv format like the example below. Make sure text is properly formatted between \"\".\n",
    "- Important: output no more than %d rows of question/answers.\n",
    "\n",
    "Example:\n",
    "\n",
    "Front;Back;Number\n",
    "\"How is necrotic tissue identified in acute pancreatitis on a CT scan?\";\"Lack of contrast enhancement.\";1\n",
    "\"Why should people create their own examples?\";\"Because Jake is too tired to think of good examples.\";2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def clean_response(response_text):\n",
    "    \"\"\"Clean Response Text\n",
    "    Args:\n",
    "        response_text (str): response text from model\n",
    "    Returns:\n",
    "        response_df (df): cleaned response df\n",
    "    \"\"\"\n",
    "\n",
    "    # clear response text\n",
    "    # delete everything before Front;Back;Number\n",
    "    response_text = response_text[response_text.find(\"Front;Back;Number\") :]\n",
    "\n",
    "    # Use StringIO to create a file-like object from the string\n",
    "    _data = StringIO(response_text)\n",
    "\n",
    "    # reset the file pointer to the beginning\n",
    "    _data.seek(0)\n",
    "\n",
    "    # read the lines in the data file\n",
    "    lines = _data.readlines()\n",
    "\n",
    "    # filter out the lines with no semicolon\n",
    "    lines = [line for line in lines if \";\" in line]\n",
    "\n",
    "    # combine filtered lines back\n",
    "    filtered_data = StringIO(\"\".join(lines))\n",
    "\n",
    "    # Read the data into a pandas dataframe\n",
    "    df = pd.read_csv(filtered_data, delimiter=\";\", quotechar='\"', on_bad_lines=\"skip\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 2. Load Model\n",
    "model_id = os.path.join(\"..\", \"Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  2%|▏         | 1/60 [00:12<12:19, 12.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  3%|▎         | 2/60 [00:24<11:40, 12.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  5%|▌         | 3/60 [00:38<12:32, 13.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  7%|▋         | 4/60 [00:51<12:17, 13.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  8%|▊         | 5/60 [01:03<11:23, 12.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 10%|█         | 6/60 [01:15<11:11, 12.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 12%|█▏        | 7/60 [01:27<10:55, 12.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 13%|█▎        | 8/60 [01:39<10:30, 12.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 15%|█▌        | 9/60 [01:54<11:09, 13.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 17%|█▋        | 10/60 [02:03<09:46, 11.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 18%|█▊        | 11/60 [02:16<10:04, 12.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 20%|██        | 12/60 [02:24<08:48, 11.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 22%|██▏       | 13/60 [02:36<08:51, 11.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 23%|██▎       | 14/60 [02:45<08:04, 10.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 25%|██▌       | 15/60 [02:55<07:38, 10.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 27%|██▋       | 16/60 [03:06<07:45, 10.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 28%|██▊       | 17/60 [03:17<07:44, 10.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 30%|███       | 18/60 [03:29<07:41, 10.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 32%|███▏      | 19/60 [03:40<07:33, 11.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 33%|███▎      | 20/60 [03:49<06:54, 10.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 35%|███▌      | 21/60 [03:57<06:17,  9.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 37%|███▋      | 22/60 [04:06<05:57,  9.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 38%|███▊      | 23/60 [04:21<06:51, 11.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 40%|████      | 24/60 [04:32<06:36, 11.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 42%|████▏     | 25/60 [04:43<06:28, 11.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 43%|████▎     | 26/60 [04:52<05:58, 10.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 45%|████▌     | 27/60 [04:58<05:01,  9.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 47%|████▋     | 28/60 [05:10<05:16,  9.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 48%|████▊     | 29/60 [05:20<05:15, 10.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 50%|█████     | 30/60 [05:32<05:13, 10.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 52%|█████▏    | 31/60 [05:43<05:14, 10.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 53%|█████▎    | 32/60 [05:53<04:52, 10.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 55%|█████▌    | 33/60 [06:04<04:51, 10.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 57%|█████▋    | 34/60 [06:15<04:40, 10.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 58%|█████▊    | 35/60 [06:26<04:29, 10.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 60%|██████    | 36/60 [06:36<04:14, 10.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 62%|██████▏   | 37/60 [06:46<04:00, 10.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 63%|██████▎   | 38/60 [06:56<03:46, 10.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 65%|██████▌   | 39/60 [07:11<04:05, 11.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 67%|██████▋   | 40/60 [07:25<04:09, 12.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 68%|██████▊   | 41/60 [07:36<03:45, 11.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 70%|███████   | 42/60 [07:48<03:37, 12.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 72%|███████▏  | 43/60 [08:00<03:21, 11.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 73%|███████▎  | 44/60 [08:12<03:10, 11.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 75%|███████▌  | 45/60 [08:21<02:45, 11.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 77%|███████▋  | 46/60 [08:33<02:40, 11.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 78%|███████▊  | 47/60 [08:45<02:29, 11.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 80%|████████  | 48/60 [08:56<02:16, 11.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 82%|████████▏ | 49/60 [09:08<02:05, 11.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 83%|████████▎ | 50/60 [09:19<01:56, 11.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 85%|████████▌ | 51/60 [09:34<01:53, 12.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 87%|████████▋ | 52/60 [09:48<01:43, 12.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 88%|████████▊ | 53/60 [10:00<01:29, 12.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 90%|█████████ | 54/60 [10:12<01:14, 12.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 92%|█████████▏| 55/60 [10:20<00:54, 11.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 93%|█████████▎| 56/60 [10:30<00:42, 10.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 95%|█████████▌| 57/60 [10:39<00:31, 10.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 97%|█████████▋| 58/60 [10:50<00:21, 10.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 98%|█████████▊| 59/60 [11:00<00:10, 10.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "100%|██████████| 60/60 [11:08<00:00, 11.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming prompt_0 and prompt_1 are defined elsewhere in your script\n",
    "# Example:\n",
    "# prompt_0 = \"Summarize the theoretical information from the following text:\\n%s\"\n",
    "# prompt_1 = \"Please write %d anki cards based on the following theoretical summary:\\n%s\"\n",
    "\n",
    "errors = list()\n",
    "first_iteration = True\n",
    "\n",
    "for i, (text, tag) in tqdm(enumerate(all_text), total=len(all_text)):\n",
    "    # Step 1: Use prompt_0 to extract theoretical information\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a teacher summarizing theoretical content from a lecture.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_0 % text,  # Apply prompt_0 to extract theory\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Generate theoretical summary\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    theory_summary = tokenizer.decode(\n",
    "        outputs[0][input_ids.shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Step 2: Use prompt_1 to generate Anki cards based on the theoretical summary\n",
    "    split_text = re.split(r\"[\\s\\n]+\", theory_summary)\n",
    "    n_flashcards = int(len(split_text) / 180)\n",
    "    n_flashcards = max(2, n_flashcards)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a teacher who is writing Anki cards for his students.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_1 % (theory_summary, n_flashcards + 1),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Generate Questions (Anki cards)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1] :]\n",
    "    response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # Step 3: Save to DataFrame\n",
    "    if first_iteration:\n",
    "        df_responses = clean_response(response_text=response_text)\n",
    "        df_responses[\"Deck_ID\"] = tag\n",
    "        first_iteration = False\n",
    "    else:\n",
    "        try:\n",
    "            df_response = clean_response(response_text=response_text)\n",
    "            df_response[\"Deck_ID\"] = tag\n",
    "            df_responses = pd.concat([df_responses, df_response])\n",
    "            del df_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in response {i}: {e}\")\n",
    "            errors.append(response_text)\n",
    "\n",
    "    # Step 4: Save to File (if necessary)\n",
    "    # df_responses.to_csv(\"anki_cards.csv\", index=False)  # Save the dataframe to a CSV file\n",
    "\n",
    "# End of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Front</th>\n",
       "      <th>Back</th>\n",
       "      <th>Number</th>\n",
       "      <th>Deck_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a linear equation?</td>\n",
       "      <td>A statement that gives information, written in...</td>\n",
       "      <td>1</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three types of systems of linear ...</td>\n",
       "      <td>Singular, Non-Singular, and Redundant</td>\n",
       "      <td>2</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the importance of understanding the th...</td>\n",
       "      <td>It provides the foundation for solving problem...</td>\n",
       "      <td>3</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a system of linear equations?</td>\n",
       "      <td>A set of equations in which each equation is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the difference between a singular and ...</td>\n",
       "      <td>A singular system has no unique solution, whil...</td>\n",
       "      <td>2</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happens to the fundamental square when a ...</td>\n",
       "      <td>It is sent to a set of measure zero, such as a...</td>\n",
       "      <td>2</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the relationship between the determina...</td>\n",
       "      <td>A matrix is singular if its determinant is zer...</td>\n",
       "      <td>3</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key concepts covered in a linear ...</td>\n",
       "      <td>Linear Systems of Equations, Matrices and Vect...</td>\n",
       "      <td>1</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the definition of a linear system of e...</td>\n",
       "      <td>A set of equations in which the coefficients o...</td>\n",
       "      <td>2</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the fundamental principles of linear ...</td>\n",
       "      <td>Linearity, Additivity, and Homogeneity, which ...</td>\n",
       "      <td>3</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Front  \\\n",
       "0                          What is a linear equation?   \n",
       "1   What are the three types of systems of linear ...   \n",
       "2   What is the importance of understanding the th...   \n",
       "0               What is a system of linear equations?   \n",
       "1   What is the difference between a singular and ...   \n",
       "..                                                ...   \n",
       "1   What happens to the fundamental square when a ...   \n",
       "2   What is the relationship between the determina...   \n",
       "0   What are the key concepts covered in a linear ...   \n",
       "1   What is the definition of a linear system of e...   \n",
       "2   What are the fundamental principles of linear ...   \n",
       "\n",
       "                                                 Back  Number  \\\n",
       "0   A statement that gives information, written in...       1   \n",
       "1               Singular, Non-Singular, and Redundant       2   \n",
       "2   It provides the foundation for solving problem...       3   \n",
       "0   A set of equations in which each equation is a...       1   \n",
       "1   A singular system has no unique solution, whil...       2   \n",
       "..                                                ...     ...   \n",
       "1   It is sent to a set of measure zero, such as a...       2   \n",
       "2   A matrix is singular if its determinant is zer...       3   \n",
       "0   Linear Systems of Equations, Matrices and Vect...       1   \n",
       "1   A set of equations in which the coefficients o...       2   \n",
       "2   Linearity, Additivity, and Homogeneity, which ...       3   \n",
       "\n",
       "                                              Deck_ID  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "..                                                ...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  0%|          | 0/60 [00:23<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # 3. Parse Lecture Material\n",
    "# errors = list()\n",
    "# first_iteration = True\n",
    "# for i, (text, tag) in tqdm(enumerate(all_text), total=len(all_text)):\n",
    "\n",
    "#     # Split the text based on space and newline\n",
    "#     split_text = re.split(r\"[\\s\\n]+\", text)\n",
    "\n",
    "#     n_flashcards = int(len(split_text) / 180)\n",
    "#     n_flashcards = max(2, n_flashcards)\n",
    "\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"You are a teacher who is writing anki cards for his students.\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             # \"content\": f\"Please write an anki card on the following text:\\n{text}\",\n",
    "#             \"content\": prompt_1 % (text, n_flashcards),\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "#     # 4. Generate Questions\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     terminators = [\n",
    "#         tokenizer.eos_token_id,\n",
    "#         tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "#     ]\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=2048,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.6,\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "#     response = outputs[0][input_ids.shape[-1] :]\n",
    "#     response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "#     # print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "#     # 5. Save to DataFrame\n",
    "#     if first_iteration:\n",
    "#         df_responses = clean_response(response_text=response)\n",
    "#         df_responses[\"Deck_ID\"] = tag\n",
    "#         first_iteration = False\n",
    "#     else:\n",
    "#         try:\n",
    "#             df_response = clean_response(response_text=response)\n",
    "#             df_response[\"Deck_ID\"] = tag\n",
    "#             df_responses = pd.concat([df_responses, df_response])\n",
    "#             del df_response\n",
    "#         except:\n",
    "#             print(f\"Error in response {i}\")\n",
    "#             errors.append(response)\n",
    "#     # 6. Save to File\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 15:56:55,050 - INFO - Number of unique tags: ['mathematics for ml::machine learning linear algebra::01 systems of linear equations'\n",
      " 'mathematics for ml::machine learning linear algebra::03 vectors and linear transformations'\n",
      " 'mathematics for ml::machine learning linear algebra::02 solving systems of linear equations'\n",
      " 'mathematics for ml::machine learning linear algebra::04 determinants and eigenvectors']\n",
      "2024-08-12 15:56:55,051 - INFO - Number of flashcards: 180\n",
      "2024-08-12 15:56:55,052 - INFO - Number of flashcards after nan cleaning: 180\n"
     ]
    }
   ],
   "source": [
    "df_responses.drop([\"Number\"], inplace=True, axis=1)\n",
    "\n",
    "logging.info(f\"Number of unique tags: {df_responses['Deck_ID'].unique()}\")\n",
    "\n",
    "logging.info(f\"Number of flashcards: {len(df_responses)}\")\n",
    "\n",
    "df_responses.dropna(inplace=True)\n",
    "logging.info(f\"Number of flashcards after nan cleaning: {len(df_responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anki package created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import genanki\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Example DataFrame with questions and answers\n",
    "output_path_decks = os.path.join(\"..\", \"generated_decks\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path_decks, exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to generate unique model and deck IDs\n",
    "def generate_id():\n",
    "    return random.randrange(1 << 30, 1 << 31)\n",
    "\n",
    "\n",
    "# Define the model for the notes\n",
    "model_id = generate_id()\n",
    "my_model = genanki.Model(\n",
    "    model_id,\n",
    "    \"Simple Model\",\n",
    "    fields=[\n",
    "        {\"name\": \"Question\"},\n",
    "        {\"name\": \"Answer\"},\n",
    "    ],\n",
    "    templates=[\n",
    "        {\n",
    "            \"name\": \"Card 1\",\n",
    "            \"qfmt\": \"{{Question}}\",\n",
    "            \"afmt\": '{{FrontSide}}<hr id=\"answer\">{{Answer}}',\n",
    "        },\n",
    "    ],\n",
    "    css=\"\"\"\n",
    "    .card {\n",
    "      font-family: arial;\n",
    "      font-size: 20px;\n",
    "      text-align: center;\n",
    "    }\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create decks\n",
    "generated_decks = list()\n",
    "d_decks = dict()\n",
    "for deck_id in df_responses[\"Deck_ID\"].unique():\n",
    "    n_levels = len(deck_id.split(\"::\"))\n",
    "    for level in range(0, n_levels, 1):\n",
    "        deck_name = \"::\".join(deck_id.split(\"::\")[: level + 1])\n",
    "        if deck_name not in generated_decks:\n",
    "            created_deck = genanki.Deck(generate_id(), deck_name)\n",
    "            generated_decks.append(deck_name)\n",
    "            d_decks[deck_name] = created_deck\n",
    "\n",
    "# Add flashcards to decks\n",
    "for deck_id in df_responses[\"Deck_ID\"].unique():\n",
    "    # Query df\n",
    "    query = f\"Deck_ID == '{deck_id}'\"\n",
    "    df_query = df_responses.query(query)\n",
    "\n",
    "    # Add flashcards to deck\n",
    "    for index, row in df_query.iterrows():\n",
    "        front = str(row[\"Front\"]) if row[\"Front\"] is not None else \"\"\n",
    "        back = str(row[\"Back\"]) if row[\"Back\"] is not None else \"\"\n",
    "        note = genanki.Note(model=my_model, fields=[front, back])\n",
    "        d_decks[deck_id].add_note(note)\n",
    "\n",
    "# Create a package and write it to an .apkg file\n",
    "all_decks = list(d_decks.values())\n",
    "package = genanki.Package(all_decks)\n",
    "\n",
    "# Write to file\n",
    "\n",
    "if specialization is not None:\n",
    "    package.write_to_file(\n",
    "        os.path.join(output_path_decks, f\"{specialization}.{course}.apkg\")\n",
    "    )\n",
    "else:\n",
    "    package.write_to_file(os.path.join(output_path_decks, f\"{course}.apkg\"))\n",
    "\n",
    "print(\"Anki package created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Front</th>\n",
       "      <th>Back</th>\n",
       "      <th>Deck_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a linear equation?</td>\n",
       "      <td>A statement that gives information, written in...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three types of systems of linear ...</td>\n",
       "      <td>Singular, Non-Singular, and Redundant</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the importance of understanding the th...</td>\n",
       "      <td>It provides the foundation for solving problem...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a system of linear equations?</td>\n",
       "      <td>A set of equations in which each equation is a...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the difference between a singular and ...</td>\n",
       "      <td>A singular system has no unique solution, whil...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happens to the fundamental square when a ...</td>\n",
       "      <td>It is sent to a set of measure zero, such as a...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the relationship between the determina...</td>\n",
       "      <td>A matrix is singular if its determinant is zer...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key concepts covered in a linear ...</td>\n",
       "      <td>Linear Systems of Equations, Matrices and Vect...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the definition of a linear system of e...</td>\n",
       "      <td>A set of equations in which the coefficients o...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the fundamental principles of linear ...</td>\n",
       "      <td>Linearity, Additivity, and Homogeneity, which ...</td>\n",
       "      <td>mathematics for ml::machine learning linear al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Front  \\\n",
       "0                          What is a linear equation?   \n",
       "1   What are the three types of systems of linear ...   \n",
       "2   What is the importance of understanding the th...   \n",
       "0               What is a system of linear equations?   \n",
       "1   What is the difference between a singular and ...   \n",
       "..                                                ...   \n",
       "1   What happens to the fundamental square when a ...   \n",
       "2   What is the relationship between the determina...   \n",
       "0   What are the key concepts covered in a linear ...   \n",
       "1   What is the definition of a linear system of e...   \n",
       "2   What are the fundamental principles of linear ...   \n",
       "\n",
       "                                                 Back  \\\n",
       "0   A statement that gives information, written in...   \n",
       "1               Singular, Non-Singular, and Redundant   \n",
       "2   It provides the foundation for solving problem...   \n",
       "0   A set of equations in which each equation is a...   \n",
       "1   A singular system has no unique solution, whil...   \n",
       "..                                                ...   \n",
       "1   It is sent to a set of measure zero, such as a...   \n",
       "2   A matrix is singular if its determinant is zer...   \n",
       "0   Linear Systems of Equations, Matrices and Vect...   \n",
       "1   A set of equations in which the coefficients o...   \n",
       "2   Linearity, Additivity, and Homogeneity, which ...   \n",
       "\n",
       "                                              Deck_ID  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "..                                                ...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "0   mathematics for ml::machine learning linear al...  \n",
       "1   mathematics for ml::machine learning linear al...  \n",
       "2   mathematics for ml::machine learning linear al...  \n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
